Reading prompt from stdin...
{"type":"thread.started","thread_id":"019c8446-94bb-7c70-91f1-f82cd226a5c0"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Planning environment and data setup**"}}
{"type":"item.completed","item":{"id":"item_1","type":"agent_message","text":"I’m starting by applying the `paper-finder` and `literature-review` skills because this task is a full resource-gathering workflow. First I’ll verify the workspace location, inspect the skill instructions, then set up a fresh `uv` environment exactly as requested."}}
{"type":"item.started","item":{"id":"item_2","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' .codex/skills/paper-finder/SKILL.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc 'pwd && ls -la'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_4","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' .codex/skills/literature-review/SKILL.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_2","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' .codex/skills/paper-finder/SKILL.md\"","aggregated_output":"---\nname: paper-finder\ndescription: Find and search for academic papers using the paper-finder service. Use when conducting literature review, searching for related work, finding baseline papers, or looking for methodology references.\n---\n\n# Paper Finder\n\nSystematic paper discovery and prioritization for research projects.\n\n## When to Use\n\n- Starting a literature review\n- Looking for related work on a topic\n- Finding baseline papers for experiments\n- Searching for methodology references\n- Building a citation list for a research paper\n\n## How to Use\n\nRun the helper script from your workspace:\n\n```bash\npython .claude/skills/paper-finder/scripts/find_papers.py \"your research topic\"\n```\n\nOptions:\n- `--mode fast` (default): Quick search\n- `--mode diligent`: Thorough search (recommended for comprehensive review)\n- `--format json`: Output as JSON instead of text\n\nExample:\n```bash\npython .claude/skills/paper-finder/scripts/find_papers.py \"hypothesis generation with large language models\" --mode fast\n```\n\n## Search Strategy\n\n### Query Formulation\n\nUse structured queries for better results:\n\n1. **Core concept + Method**: \"transformer attention mechanism\"\n2. **Problem + Domain**: \"few-shot learning natural language processing\"\n3. **Technique + Application**: \"graph neural networks drug discovery\"\n\n### Multi-Stage Search\n\n1. **Broad scan**: Start with general topic terms\n2. **Focused dive**: Use specific method/technique names from initial results\n3. **Citation chase**: Search for highly-cited papers referenced in relevant work\n\n## Citation Prioritization\n\n### Relevance Tiers\n\n| Relevance Score | Priority | Action |\n|-----------------|----------|--------|\n| 3 (High) | Must read | Download PDF, read fully, cite |\n| 2 (Medium) | Should read | Read abstract + intro, cite if relevant |\n| 1 (Low) | Optional | Skim abstract, cite only if needed |\n| 0 (Not relevant) | Skip | Do not include |\n\n### Citation Count Thresholds\n\n| Category | Citation Count | Interpretation |\n|----------|----------------|----------------|\n| Seminal | 1000+ | Foundational work, must cite |\n| Well-established | 100-999 | Widely accepted, cite if relevant |\n| Recent/Emerging | 10-99 | Current research, cite for novelty |\n| New | <10 | Very recent, check publication venue |\n\n### Venue Tiers (ML/AI Focus)\n\n**Tier 1** (Top venues, high credibility):\n- NeurIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV\n\n**Tier 2** (Strong venues):\n- AAAI, IJCAI, NAACL, COLING, ECCV, WACV\n\n**Tier 3** (Good venues):\n- *ACL workshops, COLM, EACL, CoNLL\n\n**Preprints** (arXiv):\n- Check for peer-reviewed version first\n- Cite arXiv only if no published version exists\n\n## Screening Workflow\n\n### Phase 1: Title Screening\n- Review titles from search results\n- Mark papers as \"include\", \"exclude\", or \"maybe\"\n- Goal: ~50% reduction\n\n### Phase 2: Abstract Screening\n- Read abstracts for included/maybe papers\n- Evaluate: relevance, methodology, findings\n- Goal: Identify key papers for deeper reading\n\n### Phase 3: Full-Text Review\n- Download and read full PDFs for key papers\n- Extract: methods, results, limitations, citations\n- Use the PDF chunker for detailed reading (see below)\n\n## Output Structure\n\nReturns relevance-ranked papers with:\n- Title, authors, year\n- Abstract (already extracted)\n- URL for download\n- Relevance score (0-3, focus on papers with score >= 2)\n- Citation count\n\n## After Finding Papers\n\n1. Download PDFs for papers with relevance >= 2\n2. Read abstracts first (already provided in output)\n3. Only read full PDFs for most relevant papers\n4. Write notes to literature_review.md immediately\n5. Track citations for references.bib\n\n## Reading Large PDFs\n\nUse the PDF chunker to split papers into smaller PDF files that can be read directly.\nThis preserves all formatting perfectly (unlike text extraction which loses formatting).\n\n**Dependencies:**\n```bash\n# Using uv (recommended):\nuv add pypdf\n\n# Or with pip:\npip install pypdf\n```\n\n**How to run:**\n\n```bash\npython .claude/skills/paper-finder/scripts/pdf_chunker.py <pdf_path>\n```\n\nOptions:\n- `--pages-per-chunk N`: Number of pages per chunk (default: 1)\n- `--output-dir DIR`: Output directory (default: `<pdf_dir>/pages`)\n\n**Output:**\n- Creates PDF chunk files: `<pdf_name>_chunk_001.pdf`, `<pdf_name>_chunk_002.pdf`, etc.\n- Creates a manifest: `<pdf_name>_manifest.txt` listing all chunks with page ranges\n\n**Integration with screening workflow:**\n1. Run the chunker on papers before detailed reading\n2. For abstract skimming: read only chunk 1 (page 1 or pages 1-3)\n3. For deep reading: read ALL chunk PDFs sequentially, writing notes after each\n4. Check the manifest to see how many chunks exist\n5. IMPORTANT: Do not skip chunks - methodology and results are in later chunks\n\n## If Paper-Finder Service Not Running\n\nThe script will show a fallback message. Use manual search instead:\n- arXiv: https://arxiv.org\n- Semantic Scholar: https://www.semanticscholar.org\n- Papers with Code: https://paperswithcode.com\n- Google Scholar: https://scholar.google.com\n\nManual search works well - paper-finder is just a convenience for faster, more targeted results.\n\n## References\n\nSee `references/` folder for:\n- `search_strategies.md`: Detailed search query formulation\n- `prioritization_guide.md`: Extended prioritization criteria\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc 'pwd && ls -la'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 46\ndrwxrwx--- 10 haokunliu haokunliu   12 Feb 22 01:35 .\ndrwxrwx--- 17 haokunliu haokunliu   17 Feb 22 01:35 ..\ndrwxrwx---  3 haokunliu haokunliu    3 Feb 22 01:35 .claude\ndrwxrwx---  3 haokunliu haokunliu    3 Feb 22 01:35 .codex\ndrwxrwx---  3 haokunliu haokunliu    3 Feb 22 01:35 .gemini\ndrwxrwx---  7 haokunliu haokunliu   14 Feb 22 01:35 .git\n-rw-rw----  1 haokunliu haokunliu 7975 Feb 22 01:35 .gitignore\ndrwxrwx---  2 haokunliu haokunliu    4 Feb 22 01:35 .idea-explorer\n-rw-rw----  1 haokunliu haokunliu  130 Feb 22 01:35 README.md\ndrwxrwx---  2 haokunliu haokunliu    2 Feb 22 01:35 artifacts\ndrwxrwx---  2 haokunliu haokunliu    5 Feb 22 01:35 logs\ndrwxrwx---  2 haokunliu haokunliu    2 Feb 22 01:35 results\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_4","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' .codex/skills/literature-review/SKILL.md\"","aggregated_output":"---\nname: literature-review\ndescription: Conduct systematic literature reviews using a structured workflow. Use when starting a new research project, surveying a field, or documenting related work for a paper.\n---\n\n# Literature Review\n\nSystematic workflow for conducting comprehensive literature reviews.\n\n## When to Use\n\n- Starting a new research project\n- Surveying a research field\n- Writing the Related Work section of a paper\n- Identifying research gaps\n- Building a comprehensive reading list\n\n## PRISMA-Lite Workflow\n\nThis workflow adapts the PRISMA (Preferred Reporting Items for Systematic Reviews) framework for ML/AI research.\n\n### Phase 1: Define Scope\n\nBefore searching, define:\n\n1. **Research Question**: What are you trying to learn?\n2. **Inclusion Criteria**: What makes a paper relevant?\n3. **Exclusion Criteria**: What makes a paper not relevant?\n4. **Time Frame**: How far back to search?\n5. **Search Sources**: Which databases to use?\n\nDocument in `literature_review.md`:\n\n```markdown\n## Review Scope\n\n### Research Question\n[Your research question here]\n\n### Inclusion Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n\n### Exclusion Criteria\n- [ ] Criterion 1\n- [ ] Criterion 2\n\n### Time Frame\n[e.g., 2019-present]\n\n### Sources\n- [ ] Semantic Scholar\n- [ ] arXiv\n- [ ] Google Scholar\n- [ ] ACL Anthology\n```\n\n### Phase 2: Search\n\nExecute systematic search using paper-finder or manual search:\n\n1. **Primary search**: Use core topic terms\n2. **Secondary search**: Use method/technique names\n3. **Citation search**: Check references of key papers\n\nTrack search queries:\n\n```markdown\n## Search Log\n\n| Date | Query | Source | Results | Notes |\n|------|-------|--------|---------|-------|\n| YYYY-MM-DD | \"query here\" | Semantic Scholar | N papers | Initial search |\n```\n\n### Phase 3: Screening\n\nThree-stage screening process:\n\n#### Stage 1: Title Screening\n- Review all titles\n- Quick relevance judgment\n- Mark: Include / Exclude / Maybe\n\n#### Stage 2: Abstract Screening\n- Read abstracts for Include/Maybe papers\n- Evaluate methodology and findings\n- Mark: Include / Exclude\n\n#### Stage 3: Full-Text Screening\n- Download and read full papers\n- Verify relevance and quality\n- Extract key information\n\nTrack screening:\n\n```markdown\n## Screening Results\n\n| Paper | Title Screen | Abstract Screen | Full-Text | Notes |\n|-------|-------------|-----------------|-----------|-------|\n| Paper1 | Include | Include | Include | Key baseline |\n| Paper2 | Maybe | Exclude | - | Different task |\n```\n\n### Phase 4: Data Extraction\n\nFor each included paper, extract:\n\n1. **Bibliographic**: Authors, year, venue\n2. **Problem**: What problem is addressed?\n3. **Method**: What approach is used?\n4. **Data**: What datasets/benchmarks?\n5. **Results**: Key findings\n6. **Limitations**: Acknowledged weaknesses\n7. **Relevance**: How relates to our work?\n\nUse the extraction template in `assets/review_template.md`.\n\n### Phase 5: Synthesis\n\nOrganize findings by theme:\n\n1. **Identify themes**: Group related papers\n2. **Compare approaches**: What are the differences?\n3. **Find gaps**: What's missing?\n4. **Position work**: Where does your work fit?\n\n## Output Files\n\n### literature_review.md\n\nMain document tracking the review:\n\n```markdown\n# Literature Review: [Topic]\n\n## Review Scope\n[Scope definition]\n\n## Search Log\n[Search queries and results]\n\n## Paper Summaries\n[Individual paper notes]\n\n## Themes and Synthesis\n[Grouped findings]\n\n## Research Gaps\n[Identified opportunities]\n\n## Key Citations\n[Must-cite papers for your work]\n```\n\n### papers/ directory\n\nOrganize downloaded papers:\n\n```\npapers/\n├── must_read/           # Relevance 3, priority reading\n├── should_read/         # Relevance 2\n├── reference/           # Background papers\n└── README.md            # Index of all papers\n```\n\n## Tools\n\n### Reading Large PDFs\n\nUse the PDF chunker to split papers into smaller PDF files that can be read directly.\nThis preserves all formatting perfectly (unlike text extraction which loses formatting).\n\n**Dependencies:**\n```bash\n# Using uv (recommended):\nuv add pypdf\n\n# Or with pip:\npip install pypdf\n```\n\n**How to run:**\n\n```bash\npython .claude/skills/literature-review/scripts/pdf_chunker.py <pdf_path>\n```\n\nOptions:\n- `--pages-per-chunk N`: Number of pages per chunk (default: 1)\n- `--output-dir DIR`: Output directory (default: `<pdf_dir>/pages`)\n\n**Output:**\n- Creates PDF chunk files: `<pdf_name>_chunk_001.pdf`, `<pdf_name>_chunk_002.pdf`, etc.\n- Creates a manifest: `<pdf_name>_manifest.txt` listing all chunks with page ranges\n\n**Integration with screening workflow:**\n1. During Phase 3 (Full-Text Screening), run the chunker on papers that need detailed review\n2. For abstract skimming: read only chunk 1 (page 1 or pages 1-3)\n3. For deep reading: read ALL chunk PDFs sequentially, writing notes after each\n4. Check the manifest to see how many chunks exist\n5. IMPORTANT: Do not skip chunks - methodology and results are in later chunks\n\n### Verify Citations\n\nAfter completing the review, verify all citations are valid:\n\n```bash\npython .claude/skills/literature-review/scripts/verify_citations.py literature_review.md\n```\n\n## Quality Checklist\n\n- [ ] Research question clearly defined\n- [ ] Inclusion/exclusion criteria documented\n- [ ] Multiple sources searched\n- [ ] Search queries logged\n- [ ] Screening decisions recorded\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_5","type":"reasoning","text":"**Setting up environment and directories**"}}
{"type":"item.completed","item":{"id":"item_6","type":"agent_message","text":"I confirmed we’re at the repository root and loaded both skill workflows. Next I’ll perform the required environment setup (`uv venv`, local `pyproject.toml`, dependency install) and then start Phase 1 paper search."}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc \"pwd && uv venv && cat > pyproject.toml << 'EOF'\n[project]\nname = \\\"research-workspace\\\"\nversion = \\\"0.1.0\\\"\ndescription = \\\"Research workspace for experiments\\\"\nrequires-python = \\\">=3.10\\\"\ndependencies = []\n\n[build-system]\nrequires = [\\\"hatchling\\\"]\nbuild-backend = \\\"hatchling.build\\\"\nEOF\npwd && source .venv/bin/activate && uv add pypdf requests arxiv datasets pandas yfinance pyarrow\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc \"pwd && uv venv && cat > pyproject.toml << 'EOF'\n[project]\nname = \\\"research-workspace\\\"\nversion = \\\"0.1.0\\\"\ndescription = \\\"Research workspace for experiments\\\"\nrequires-python = \\\">=3.10\\\"\ndependencies = []\n\n[build-system]\nrequires = [\\\"hatchling\\\"]\nbuild-backend = \\\"hatchling.build\\\"\nEOF\npwd && source .venv/bin/activate && uv add pypdf requests arxiv datasets pandas yfinance pyarrow\"","aggregated_output":"Resolved 67 packages in 181ms\n   Building multitasking==0.0.12\n   Building research-workspace @ file:///data/hypogenicai/workspaces/refine-llm-trading-2354-codex\nDownloading curl-cffi (7.9MiB)\n Downloading curl-cffi\n  × Failed to build `research-workspace @\n  │ file:///data/hypogenicai/workspaces/refine-llm-trading-2354-codex`\n  ├─▶ The build backend returned an error\n  ╰─▶ Call to `hatchling.build.build_editable` failed (exit status: 1)\n\n      [stderr]\n      Traceback (most recent call last):\n        File \"<string>\", line 11, in <module>\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/build.py\",\n      line 83, in build_editable\n          return os.path.basename(next(builder.build(directory=wheel_directory,\n      versions=[\"editable\"])))\n      \n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/plugin/interface.py\",\n      line 157, in build\n          artifact = version_api[version](directory, **build_data)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 522, in build_editable\n          return self.build_editable_detection(directory, **build_data)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 534, in build_editable_detection\n          for included_file in self.recurse_selected_project_files():\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/plugin/interface.py\",\n      line 182, in recurse_selected_project_files\n          if self.config.only_include:\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/data/miniconda3/lib/python3.12/functools.py\", line 995, in\n      __get__\n          val = self.func(instance)\n                ^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/config.py\",\n      line 715, in only_include\n          only_include = only_include_config.get(\"only-include\",\n      self.default_only_include()) or self.packages\n      \n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 268, in default_only_include\n          return self.default_file_selection_options.only_include\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/data/miniconda3/lib/python3.12/functools.py\", line 995, in\n      __get__\n          val = self.func(instance)\n                ^^^^^^^^^^^^^^^^^^^\n        File\n      \"/data/haokunliu/.cache/builds-v0/.tmpZ5cghX/lib/python3.12/site-packages/hatchling/builders/wheel.py\",\n      line 256, in default_file_selection_options\n          raise ValueError(message)\n      ValueError: Unable to determine which files to ship\n      inside the wheel using the following heuristics:\n      https://hatch.pypa.io/latest/plugins/builder/wheel/#default-file-selection\n\n      The most likely cause of this is that there is no directory that matches\n      the name of your project (research_workspace).\n\n      At least one file selection option must be defined\n      in the `tool.hatch.build.targets.wheel` table, see:\n      https://hatch.pypa.io/latest/config/build/\n\n      As an example, if you intend to ship a directory named `foo` that\n      resides within a `src` directory located at the root of your project,\n      you can define the following:\n\n      [tool.hatch.build.targets.wheel]\n      packages = [\"src/foo\"]\n\n      hint: This usually indicates a problem with the package or the build\n      environment.\n  help: If you want to add the package regardless of the failed resolution,\n        provide the `--frozen` flag to skip locking and syncing.\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_8","type":"reasoning","text":"**Adjusting dependency add approach**"}}
{"type":"item.completed","item":{"id":"item_9","type":"agent_message","text":"`uv add` failed because this workspace isn’t structured as a Python package, so I’ll use the instructed fallback (`uv pip install`) and then continue with paper search."}}
{"type":"item.started","item":{"id":"item_10","type":"command_execution","command":"/bin/bash -lc 'pwd && source .venv/bin/activate && uv pip install pypdf requests arxiv datasets pandas yfinance pyarrow'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_10","type":"command_execution","command":"/bin/bash -lc 'pwd && source .venv/bin/activate && uv pip install pypdf requests arxiv datasets pandas yfinance pyarrow'","aggregated_output":"Resolved 60 packages in 49ms\n   Building multitasking==0.0.12\n      Built multitasking==0.0.12\nPrepared 1 package in 638ms\nInstalled 60 packages in 167ms\n + aiohappyeyeballs==2.6.1\n + aiohttp==3.13.3\n + aiosignal==1.4.0\n + annotated-doc==0.0.4\n + anyio==4.12.1\n + arxiv==2.4.0\n + attrs==25.4.0\n + beautifulsoup4==4.14.3\n + certifi==2026.1.4\n + cffi==2.0.0\n + charset-normalizer==3.4.4\n + click==8.3.1\n + curl-cffi==0.13.0\n + datasets==4.5.0\n + dill==0.4.0\n + feedparser==6.0.12\n + filelock==3.24.3\n + frozendict==2.4.7\n + frozenlist==1.8.0\n + fsspec==2025.10.0\n + h11==0.16.0\n + hf-xet==1.2.0\n + httpcore==1.0.9\n + httpx==0.28.1\n + huggingface-hub==1.4.1\n + idna==3.11\n + markdown-it-py==4.0.0\n + mdurl==0.1.2\n + multidict==6.7.1\n + multiprocess==0.70.18\n + multitasking==0.0.12\n + numpy==2.4.2\n + packaging==26.0\n + pandas==3.0.1\n + peewee==4.0.0\n + platformdirs==4.9.2\n + propcache==0.4.1\n + protobuf==6.33.5\n + pyarrow==23.0.1\n + pycparser==3.0\n + pygments==2.19.2\n + pypdf==6.7.1\n + python-dateutil==2.9.0.post0\n + pytz==2025.2\n + pyyaml==6.0.3\n + requests==2.32.5\n + rich==14.3.3\n + sgmllib3k==1.0.0\n + shellingham==1.5.4\n + six==1.17.0\n + soupsieve==2.8.3\n + tqdm==4.67.3\n + typer==0.24.1\n + typer-slim==0.24.0\n + typing-extensions==4.15.0\n + urllib3==2.6.3\n + websockets==16.0\n + xxhash==3.6.0\n + yarl==1.22.0\n + yfinance==1.2.0\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_11","type":"reasoning","text":"**Verifying script path existence**"}}
{"type":"item.completed","item":{"id":"item_12","type":"agent_message","text":"Environment is now ready with required libraries installed. I’ll create the resource folders, run paper-finder in diligent mode, and then download all relevance-qualified papers into `papers/`."}}
{"type":"item.started","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc \"pwd && mkdir -p papers datasets code && pwd && ls -la .claude/skills/paper-finder/scripts && source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \\\"LLM agents long-term trading decisions finance\\\" --mode diligent --format json > paper_search.json && wc -c paper_search.json && sed -n '1,220p' paper_search.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_14","type":"reasoning","text":"**Monitoring command execution**"}}
{"type":"item.completed","item":{"id":"item_15","type":"reasoning","text":"**Confirming service timeout**"}}
{"type":"item.completed","item":{"id":"item_16","type":"agent_message","text":"I’m waiting on the paper-finder call; if the service is unavailable I’ll switch immediately to manual arXiv/Semantic Scholar retrieval and still complete the full deliverables."}}
{"type":"item.completed","item":{"id":"item_17","type":"reasoning","text":"**Terminating unresponsive session**"}}
{"type":"item.completed","item":{"id":"item_18","type":"reasoning","text":"**Checking file creation status**"}}
{"type":"item.started","item":{"id":"item_19","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -l paper_search.json && sed -n '1,220p' paper_search.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_19","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -l paper_search.json && sed -n '1,220p' paper_search.json\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n-rw-rw---- 1 haokunliu haokunliu 0 Feb 22 01:36 paper_search.json\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_20","type":"reasoning","text":"**Testing service response without redirect**"}}
{"type":"item.started","item":{"id":"item_21","type":"command_execution","command":"/bin/bash -lc 'pwd && source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \"LLM agents long-term trading decisions finance\" --mode fast'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_21","type":"command_execution","command":"/bin/bash -lc 'pwd && source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \"LLM agents long-term trading decisions finance\" --mode fast'","aggregated_output":"{\n  \"success\": true,\n  \"total\": 71,\n  \"papers\": [\n    {\n      \"title\": \"StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?\",\n      \"year\": 2025,\n      \"authors\": \"Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:281724665\",\n      \"relevance\": 3,\n      \"abstract\": \"Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.\",\n      \"citations\": 7\n    },\n    {\n      \"title\": \"FinPos: A Position-Aware Trading Agent System for Real Financial Markets\",\n      \"year\": 2025,\n      \"authors\": \"Bijia Liu, Ronghao Dang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282719341\",\n      \"relevance\": 3,\n      \"abstract\": \"The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, most existing trading agents operate under intraday, independent unit-based trading tasks, where decisions are made as isolated directional actions, and thus lack awareness of continuous position management. Therefore, we propose a position-aware trading task designed to simulate a more realistic market. To address this task, we propose FinPos, a position-aware trading agent system designed to explicitly model and manage continuous positions. FinPos enhances position awareness through three key mechanisms: (1) professional-level interpretation of heterogeneous market information; (2) a dual-agent decision structure that separates directional reasoning from risk-aware position adjustment; and (3) multi-timescale reward signals, allowing the agent to internalize position awareness through experiential feedback rather than static instructions alone. Extensive experiments demonstrate that FinPos surpasses state-of-the-art trading agents in the position-aware trading task, which closely mirrors real market conditions. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-term market decision-making.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"Hierarchical Organization Simulacra in the Investment Sector\",\n      \"year\": 2024,\n      \"authors\": \"Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:273022534\",\n      \"relevance\": 3,\n      \"abstract\": \"This paper explores designing artificial organizations with professional behavior in investments using a multi-agent simulation. The method mimics hierarchical decision-making in investment firms, using news articles to inform decisions. A large-scale study analyzing over 115,000 news articles of 300 companies across 15 years compared this approach against professional traders' decisions. Results show that hierarchical simulations align closely with professional choices, both in frequency and profitability. However, the study also reveals biases in decision-making, where changes in prompt wording and perceived agent seniority significantly influence outcomes. This highlights both the potential and limitations of large language models in replicating professional financial decision-making.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?\",\n      \"year\": 2025,\n      \"authors\": \"Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278501425\",\n      \"relevance\": 3,\n      \"abstract\": \"Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.\",\n      \"citations\": 8\n    },\n    {\n      \"title\": \"HedgeAgents: A Balanced-aware Multi-agent Financial Trading System\",\n      \"year\": 2025,\n      \"authors\": \"Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276449781\",\n      \"relevance\": 3,\n      \"abstract\": \"As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ''hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).\",\n      \"citations\": 23\n    },\n    {\n      \"title\": \"Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance\",\n      \"year\": 2026,\n      \"authors\": \"Mostapha Benhenda\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:284911867\",\n      \"relevance\": 3,\n      \"abstract\": \"We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation\",\n      \"year\": 2026,\n      \"authors\": \"Zeping Li, Guancheng Wan, Keyang Chen, Yu Chen, Yiwen Zhao, Philip Torr, Guangnan Ye, Zhenfei Yin, Hongfeng Chai\",\n      \"url\": \"https://www.semanticscholar.org/paper/7550169d076668ede36673525ad2b96baa54fe0a\",\n      \"relevance\": 3,\n      \"abstract\": \"Recent works have increasingly applied Large Language Models (LLMs) as agents in financial stock market simulations to test if micro-level behaviors aggregate into macro-level phenomena. However, a crucial question arises: Do LLM agents'behaviors align with real market participants? This alignment is key to the validity of simulation results. To explore this, we select a financial stock market scenario to test behavioral consistency. Investors are typically classified as fundamental or technical traders, but most simulations fix strategies at initialization, failing to reflect real-world trading dynamics. In this work, we assess whether agents'strategy switching aligns with financial theory, providing a framework for this evaluation. We operationalize four behavioral-finance drivers-loss aversion, herding, wealth differentiation, and price misalignment-as personality traits set via prompting and stored long-term. In year-long simulations, agents process daily price-volume data, trade under a designated style, and reassess their strategy every 10 trading days. We introduce four alignment metrics and use Mann-Whitney U tests to compare agents'style-switching behavior with financial theory. Our results show that recent LLMs'switching behavior is only partially consistent with behavioral-finance theories, highlighting the need for further refinement in aligning agent behavior with financial theory.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design\",\n      \"year\": 2023,\n      \"authors\": \"Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Jordan W. Suchow, Denghui Zhang, K. Khashanah\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:265445755\",\n      \"relevance\": 1,\n      \"abstract\": \"We introduce FinMem, a novel Large Language Models (LLM)-based agent framework for financial trading, designed to address the need for automated systems that can transform real-time data into executable decisions. FinMem comprises three core modules: Profile for customizing agent characteristics, Memory for hierarchical financial data assimilation, and Decision-making for converting insights into investment choices. The Memory module, which mimics human traders\\u2019 cognitive structure, offers interpretability and real-time tuning while handling the critical timing of various information types. It employs a layered approach to process and prioritize data based on its timeliness and relevance, ensuring that the most recent and impactful information is given appropriate weight in decision-making. FinMem\\u2019s adjustable cognitive span allows retention of critical information beyond human limits, enabling it to balance historical patterns with current market dynamics. This framework facilitates self-evolution of professional knowledge, agile reactions to investment cues, and continuous refinement of trading decisions in financial environments. When compared against advanced algorithmic agents using a large-scale real-world financial dataset, FinMem demonstrates superior performance across classic metrics like Cumulative Return and Sharpe ratio. Further tuning of the agent\\u2019s perceptual span and character setting enhances its trading performance, positioning FinMem as a cutting-edge solution for automated trading.\",\n      \"citations\": 148\n    },\n    {\n      \"title\": \"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist\",\n      \"year\": 2024,\n      \"authors\": \"Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:268041696\",\n      \"relevance\": 1,\n      \"abstract\": \"Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.\",\n      \"citations\": 126\n    },\n    {\n      \"title\": \"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making\",\n      \"year\": 2024,\n      \"authors\": \"Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Rong Liu, Zhenyu Cui, Denghui Zhang, K. Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:271064881\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management.\",\n      \"citations\": 97\n    },\n    {\n      \"title\": \"TradingAgents: Multi-Agents LLM Financial Trading Framework\",\n      \"year\": 2024,\n      \"authors\": \"Yijia Xiao, Edward Sun, Di Luo, Wei Wang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:275133732\",\n      \"relevance\": 1,\n      \"abstract\": \"Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, the multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. TradingAgents is available at https://github.com/TauricResearch/TradingAgents.\",\n      \"citations\": 92\n    },\n    {\n      \"title\": \"A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges\",\n      \"year\": 2024,\n      \"authors\": \"Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Poor, Qingsong Wen, Stefan Zohren\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:270562262\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.\",\n      \"citations\": 126\n    },\n    {\n      \"title\": \"TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance\",\n      \"year\": 2023,\n      \"authors\": \"Yang Li, Yangyang Yu, Haohang Li, Z. Chen, K. Khashanah\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:261582775\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs), prominently highlighted by the recent evolution in the Generative Pre-trained Transformers (GPT) series, have displayed significant prowess across various domains, such as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of GPTs lies in their ability to decode human instructions, achieved through comprehensively processing historical inputs as an entirety within their memory system. Yet, the memory processing of GPTs does not precisely emulate the hierarchical nature of human memory. This can result in LLMs struggling to prioritize immediate and critical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework endowed with layered memories. We assert that this framework is well-suited for stock and fund trading, where the extraction of highly relevant insights from hierarchical financial data is imperative to inform trading decisions. Within this framework, one agent organizes memory into three distinct layers, each governed by a custom decay mechanism, aligning more closely with human cognitive processes. Agents can also engage in inter-agent debate. In financial trading contexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system to integrate multi-source historical actions and market insights. This equips them to navigate financial changes, formulate strategies, and debate with peer agents about investment decisions. Another standout feature of our approach is to equip agents with individualized trading traits, enhancing memory diversity and decision robustness. These sophisticated designs boost the system's responsiveness to historical trades and real-time market signals, ensuring superior automated trading accuracy.\",\n      \"citations\": 86\n    },\n    {\n      \"title\": \"Large Language Model Agent in Financial Trading: A Survey\",\n      \"year\": 2024,\n      \"authors\": \"Han Ding, Yinheng Li, Junhao Wang, Hang Chen\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:271860230\",\n      \"relevance\": 1,\n      \"abstract\": \"Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.\",\n      \"citations\": 53\n    },\n    {\n      \"title\": \"INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent\",\n      \"year\": 2024,\n      \"authors\": \"Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, K. Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, Jordan W. Suchow\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:274992211\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \\\\textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.\",\n      \"citations\": 26\n    },\n    {\n      \"title\": \"From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review\",\n      \"year\": 2025,\n      \"authors\": \"M. Ferrag, Norbert Tihanyi, M. Debbah\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278165282\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.\",\n      \"citations\": 97\n    },\n    {\n      \"title\": \"Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations\",\n      \"year\": 2025,\n      \"authors\": \"Alejandro Lopez-Lira\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:277787336\",\n      \"relevance\": 1,\n      \"abstract\": \"This paper presents a realistic simulated stock market where large language models (LLMs) act as heterogeneous competing trading agents. The open-source framework incorporates a persistent order book with market and limit orders, partial fills, dividends, and equilibrium clearing alongside agents with varied strategies, information sets, and endowments. Agents submit standardized decisions using structured outputs and function calls while expressing their reasoning in natural language. Three findings emerge: First, LLMs demonstrate consistent strategy adherence and can function as value investors, momentum traders, or market makers per their instructions. Second, market dynamics exhibit features of real financial markets, including price discovery, bubbles, underreaction, and strategic liquidity provision. Third, the framework enables analysis of LLMs' responses to varying market conditions, similar to partial dependence plots in machine-learning interpretability. The framework allows simulating financial theories without closed-form solutions, creating experimental designs that would be costly with human participants, and establishing how prompts can generate correlated behaviors affecting market stability.\",\n      \"citations\": 9\n    },\n    {\n      \"title\": \"From Deep Learning to LLMs: A survey of AI in Quantitative Investment\",\n      \"year\": 2025,\n      \"authors\": \"Bokai Cao, Sai Wang, Xinyi Lin, Xiaojun Wu, Haohan Zhang, Lionel M. Ni, Jian Guo\",\n      \"url\": \"https://www.semanticscholar.org/paper/a357eb33b3694f1e8316690b2a47c5e9f62203ce\",\n      \"relevance\": 1,\n      \"abstract\": \"Quantitative investment (quant) is an emerging, technology-driven approach in asset management, increasingy shaped by advancements in artificial intelligence. Recent advances in deep learning and large language models (LLMs) for quant finance have improved predictive modeling and enabled agent-based automation, suggesting a potential paradigm shift in this field. In this survey, taking alpha strategy as a representative example, we explore how AI contributes to the quantitative investment pipeline. We first examine the early stage of quant research, centered on human-crafted features and traditional statistical models with an established alpha pipeline. We then discuss the rise of deep learning, which enabled scalable modeling across the entire pipeline from data processing to order execution. Building on this, we highlight the emerging role of LLMs in extending AI beyond prediction, empowering autonomous agents to process unstructured data, generate alphas, and support self-iterative workflows.\",\n      \"citations\": 12\n    },\n    {\n      \"title\": \"FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading\",\n      \"year\": 2025,\n      \"authors\": \"Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb E Smith, Xiao-Yang Liu, Jimin Huang, Sophia Ananiadou, Qianqian Xie\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276408244\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \\\\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.\",\n      \"citations\": 17\n    },\n    {\n      \"title\": \"FinVision: A Multi-Agent Framework for Stock Market Prediction\",\n      \"year\": 2024,\n      \"authors\": \"Sorouralsadat Fatemi, Yuheng Hu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:274023255\",\n      \"relevance\": 1,\n      \"abstract\": \"Financial trading has been a challenging task, as it requires the integration of vast amounts of data from various modalities. Traditional deep learning and reinforcement learning methods require large training data and often involve encoding various data types into numerical formats for model input, which limits the explainability of model behavior. Recently, LLM-based agents have demonstrated remarkable advancements in handling multi-modal data, enabling them to execute complex, multi-step decision-making tasks while providing insights into their thought processes. This research introduces a multi-modal multi-agent system designed specifically for financial trading tasks. Our framework employs a team of specialized LLM-based agents, each adept at processing and interpreting various forms of financial data, such as textual news reports, candlestick charts, and trading signal charts. A key feature of our approach is the integration of a reflection module, which conducts analyses of historical trading signals and their outcomes. This reflective process is instrumental in enhancing the decision-making capabilities of the system for future trading scenarios. Furthermore, the ablation studies indicate that the visual reflection module plays a crucial role in enhancing the decision-making capabilities of our framework.\",\n      \"citations\": 19\n    },\n    {\n      \"title\": \"When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents\",\n      \"year\": 2025,\n      \"authors\": \"Lingfei Qian, Xueqing Peng, Yan Wang, Vincent J. Zhang, Huan He, Hanley Smith, Yi Han, Yueru He, Haohang Li, Yupeng Cao, Yangyang Yu, Alejandro Lopez-Lira, Peng Lu, Jian-yun Nie, Guojun Xiong, Jimin Huang, Sophia Ananiadou\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282057127\",\n      \"relevance\": 1,\n      \"abstract\": \"Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions\",\n      \"year\": 2025,\n      \"authors\": \"Dimitrios Emmanoulopoulos, Ollie Olby, Justin Lyon, Namid R Stillman\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280281425\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) are increasingly deployed in agentic frameworks, in which prompts trigger complex tool-based analysis in pursuit of a goal. While these frameworks have shown promise across multiple domains including in finance, they typically lack a principled model-building step, relying instead on sentiment- or trend-based analysis. We address this gap by developing an agentic system that uses LLMs to iteratively discover stochastic differential equations for financial time series. These models generate risk metrics which inform daily trading decisions. We evaluate our system in both traditional backtests and using a market simulator, which introduces synthetic but causally plausible price paths and news events. We find that model-informed trading strategies outperform standard LLM-based agents, improving Sharpe ratios across multiple equities. Our results show that combining LLMs with agentic model discovery enhances market risk estimation and enables more profitable trading decisions.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"Standard Benchmarks Fail -- Auditing LLM Agents in Finance Must Prioritize Risk\",\n      \"year\": 2025,\n      \"authors\": \"Zichen Chen, Jiaao Chen, Jianda Chen, Misha Sra\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276575244\",\n      \"relevance\": 1,\n      \"abstract\": \"Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.\",\n      \"citations\": 5\n    },\n    {\n      \"title\": \"Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of Bilateral Financial Exchanges, A bond market study\",\n      \"year\": 2025,\n      \"authors\": \"Alicia Vidler, Toby Walsh\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276741474\",\n      \"relevance\": 1,\n      \"abstract\": \"Bilateral markets, such as those for government bonds, involve decentralized and opaque transactions between market makers (MMs) and clients, posing significant challenges for traditional modeling approaches. To address these complexities, we introduce TRIBE an agent-based model (ABM) augmented with a large language model (LLM) to simulate human-like decision-making in trading environments. TRIBE leverages publicly available data and stylized facts to capture realistic trading dynamics, integrating human biases like risk aversion and ambiguity sensitivity into the decision-making processes of agents. Our research yields three key contributions: first, we demonstrate that integrating LLMs into ABMs, to enhance client agency, is feasible and enriches the simulation of agent behaviors in complex markets; second, we find that even slight trade aversion encoded within the LLM leads to a complete cessation of trading activity, highlighting the sensitivity of market dynamics to agents' risk profiles; third, we show that incorporating human-like variability shifts power dynamics towards clients and can disproportionately affect the entire system, often resulting in systemic agent collapse across simulations. These findings underscore the emergent properties that arise when introducing stochastic, human-like decision processes, revealing new system behaviors that enhance the realism and complexity of artificial trading societies.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis\",\n      \"year\": 2025,\n      \"authors\": \"Feng Tian, Flora D. Salim, Hao Xue\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280711571\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advancements in large language models (LLMs) have enabled powerful agent-based applications in finance, particularly for sentiment analysis, financial report comprehension, and stock forecasting. However, existing systems often lack inter-agent coordination, structured self-reflection, and access to high-quality, domain-specific post-training data such as data from trading activities including both market conditions and agent decisions. These data are crucial for agents to understand the market dynamics, improve the quality of decision-making and promote effective coordination. We introduce TradingGroup, a multi-agent trading system designed to address these limitations through a self-reflective architecture and an end-to-end data-synthesis pipeline. TradingGroup consists of specialized agents for news sentiment analysis, financial report interpretation, stock trend forecasting, trading style adaptation, and a trading decision making agent that merges all signals and style preferences to produce buy, sell or hold decisions. Specifically, we design self-reflection mechanisms for the stock forecasting, style, and decision-making agents to distill past successes and failures for similar reasoning in analogous future scenarios and a dynamic risk-management model to offer configurable dynamic stop-loss and take-profit mechanisms. In addition, TradingGroup embeds an automated data-synthesis and annotation pipeline that generates high-quality post-training data for further improving the agent performance through post-training. Our backtesting experiments across five real-world stock datasets demonstrate TradingGroup's superior performance over rule-based, machine learning, reinforcement learning, and existing LLM-based trading strategies.\",\n      \"citations\": 3\n    },\n    {\n      \"title\": \"Integrating Large Language Models in Financial Investments and Market Analysis: A Survey\",\n      \"year\": 2025,\n      \"authors\": \"Sedigheh Mahdavi, Jiating Chen, Pradeep Kumar Joshi, L. H. Guativa, Upmanyu Singh\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278900379\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs) have been employed in financial decision making, enhancing analytical capabilities for investment strategies. Traditional investment strategies often utilize quantitative models, fundamental analysis, and technical indicators. However, LLMs have introduced new capabilities to process and analyze large volumes of structured and unstructured data, extract meaningful insights, and enhance decision-making in real-time. This survey provides a structured overview of recent research on LLMs within the financial domain, categorizing research contributions into four main frameworks: LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and Adaptation Approaches, and Agent-Based Architectures. This study provides a structured review of recent LLMs research on applications in stock selection, risk assessment, sentiment analysis, trading, and financial forecasting. By reviewing the existing literature, this study highlights the capabilities, challenges, and potential directions of LLMs in financial markets.\",\n      \"citations\": 5\n    },\n    {\n      \"title\": \"AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets\",\n      \"year\": 2025,\n      \"authors\": \"Tianyu Fan, Yuhao Yang, Ya Jiang, Yifei Zhang, Yuxuan Chen, Chao Huang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:283883436\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"LiveTradeBench: Seeking Real-World Alpha with Large Language Models\",\n      \"year\": 2025,\n      \"authors\": \"Haofei Yu, Fenghai Li, Jiaxuan You\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282758249\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.\",\n      \"citations\": 3\n    },\n    {\n      \"title\": \"FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment\",\n      \"year\": 2025,\n      \"authors\": \"Wentao Zhang, Yilei Zhao, Chuqiao Zong, Xinrun Wang, Bo An\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280421572\",\n      \"relevance\": 1,\n      \"abstract\": \"Financial AI holds great promise for transforming modern finance, with the potential to support a wide range of tasks such as market forecasting, portfolio management, quantitative trading, and automated analysis. However, existing platforms remain limited in task coverage, lack robust multimodal data integration, and offer insufficient support for the training and deployment of large language models (LLMs). In response to these limitations, we present FinWorld, an all-in-one open-source platform that provides end-to-end support for the entire financial AI workflow, from data acquisition to experimentation and deployment. FinWorld distinguishes itself through native integration of heterogeneous financial data, unified support for diverse AI paradigms, and advanced agent automation, enabling seamless development and deployment. Leveraging data from 2 representative markets, 4 stock pools, and over 800 million financial data points, we conduct comprehensive experiments on 4 key financial AI tasks. These experiments systematically evaluate deep learning and reinforcement learning algorithms, with particular emphasis on RL-based finetuning for LLMs and LLM Agents. The empirical results demonstrate that FinWorld significantly enhances reproducibility, supports transparent benchmarking, and streamlines deployment, thereby providing a strong foundation for future research and real-world applications. Code is available at Github~\\\\footnote{https://github.com/DVampire/FinWorld}.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking\",\n      \"year\": 2025,\n      \"authors\": \"Changlun Li, Yao Shi, Chen Wang, Qiqi Duan, Runke Ruan, Weijie Huang, Haonan Long, Lijun Huang, Yuyu Luo, Nan Tang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278715123\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in managing complex fund investment remains inadequately assessed. A fundamental limitation of existing benchmarks for evaluating LLM-driven trading strategies is their reliance on historical back-testing, inadvertently enabling LLMs to\\\"time travel\\\"-leveraging future information embedded in their training corpora, thus resulting in possible information leakage and overly optimistic performance estimates. To address this issue, we introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate LLM in real-time market conditions. Utilizing a multi-agent architecture, DeepFund connects directly with real-time stock market data-specifically data published after each model pretraining cutoff-to ensure fair and leakage-free evaluations. Empirical tests on nine flagship LLMs from leading global institutions across multiple investment dimensions-including ticker-level analysis, investment decision-making, portfolio management, and risk control-reveal significant practical challenges. Notably, even cutting-edge models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses within DeepFund real-time evaluation environment, underscoring the present limitations of LLMs for active fund management. Our code is available at https://github.com/HKUSTDial/DeepFund.\",\n      \"citations\": 6\n    },\n    {\n      \"title\": \"FINRS: A Risk-Sensitive Trading Framework for Real Financial Markets\",\n      \"year\": 2025,\n      \"authors\": \"Bijia Liu, Ronghao Dang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:283072776\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) have shown strong reasoning capabilities and are increasingly explored for financial trading. Existing LLM-based trading agents, however, largely focus on single-step prediction and lack integrated mechanisms for risk management, which reduces their effectiveness in volatile markets. We introduce FinRS, a risk-sensitive trading framework that combines hierarchical market analysis, dual-decision agents, and multi-timescale reward reflection to align trading actions with both return objectives and downside risk constraints. Experiments on multiple stocks and market conditions show that FinRS achieves superior profitability and stability compared to state-of-the-art methods.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"Bridging Language Models and Financial Analysis\",\n      \"year\": 2025,\n      \"authors\": \"Alejandro Lopez-Lira, Jihoon Kwon, Sangwoong Yoon, Jy-yong Sohn, Chanyeol Choi\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:277451826\",\n      \"relevance\": 1,\n      \"abstract\": \"The rapid advancements in Large Language Models (LLMs) have unlocked transformative possibilities in natural language processing, particularly within the financial sector. Financial data is often embedded in intricate relationships across textual content, numerical tables, and visual charts, posing challenges that traditional methods struggle to address effectively. However, the emergence of LLMs offers new pathways for processing and analyzing this multifaceted data with increased efficiency and insight. Despite the fast pace of innovation in LLM research, there remains a significant gap in their practical adoption within the finance industry, where cautious integration and long-term validation are prioritized. This disparity has led to a slower implementation of emerging LLM techniques, despite their immense potential in financial applications. As a result, many of the latest advancements in LLM technology remain underexplored or not fully utilized in this domain. This survey seeks to bridge this gap by providing a comprehensive overview of recent developments in LLM research and examining their applicability to the financial sector. Building on previous survey literature, we highlight several novel LLM methodologies, exploring their distinctive capabilities and their potential relevance to financial data analysis. By synthesizing insights from a broad range of studies, this paper aims to serve as a valuable resource for researchers and practitioners, offering direction on promising research avenues and outlining future opportunities for advancing LLM applications in finance.\",\n      \"citations\": 5\n    },\n    {\n      \"title\": \"MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading\",\n      \"year\": 2025,\n      \"authors\": \"Siyi Wu, Zhaoyang Guan, Leyi Zhao, Xinyuan Song, Xinyu Ying, Hanlin Zhang, Michele Pak, Yangfan He, Yi Xin, Jianhui Wang, Tianyu Shi\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280322773\",\n      \"relevance\": 1,\n      \"abstract\": \"Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \\\\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges\",\n      \"year\": 2025,\n      \"authors\": \"Liyuan Chen, Shuoling Liu, Jiangpeng Yan, Xiaoyu Wang, Henglin Liu, Chuang Li, Kechen Jiao, Jixuan Ying, Yang Liu, Qiang Yang, Xiu Li\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280017319\",\n      \"relevance\": 1,\n      \"abstract\": \"\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis\",\n      \"year\": 2025,\n      \"authors\": \"Chen-Che Lu, Yun-Cheng Chou, Teng-Ruei Chen\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282389311\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advances in large language models (LLMs) have enabled multi-agent reasoning systems capable of collaborative decision-making. However, in financial analysis, most frameworks remain narrowly focused on either isolated single-agent predictors or loosely connected analyst ensembles, and they lack a coherent reasoning workflow that unifies diverse data modalities. We introduce P1GPT, a layered multi-agent LLM framework for multi-modal financial information analysis and interpretable trading decision support. Unlike prior systems that emulate trading teams through role simulation, P1GPT implements a structured reasoning pipeline that systematically fuses technical, fundamental, and news-based insights through coordinated agent communication and integration-time synthesis. Backtesting on multi-modal datasets across major U.S. equities demonstrates that P1GPT achieves superior cumulative and risk-adjusted returns, maintains low drawdowns, and provides transparent causal rationales. These findings suggest that structured reasoning workflows, rather than agent role imitation, offer a scalable path toward explainable and trustworthy financial AI systems.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"A Survey of Financial AI: Architectures, Advances and Open Challenges\",\n      \"year\": 2024,\n      \"authors\": \"Junhua Liu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:274149777\",\n      \"relevance\": 1,\n      \"abstract\": \"Financial AI empowers sophisticated approaches to financial market forecasting, portfolio optimization, and automated trading. This survey provides a systematic analysis of these developments across three primary dimensions: predictive models that capture complex market dynamics, decision-making frameworks that optimize trading and investment strategies, and knowledge augmentation systems that leverage unstructured financial information. We examine significant innovations including foundation models for financial time series, graph-based architectures for market relationship modeling, and hierarchical frameworks for portfolio optimization. Analysis reveals crucial trade-offs between model sophistication and practical constraints, particularly in high-frequency trading applications. We identify critical gaps and open challenges between theoretical advances and industrial implementation, outlining open challenges and opportunities for improving both model performance and practical applicability.\",\n      \"citations\": 10\n    },\n    {\n      \"title\": \"A Survey on the Optimization of Large Language Model-based Agents\",\n      \"year\": 2025,\n      \"authors\": \"Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang He\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:277065966\",\n      \"relevance\": 1,\n      \"abstract\": \"With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness in complex agent-related environments. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective remains lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the evaluation for agents, review key applications of LLM-based agents, and discuss the major challenges and promising future directions. A curated collection of the surveyed works is provided at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.\",\n      \"citations\": 21\n    },\n    {\n      \"title\": \"Large Language Models in equity markets: applications, techniques, and insights\",\n      \"year\": 2025,\n      \"authors\": \"Aakanksha Jadhav, Vishal Mirza\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280955402\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent breakthroughs in Large Language Models (LLMs) have the potential to disrupt equity investing by enabling sophisticated data analysis, market prediction, and automated trading. This paper presents a comprehensive review of 84 research studies conducted between 2022 and early 2025, synthesizing the state of LLM applications in stock investing. We provide a dual-layered categorization: first, by financial applications such as stock price forecasting, sentiment analysis, portfolio management, and algorithmic trading; second, by technical methodologies, including prompting, fine-tuning, multi-agent frameworks, reinforcement learning, and custom architectures. Additionally, we consolidate findings on the datasets used, ranging from financial statements to multimodal data (news, market trends, earnings transcripts, social media), and systematically compare general-purpose vs. finance-specialized LLMs used in research. Our analysis identifies key research trends, commonalities, and divergences across studies, evaluating both their empirical contributions and methodological innovations. We highlight the strengths of existing research, such as improved sentiment extraction and the use of reinforcement learning to factor market feedback, alongside critical gaps in scalability, interpretability, and real-world validation. Finally, we propose directions for future research, emphasizing hybrid modeling approaches, architectures that factor reasoning and large context windows, and robust evaluation frameworks to advance AI-driven financial strategies. By mapping the intersection of LLMs and equity markets, this review provides a foundation and roadmap for future research and practical implementation in the financial sector.\",\n      \"citations\": 10\n    },\n    {\n      \"title\": \"LLM-Powered AI Agent Systems and Their Applications in Industry\",\n      \"year\": 2025,\n      \"authors\": \"Guannan Liang, Qianqian Tong\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278789054\",\n      \"relevance\": 1,\n      \"abstract\": \"The emergence of Large Language Models (LLMs) has reshaped agent systems. Unlike traditional rule-based agents with limited task scope, LLM-powered agents offer greater flexibility, cross-domain reasoning, and natural language interaction. Moreover, with the integration of multi-modal LLMs, current agent systems are highly capable of processing diverse data modalities, including text, images, audio, and structured tabular data, enabling richer and more adaptive real-world behavior. This paper comprehensively examines the evolution of agent systems from the pre-LLM era to current LLM-powered architectures. We categorize agent systems into software-based, physical, and adaptive hybrid systems, highlighting applications across customer service, software development, manufacturing automation, personalized education, financial trading, and healthcare. We further discuss the primary challenges posed by LLM-powered agents, including high inference latency, output uncertainty, lack of evaluation metrics, and security vulnerabilities, and propose potential solutions to mitigate these concerns.\",\n      \"citations\": 18\n    },\n    {\n      \"title\": \"When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments\",\n      \"year\": 2024,\n      \"authors\": \"Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhengting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li, Mengnan Du, Yongfeng Zhang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:271533952\",\n      \"relevance\": 1,\n      \"abstract\": \"Can AI Agents simulate real-world trading environments to investigate the impact of external factors on stock trading activities (e.g., macroeconomics, policy changes, company fundamentals, and global events)? These factors, which frequently influence trading behaviors, are critical elements in the quest for maximizing investors' profits. Our work attempts to solve this problem through large language model based agents. We have developed a multi-agent AI system called StockAgent, driven by LLMs, designed to simulate investors' trading behaviors in response to the real stock market. The StockAgent allows users to evaluate the impact of different external factors on investor trading and to analyze trading behavior and profitability effects. Additionally, StockAgent avoids the test set leakage issue present in existing trading simulation systems based on AI Agents. Specifically, it prevents the model from leveraging prior knowledge it may have acquired related to the test data. We evaluate different LLMs under the framework of StockAgent in a stock trading environment that closely resembles real-world conditions. The experimental results demonstrate the impact of key external factors on stock market trading, including trading behavior and stock price fluctuation rules. This research explores the study of agents' free trading gaps in the context of no prior knowledge related to market data. The patterns identified through StockAgent simulations provide valuable insights for LLM-based investment advice and stock recommendation. The code is available at https://github.com/MingyuJ666/Stockagent.\",\n      \"citations\": 23\n    },\n    {\n      \"title\": \"QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model\",\n      \"year\": 2024,\n      \"authors\": \"Sai Wang, Hang Yuan, Lionel M. Ni, Jian Guo\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:267499667\",\n      \"relevance\": 1,\n      \"abstract\": \"Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts.\",\n      \"citations\": 26\n    },\n    {\n      \"title\": \"TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?\",\n      \"year\": 2025,\n      \"authors\": \"Lewen Yan, Jilin Mei, Tianyi Zhou, Lige Huang, Jie Zhang, Dongrui Liu, Jing Shao\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:283458569\",\n      \"relevance\": 1,\n      \"abstract\": \"LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading\",\n      \"year\": 2025,\n      \"authors\": \"Zifan Song, Kaitao Song, Guosheng Hu, Ding Qi, Junyao Gao, Xiaohua Wang, Dongsheng Li, Cairong Zhao\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:281842508\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advancements in large language models (LLMs) and agentic systems have shown exceptional decision-making capabilities, revealing significant potential for autonomic finance. Current financial trading agents predominantly simulate anthropomorphic roles that inadvertently introduce emotional biases and rely on peripheral information, while being constrained by the necessity for continuous inference during deployment. In this paper, we pioneer the harmonization of strategic depth in agents with the mechanical rationality essential for quantitative trading. Consequently, we present TiMi (Trade in Minutes), a rationality-driven multi-agent system that architecturally decouples strategy development from minute-level deployment. TiMi leverages specialized LLM capabilities of semantic analysis, code programming, and mathematical reasoning within a comprehensive policy-optimization-deployment chain. Specifically, we propose a two-tier analytical paradigm from macro patterns to micro customization, layered programming design for trading bot implementation, and closed-loop optimization driven by mathematical reflection. Extensive evaluations across 200+ trading pairs in stock and cryptocurrency markets empirically validate the efficacy of TiMi in stable profitability, action efficiency, and risk control under volatile market dynamics.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Language Model Guided Reinforcement Learning in Quantitative Trading\",\n      \"year\": 2025,\n      \"authors\": \"Adam Darmanin, Vince Vella\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280421329\",\n      \"relevance\": 1,\n      \"abstract\": \"Algorithmic trading requires short-term tactical decisions consistent with long-term financial objectives. Reinforcement Learning (RL) has been applied to such problems, but adoption is limited by myopic behaviour and opaque policies. Large Language Models (LLMs) offer complementary strategic reasoning and multi-modal signal interpretation when guided by well-structured prompts. This paper proposes a hybrid framework in which LLMs generate high-level trading strategies to guide RL agents. We evaluate (i) the economic rationale of LLM-generated strategies through expert review, and (ii) the performance of LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and Maximum Drawdown (MDD). Empirical results indicate that LLM guidance improves both return and risk metrics relative to standard RL.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Agent-Based Simulation of a Financial Market with Large Language Models\",\n      \"year\": 2025,\n      \"authors\": \"Ryuji Hashimoto, Takehiro Takayanagi, Masahiro Suzuki, Kiyoshi Izumi\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282064483\",\n      \"relevance\": 1,\n      \"abstract\": \"In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents'behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective\",\n      \"year\": 2025,\n      \"authors\": \"Changlun Li, Yao Shi, Yuyu Luo, Nan Tang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:277272186\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs'understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs'performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes Under Herd Behavior\",\n      \"year\": 2025,\n      \"authors\": \"Huisheng Wang, Zhuoshi Pan, Hangjing Zhang, Mingxiao Liu, Hanqing Gao, \\u2020. H.VickyZhao, Samuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman, Prashant Reddy, Manuela Veloso. 2020, Christopher Avery, Peter Zemsky. 1998, Multidimensional, Yuntao Bai, Andy Jones, Kamal K. Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil\\u02d9e Luko\\u0161i\\u00afut\\u02d9e, Hong-you Chen, Cheng-Hao Tu, Ziwei Li, Han-Wei Shen, Wei-Lun Chao, Team Glm, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, L. Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, S. Cao, Weng Shuxun Yang, Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, A. Schelten, Edward J. Hu, Yelong Shen, Zeyuan Phillip Wallis, An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Shujuan Zhao, Lingfeng Qiao, Kangyang Luo, Qianwen Zhang, Junru Lu, Di Yin. 2024, SNFinLLM, Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280034807\",\n      \"relevance\": 1,\n      \"abstract\": \"Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading\",\n      \"year\": 2025,\n      \"authors\": \"Zheye Deng, Jiashu Wang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282139067\",\n      \"relevance\": 1,\n      \"abstract\": \"While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination\",\n      \"year\": 2025,\n      \"authors\": \"Charidimos Papadakis, Angeliki Dimitriou, Giorgos Filandrianos, Maria Lymperaiou, Konstantinos Thomas, G. Stamou\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282209036\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs\",\n      \"year\": 2025,\n      \"authors\": \"Fangyi Yu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280527079\",\n      \"relevance\": 1,\n      \"abstract\": \"As large language models (LLMs) grow in capability and autonomy, evaluating their outputs-especially in open-ended and complex tasks-has become a critical bottleneck. A new paradigm is emerging: using AI agents as the evaluators themselves. This\\\"agent-as-a-judge\\\"approach leverages the reasoning and perspective-taking abilities of LLMs to assess the quality and safety of other models, promising calable and nuanced alternatives to human evaluation. In this review, we define the agent-as-a-judge concept, trace its evolution from single-model judges to dynamic multi-agent debate frameworks, and critically examine their strengths and shortcomings. We compare these approaches across reliability, cost, and human alignment, and survey real-world deployments in domains such as medicine, law, finance, and education. Finally, we highlight pressing challenges-including bias, robustness, and meta evaluation-and outline future research directions. By bringing together these strands, our review demonstrates how agent-based judging can complement (but not replace) human oversight, marking a step toward trustworthy, scalable evaluation for next-generation LLMs.\",\n      \"citations\": 5\n    },\n    {\n      \"title\": \"StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets\",\n      \"year\": 2025,\n      \"authors\": \"Charidimos Papadakis, Giorgos Filandrianos, Angeliki Dimitriou, Maria Lymperaiou, Konstantinos Thomas, G. Stamou\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280265856\",\n      \"relevance\": 1,\n      \"abstract\": \"We present StockSim, an open-source simulation platform for systematic evaluation of large language models (LLMs) in realistic financial decision-making scenarios. Unlike previous toolkits that offer limited scope, StockSim delivers a comprehensive system that fully models market dynamics and supports diverse simulation modes of varying granularity. It incorporates critical real-world factors, such as latency, slippage, and order-book microstructure, that were previously neglected, enabling more faithful and insightful assessment of LLM-based trading agents. An extensible, role-based agent framework supports heterogeneous trading strategies and multi-agent coordination, making StockSim a uniquely capable testbed for NLP research on reasoning under uncertainty and sequential decision-making. We open-source all our code at https: //github.com/harrypapa2002/StockSim.\",\n      \"citations\": 5\n    },\n    {\n      \"title\": \"Leveraging LLMS for Top-Down Sector Allocation In Automated Trading\",\n      \"year\": 2025,\n      \"authors\": \"Ryan Quek Wei Heng, Edoardo Vittori, Keane Ong, Rui Mao, Erik Cambria, G. Mengaldo\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276961799\",\n      \"relevance\": 1,\n      \"abstract\": \"This paper introduces a methodology leveraging Large Language Models (LLMs) for sector-level portfolio allocation through systematic analysis of macroeconomic conditions and market sentiment. Our framework emphasizes top-down sector allocation by processing multiple data streams simultaneously, including policy documents, economic indicators, and sentiment patterns. Empirical results demonstrate superior risk-adjusted returns compared to traditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and portfolio return of 8.79% versus -0.61 and -1.39% respectively. These results suggest that LLM-based systematic macro analysis presents a viable approach for enhancing automated portfolio allocation decisions at the sector level.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents\",\n      \"year\": 2025,\n      \"authors\": \"Mohammad Rubyet Islam\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278782847\",\n      \"relevance\": 1,\n      \"abstract\": \"The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.\",\n      \"citations\": 4\n    },\n    {\n      \"title\": \"FinEvo: From Isolated Backtests to Ecological Market Games for Multi-Agent Financial Strategy Evolution\",\n      \"year\": 2026,\n      \"authors\": \"Mingxi Zou, Jiaxiang Chen, Aotian Luo, Jingyi Dai, Chi Zhang, Dongning Sun, Zenglin Xu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:285270243\",\n      \"relevance\": 1,\n      \"abstract\": \"Conventional financial strategy evaluation relies on isolated backtests in static environments. Such evaluations assess each policy independently, overlook correlations and interactions, and fail to explain why strategies ultimately persist or vanish in evolving markets. We shift to an ecological perspective, where trading strategies are modeled as adaptive agents that interact and learn within a shared market. Instead of proposing a new strategy, we present FinEvo, an ecological game formalism for studying the evolutionary dynamics of multi-agent financial strategies. At the individual level, heterogeneous ML-based traders-rule-based, deep learning, reinforcement learning, and large language model (LLM) agents-adapt using signals such as historical prices and external news. At the population level, strategy distributions evolve through three designed mechanisms-selection, innovation, and environmental perturbation-capturing the dynamic forces of real markets. Together, these two layers of adaptation link evolutionary game theory with modern learning dynamics, providing a principled environment for studying strategic behavior. Experiments with external shocks and real-world news streams show that FinEvo is both stable for reproducibility and expressive in revealing context-dependent outcomes. Strategies may dominate, collapse, or form coalitions depending on their competitors-patterns invisible to static backtests. By reframing strategy evaluation as an ecological game formalism, FinEvo provides a unified, mechanism-level protocol for analyzing robustness, adaptation, and emergent dynamics in multi-agent financial markets, and may offer a means to explore the potential impact of macroeconomic policies and financial regulations on price evolution and equilibrium.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"Large Language Model based Multi-Agents: A Survey of Progress and Challenges\",\n      \"year\": 2024,\n      \"authors\": \"Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, Xiangliang Zhang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:267412980\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.\",\n      \"citations\": 702\n    },\n    {\n      \"title\": \"CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading\",\n      \"year\": 2024,\n      \"authors\": \"Yuan Li, B. Luo, Qian Wang, Nuo Chen, Xu Liu, Bingsheng He\",\n      \"url\": \"https://www.semanticscholar.org/paper/41e49f3e7cef50ec4b1fc4b2fe4dd3ba04ef3b9f\",\n      \"relevance\": 1,\n      \"abstract\": \"The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data\\u2019s transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to time-series baselines, but not compared to traditional trading signals, across various cryptocurrencies and market conditions. Our code and data are available at https://github.com/Xtra-Computing/CryptoTrade\",\n      \"citations\": 31\n    },\n    {\n      \"title\": \"QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading\",\n      \"year\": 2025,\n      \"authors\": \"Fei Xiong, Xiang Zhang, Aosong Feng, Siqi Sun, Chenyu You\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:281309960\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advances in Large Language Models (LLMs) have shown remarkable capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks by leveraging fundamental and sentiment-based inputs for strategic decision-making. However, these approaches are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT typically requires rapid, risk-aware decisions driven by structured, short-horizon signals, such as technical indicators, chart patterns, and trend features. These signals stand in sharp contrast to the long-horizon, text-driven reasoning that characterizes most existing LLM-based systems in finance. To bridge this gap, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents--Indicator, Pattern, Trend, and Risk--each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. Extensive experiments across nine financial instruments, including Bitcoin and Nasdaq futures, demonstrate that QuantAgent consistently outperforms baseline methods, achieving higher predictive accuracy at both 1-hour and 4-hour trading intervals across multiple evaluation metrics. Our findings suggest that coupling structured trading signals with LLM-based reasoning provides a viable path for traceable, real-time decision systems in high-frequency financial markets.\",\n      \"citations\": 5\n    },\n    {\n      \"title\": \"Exploring LLM Cryptocurrency Trading Through Fact-Subjectivity Aware Reasoning\",\n      \"year\": 2024,\n      \"authors\": \"Qian Wang, Yuchen Gao, Zhenheng Tang, B. Luo, Nuo Chen, Bingsheng He\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:273374802\",\n      \"relevance\": 1,\n      \"abstract\": \"While many studies show that more advanced LLMs excel in tasks such as mathematics and coding, we observe that in cryptocurrency trading, stronger LLMs sometimes underperform compared to weaker ones. To investigate this counterintuitive phenomenon, we examine how LLMs reason when making trading decisions. Our findings reveal that (1) stronger LLMs show a preference for factual information over subjectivity; (2) separating the reasoning process into factual and subjective components leads to higher profits. Building on these insights, we propose a multi-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and learn from both factual and subjective reasoning. Extensive experiments demonstrate that this fine-grained reasoning approach enhances LLM trading performance in cryptocurrency markets, yielding profit improvements of 7\\\\% in BTC, 2\\\\% in ETH, and 10\\\\% in SOL. Additionally, an ablation study reveals that relying on subjective news generates higher returns in bull markets, while focusing on factual information yields better results in bear markets. Code is available at https://github.com/Persdre/FS-ReasoningAgent.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs\",\n      \"year\": 2025,\n      \"authors\": \"Hao Kang, Qingru Zhang, Han Cai, Weiyuan Xu, Tushar Krishna, Yilun Du, Tsachy Weissman\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278904558\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.\",\n      \"citations\": 6\n    },\n    {\n      \"title\": \"LLM Agents Do Not Replicate Human Market Traders: Evidence From Experimental Finance\",\n      \"year\": 2025,\n      \"authors\": \"Thomas Henning, Siddhartha Ojha, Ross Spoon, Jiatong Han, C. F. Camerer\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276575185\",\n      \"relevance\": 1,\n      \"abstract\": \"This paper explores how Large Language Models (LLMs) behave in a classic experimental finance paradigm widely known for eliciting bubbles and crashes in human participants. We adapt an established trading design, where traders buy and sell a risky asset with a known fundamental value, and introduce several LLM-based agents, both in single-model markets (all traders are instances of the same LLM) and in mixed-model\\\"battle royale\\\"settings (multiple LLMs competing in the same market). Our findings reveal that LLMs generally exhibit a\\\"textbook-rational\\\"approach, pricing the asset near its fundamental value, and show only a muted tendency toward bubble formation. Further analyses indicate that LLM-based agents display less trading strategy variance in contrast to humans. Taken together, these results highlight the risk of relying on LLM-only data to replicate human-driven market phenomena, as key behavioral features, such as large emergent bubbles, were not robustly reproduced. While LLMs clearly possess the capacity for strategic decision-making, their relative consistency and rationality suggest that they do not accurately mimic human market dynamics.\",\n      \"citations\": 4\n    },\n    {\n      \"title\": \"TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions\",\n      \"year\": 2024,\n      \"authors\": \"Alicia Vidler, Toby Walsh\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:273661563\",\n      \"relevance\": 1,\n      \"abstract\": \"We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviours generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges\\u2014such as coordinating turn-taking between realistic LLM-based agents\\u2014and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations.\",\n      \"citations\": 3\n    },\n    {\n      \"title\": \"FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models\",\n      \"year\": 2024,\n      \"authors\": \"Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, Chris Wang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:269983596\",\n      \"relevance\": 1,\n      \"abstract\": \"As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \\\\url{https://github.com/AI4Finance-Foundation/FinRobot}.\",\n      \"citations\": 34\n    },\n    {\n      \"title\": \"WebCryptoAgent: Agentic Crypto Trading with Web Informatics\",\n      \"year\": 2026,\n      \"authors\": \"Ali Kurban, Wei Luo, Liangyu Zuo, Zeyu Zhang, Renda Han, Zhaolu Kang, Hao Tang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:284544510\",\n      \"relevance\": 1,\n      \"abstract\": \"Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"LLMs for Financial Advisement: A Fairness and Efficacy Study in Personal Decision Making\",\n      \"year\": 2023,\n      \"authors\": \"Kausik Lakkaraju, Sara E Jones, S. Vuruma, Vishal Pallagani, Bharath Muppasani, Biplav Srivastava\",\n      \"url\": \"https://www.semanticscholar.org/paper/1dddc3cdca26cd434d48110f8d73674bb7f63c4f\",\n      \"relevance\": 1,\n      \"abstract\": \"As Large Language Model (LLM) based chatbots are becoming more accessible, users are relying on these chatbots for reliable and personalized recommendations in diverse domains, ranging from code generation to financial advisement. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We test widely used LLM-based chatbots, ChatGPT and Bard, and compare their performance against SafeFinance, a rule-based chatbot built using the Rasa platform. The comparison is across two critical tasks: product discovery and multi-product interaction, where product refers to banking products like Credit Cards, Certificate of Deposits, and Checking Accounts. With this study, we provide interesting insights into the chatbots\\u2019 efficacy in financial advisement and their ability to provide fair treatment across different user groups. We find that both Bard and ChatGPT can make errors in retrieving basic online information, the responses they generate are inconsistent across different user groups, and they cannot be relied on for reasoning involving banking products. On the other hand, despite their limited generalization capabilities, rule-based chatbots like SafeFinance provide safe and reliable answers to users that can be traced back to their original source. Overall, although the outputs of the LLM-based chatbots are fluent and plausible, there are still critical gaps in providing consistent and reliable financial information.\",\n      \"citations\": 48\n    },\n    {\n      \"title\": \"MASCA: LLM based-Multi Agents System for Credit Assessment\",\n      \"year\": 2025,\n      \"authors\": \"Gautam Jajoo, Pranjal A. Chitale, Saksham Agarwal\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280391092\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.\",\n      \"citations\": 4\n    },\n    {\n      \"title\": \"FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation\",\n      \"year\": 2025,\n      \"authors\": \"Song Jin, Shuqi Li, Shukun Zhang, Rui Yan\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282911316\",\n      \"relevance\": 1,\n      \"abstract\": \"While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents\",\n      \"year\": 2025,\n      \"authors\": \"Tianmi Ma, Jiawei Du, Wenxin Huang, Wenjie Wang, Liang Xie, Xian Zhong, Joey Tianyi Zhou\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276580341\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) have demonstrated remarkable capabilities in natural language tasks, yet their performance in dynamic, real-world financial environments remains underexplored. Existing approaches are limited to historical backtesting, where trading actions cannot influence market prices and agents train only on static data. To address this limitation, we present the Agent Trading Arena, a virtual zero-sum stock market in which LLM-based agents engage in competitive multi-agent trading and directly impact price dynamics. By simulating realistic bid-ask interactions, our platform enables training in scenarios that closely mirror live markets, thereby narrowing the gap between training and evaluation. Experiments reveal that LLMs struggle with numerical reasoning when given plain-text data, often overfitting to local patterns and recent values. In contrast, chart-based visualizations significantly enhance both numerical reasoning and trading performance. Furthermore, incorporating a reflection module yields additional improvements, especially with visual inputs. Evaluations on NASDAQ and CSI datasets demonstrate the superiority of our method, particularly under high volatility. All code and data are available at https://github.com/wekjsdvnm/Agent-Trading-Arena.\",\n      \"citations\": 3\n    },\n    {\n      \"title\": \"EconAgentic in DePIN Markets: A Large Language Model Approach to the Sharing Economy of Decentralized Physical Infrastructure\",\n      \"year\": 2025,\n      \"authors\": \"Yulin Liu, Mocca Schweitzer\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280985031\",\n      \"relevance\": 1,\n      \"abstract\": \"The Decentralized Physical Infrastructure (DePIN) market is revolutionizing the sharing economy through token-based economics and smart contracts that govern decentralized operations. By 2024, DePIN projects have exceeded \\\\$10 billion in market capitalization, underscoring their rapid growth. However, the unregulated nature of these markets, coupled with the autonomous deployment of AI agents in smart contracts, introduces risks such as inefficiencies and potential misalignment with human values. To address these concerns, we introduce EconAgentic, a Large Language Model (LLM)-powered framework designed to mitigate these challenges. Our research focuses on three key areas: 1) modeling the dynamic evolution of DePIN markets, 2) evaluating stakeholders'actions and their economic impacts, and 3) analyzing macroeconomic indicators to align market outcomes with societal goals. Through EconAgentic, we simulate how AI agents respond to token incentives, invest in infrastructure, and adapt to market conditions, comparing AI-driven decisions with human heuristic benchmarks. Our results show that EconAgentic provides valuable insights into the efficiency, inclusion, and stability of DePIN markets, contributing to both academic understanding and practical improvements in the design and governance of decentralized, tokenized economies.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework\",\n      \"year\": 2025,\n      \"authors\": \"Junhyeog Yun, Hyoun Jun Lee, Insu Jeon\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282246502\",\n      \"relevance\": 1,\n      \"abstract\": \"Automating quantitative trading strategy development in dynamic markets is challenging, especially with increasing demand for personalized investment solutions. Existing methods often fail to explore the vast strategy space while preserving the diversity essential for robust performance across changing market conditions. We present QuantEvolve, an evolutionary framework that combines quality-diversity optimization with hypothesis-driven strategy generation. QuantEvolve employs a feature map aligned with investor preferences, such as strategy type, risk profile, turnover, and return characteristics, to maintain a diverse set of effective strategies. It also integrates a hypothesis-driven multi-agent system to systematically explore the strategy space through iterative generation and evaluation. This approach produces diverse, sophisticated strategies that adapt to both market regime shifts and individual investment needs. Empirical results show that QuantEvolve outperforms conventional baselines, validating its effectiveness. We release a dataset of evolved strategies to support future research.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"Evaluating and Aligning Human Economic Risk Preferences in LLMs\",\n      \"year\": 2025,\n      \"authors\": \"Jiaxin Liu, Yi Yang, K. Tam\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276902692\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs) are increasingly used in decision-making scenarios that involve risk assessment, yet their alignment with human economic rationality remains unclear. In this study, we investigate whether LLMs exhibit risk preferences consistent with human expectations across different personas. Specifically, we assess whether LLM-generated responses reflect appropriate levels of risk aversion or risk-seeking behavior based on individual's persona. Our results reveal that while LLMs make reasonable decisions in simplified, personalized risk contexts, their performance declines in more complex economic decision-making tasks. To address this, we propose an alignment method designed to enhance LLM adherence to persona-specific risk preferences. Our approach improves the economic rationality of LLMs in risk-related applications, offering a step toward more human-aligned AI decision-making.\",\n      \"citations\": 6\n    },\n    {\n      \"title\": \"Towards a Realistic Long-Term Benchmark for Open-Web Research Agents\",\n      \"year\": 2024,\n      \"authors\": \"Peter M\\u00fchlbacher, N. Bosse, Lawrence Phillips\",\n      \"url\": \"https://www.semanticscholar.org/paper/c7edf4733ae230433f09fb5cdd3f43cca7926f41\",\n      \"relevance\": 1,\n      \"abstract\": \"We present initial results of a forthcoming benchmark for evaluating LLM agents on white-collar tasks of economic value. We evaluate agents on real-world\\\"messy\\\"open-web research tasks of the type that are routine in finance and consulting. In doing so, we lay the groundwork for an LLM agent evaluation suite where good performance directly corresponds to a large economic and societal impact. We built and tested several agent architectures with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini. On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview substantially outperformed agents using GPT-4o, with agents based on Llama 3.1 (405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct architecture with the ability to delegate subtasks to subagents performed best. In addition to quantitative evaluations, we qualitatively assessed the performance of the LLM agents by inspecting their traces and reflecting on their observations. Our evaluation represents the first in-depth assessment of agents' abilities to conduct challenging, economically valuable analyst-style research on the real open web.\",\n      \"citations\": 3\n    }\n  ]\n}\n\nResults saved to: paper_search_results/LLM_agents_long-term_trading_decisions_finance_20260222_013819.jsonl\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_22","type":"reasoning","text":"**Planning paper download strategy**"}}
{"type":"item.completed","item":{"id":"item_23","type":"agent_message","text":"Paper-finder worked and returned 71 candidates; next I’ll parse the saved results, keep high-relevance entries, and download available PDFs (prioritizing arXiv/OpenReview links) into `papers/` with a clean index."}}
{"type":"item.started","item":{"id":"item_24","type":"command_execution","command":"/bin/bash -lc 'pwd && ls -l paper_search_results && latest=$(ls -1t paper_search_results/*.jsonl | head -n 1); echo \"$latest\" && sed -n '\"'1,40p' \\\"\"'$latest\"'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_24","type":"command_execution","command":"/bin/bash -lc 'pwd && ls -l paper_search_results && latest=$(ls -1t paper_search_results/*.jsonl | head -n 1); echo \"$latest\" && sed -n '\"'1,40p' \\\"\"'$latest\"'","aggregated_output":"Total output lines: 44\n\n/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 65\n-rw-rw---- 1 haokunliu haokunliu 116117 Feb 22 01:38 LLM_agents_long-term_trading_decisions_finance_20260222_013819.jsonl\npaper_search_results/LLM_agents_long-term_trading_decisions_finance_20260222_013819.jsonl\n{\"title\": \"StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?\", \"year\": 2025, \"authors\": \"Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li\", \"url\": \"https://api.semanticscholar.org/CorpusId:281724665\", \"relevance\": 3, \"abstract\": \"Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.\", \"citations\": 7}\n{\"title\": \"FinPos: A Position-Aware Trading Agent System for Real Financial Markets\", \"year\": 2025, \"authors\": \"Bijia Liu, Ronghao Dang\", \"url\": \"https://api.semanticscholar.org/CorpusId:282719341\", \"relevance\": 3, \"abstract\": \"The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, most existing trading agents operate under intraday, independent unit-based trading tasks, where decisions are made as isolated directional actions, and thus lack awareness of continuous position management. Therefore, we propose a position-aware trading task designed to simulate a more realistic market. To address this task, we propose FinPos, a position-aware trading agent system designed to explicitly model and manage continuous positions. FinPos enhances position awareness through three key mechanisms: (1) professional-level interpretation of heterogeneous market information; (2) a dual-agent decision structure that separates directional reasoning from risk-aware position adjustment; and (3) multi-timescale reward signals, allowing the agent to internalize position awareness through experiential feedback rather than static instructions alone. Extensive experiments demonstrate that FinPos surpasses state-of-the-art trading agents in the position-aware trading task, which closely mirrors real market conditions. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-term market decision-making.\", \"citations\": 2}\n{\"title\": \"Hierarchical Organization Simulacra in the Investment Sector\", \"year\": 2024, \"authors\": \"Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao\", \"url\": \"https://api.semanticscholar.org/CorpusId:273022534\", \"relevance\": 3, \"abstract\": \"This paper explores designing artificial organizations with professional behavior in investments using a multi-agent simulation. The method mimics hierarchical decision-making in investment firms, using news articles to inform decisions. A large-scale study analyzing over 115,000 news articles of 300 companies across 15 years compared this approach against professional traders' decisions. Results show that hierarchical simulations align closely with professional choices, both in frequency and profitability. However, the study also reveals biases in decision-making, where changes in prompt wording and perceived agent seniority significantly influence outcomes. This highlights both the potential and limitations of large language models in replicating professional financial decision-making.\", \"citations\": 1}\n{\"title\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?\", \"year\": 2025, \"authors\": \"Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma\", \"url\": \"https://api.semanticscholar.org/CorpusId:278501425\", \"relevance\": 3, \"abstract\": \"Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.\", \"citations\": 8}\n{\"title\": \"HedgeAgents: A Balanced-aware Multi-agent Financial Trading System\", \"year\": 2025, \"authors\": \"Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu\", \"url\": \"https://api.semanticscholar.org/CorpusId:276449781\", \"relevance\": 3, \"abstract\": \"As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ''hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).\", \"citations\": 23}\n{\"title\": \"Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance\", \"year\": 2026, \"authors\": \"Mostapha Benhenda\", \"url\": \"https://api.semanticscholar.org/CorpusId:284911867\", \"relevance\": 3, \"abstract\": \"We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench\", \"citations\": 0}\n{\"title\": \"Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation\", \"year\": 2026, \"authors\": \"Zeping Li, Guancheng Wan, Keyang Chen, Yu Chen, Yiwen Zhao, Philip Torr, Guangnan Ye, Zhenfei Yin, Hongfeng Chai\", \"url\": \"https://www.semanticscholar.org/paper/7550169d076668ede36673525ad2b96baa54fe0a\", \"relevance\": 3, \"abstract\": \"Recent works have increasingly applied Large Language Models (LLMs) as agents in financial stock market simulations to test if micro-level behaviors aggregate into macro-level phenomena. However, a crucial question arises: Do LLM agents'behaviors align with real market participants? This alignment is key to the validity of simulation results. To explore this, we select a financial stock market scenario to test behavioral consistency. Investors are typically classified as fundamental or technical traders, but most simulations fix strategies at initialization, failing to reflect real-world trading dynamics. In this work, we assess whether agents'strategy switching aligns with financial theory, providing a framework for this evaluation. We operationalize four behavioral-finance drivers-loss aversion, herding, wealth differentiation, and price misalignment-as personality traits set via prompting and stored long-term. In year-long simulations, agents process daily price-volume data, trade under a designated style, and reassess their strategy every 10 trading days. We introduce four alignment metrics and use Mann-Whitney U tests to compare agents'style-switching behavior with financial theory. Our results show that recent LLMs'switching behavior is only partially consistent with behavioral-finance theories, highlighting the need for further refinement in aligning agent behavior with financial theory.\", \"citations\": 0}\n{\"title\": \"FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design\", \"year\": 2023, \"authors\": \"Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Jordan W. Suchow, Denghui Zhang, K. Khashanah\", \"url\": \"https://api.semanticscholar.org/CorpusId:265445755\", \"relevance\": 1, \"abstract\": \"We introduce FinMem, a novel Large Language Models (LLM)-based agent framework for financial trading, designed to address the need for automated systems that can transform real-time data into executable decisions. FinMem comprises three core modules: Profile for customizing agent characteristics, Memory for hierarchical financial data assimilation, and Decision-making for converting insights into investment choices. The Memory module, which mimics human traders\\u2019 cognitive structure, offers interpretability and real-time tuning while handling the critical timing of various information types. It employs a layered approach to process and prioritize data based on its timeliness and relevance, ensuring that the most recent and impactful information is given appropriate weight in decision-making. FinMem\\u2019s adjustable cognitive span allows retention of critical information beyond human limits, enabling it to balance historical patterns with current market dynamics. This framework facilitates self-evolution of professional knowledge, agile reactions to investment cues, and continuous refinement of trading decisions in financial environments. When compared against advanced algorithmic agents using a large-scale real-world financial dataset, FinMem demonstrates superior performance across classic metrics like Cumulative Return and Sharpe ratio. Further tuning of the agent\\u2019s perceptual span and character setting enhances its trading performance, positioning FinMem as a cutting-edge solution for automated trading.\", \"citations\": 148}\n{\"title\": \"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist\", \"year\": 2024, \"authors\": \"Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An\", \"url\": \"https://api.semanticscholar.org/CorpusId:268041696\", \"relevance\": 1, \"abstract\": \"Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.\", \"citations\": 126}\n{\"title\": \"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making\", \"year\": 2024, \"authors\": \"Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Rong Liu, Zhenyu Cui, Denghui Zhang, K. Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie\", \"url\": \"https://api.semanticscholar.org/CorpusId:271064881\", \"relevance\": 1, \"abstract\": \"Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management.\", \"citations\": 97}\n{\"title\": \"TradingAgents: Multi-Agents LLM Financial Trading Framework\", \"year\": 2024, \"authors\": \"Yijia Xiao, Edward Sun, Di Luo, Wei Wang\", \"url\": \"https://api.semanticscholar.org/CorpusId:275133732\", \"relevance\": 1, \"abstract\": \"Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, the multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. TradingAgents is available at https://github.com/TauricResearch/TradingAgents.\", \"citations\": 92}\n{\"title\": \"A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges\", \"year\": 2024, \"authors\": \"Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Poor, Qingsong Wen, Stefan Zohren\", \"url\": \"https://api.semanticscholar.org/CorpusId:270562262\", \"relevance\": 1, \"abstract\": \"Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing th…6560 tokens truncated…ty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.\", \"citations\": 3}\n{\"title\": \"FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment\", \"year\": 2025, \"authors\": \"Wentao Zhang, Yilei Zhao, Chuqiao Zong, Xinrun Wang, Bo An\", \"url\": \"https://api.semanticscholar.org/CorpusId:280421572\", \"relevance\": 1, \"abstract\": \"Financial AI holds great promise for transforming modern finance, with the potential to support a wide range of tasks such as market forecasting, portfolio management, quantitative trading, and automated analysis. However, existing platforms remain limited in task coverage, lack robust multimodal data integration, and offer insufficient support for the training and deployment of large language models (LLMs). In response to these limitations, we present FinWorld, an all-in-one open-source platform that provides end-to-end support for the entire financial AI workflow, from data acquisition to experimentation and deployment. FinWorld distinguishes itself through native integration of heterogeneous financial data, unified support for diverse AI paradigms, and advanced agent automation, enabling seamless development and deployment. Leveraging data from 2 representative markets, 4 stock pools, and over 800 million financial data points, we conduct comprehensive experiments on 4 key financial AI tasks. These experiments systematically evaluate deep learning and reinforcement learning algorithms, with particular emphasis on RL-based finetuning for LLMs and LLM Agents. The empirical results demonstrate that FinWorld significantly enhances reproducibility, supports transparent benchmarking, and streamlines deployment, thereby providing a strong foundation for future research and real-world applications. Code is available at Github~\\\\footnote{https://github.com/DVampire/FinWorld}.\", \"citations\": 2}\n{\"title\": \"Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking\", \"year\": 2025, \"authors\": \"Changlun Li, Yao Shi, Chen Wang, Qiqi Duan, Runke Ruan, Weijie Huang, Haonan Long, Lijun Huang, Yuyu Luo, Nan Tang\", \"url\": \"https://api.semanticscholar.org/CorpusId:278715123\", \"relevance\": 1, \"abstract\": \"Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in managing complex fund investment remains inadequately assessed. A fundamental limitation of existing benchmarks for evaluating LLM-driven trading strategies is their reliance on historical back-testing, inadvertently enabling LLMs to\\\"time travel\\\"-leveraging future information embedded in their training corpora, thus resulting in possible information leakage and overly optimistic performance estimates. To address this issue, we introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate LLM in real-time market conditions. Utilizing a multi-agent architecture, DeepFund connects directly with real-time stock market data-specifically data published after each model pretraining cutoff-to ensure fair and leakage-free evaluations. Empirical tests on nine flagship LLMs from leading global institutions across multiple investment dimensions-including ticker-level analysis, investment decision-making, portfolio management, and risk control-reveal significant practical challenges. Notably, even cutting-edge models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses within DeepFund real-time evaluation environment, underscoring the present limitations of LLMs for active fund management. Our code is available at https://github.com/HKUSTDial/DeepFund.\", \"citations\": 6}\n{\"title\": \"FINRS: A Risk-Sensitive Trading Framework for Real Financial Markets\", \"year\": 2025, \"authors\": \"Bijia Liu, Ronghao Dang\", \"url\": \"https://api.semanticscholar.org/CorpusId:283072776\", \"relevance\": 1, \"abstract\": \"Large language models (LLMs) have shown strong reasoning capabilities and are increasingly explored for financial trading. Existing LLM-based trading agents, however, largely focus on single-step prediction and lack integrated mechanisms for risk management, which reduces their effectiveness in volatile markets. We introduce FinRS, a risk-sensitive trading framework that combines hierarchical market analysis, dual-decision agents, and multi-timescale reward reflection to align trading actions with both return objectives and downside risk constraints. Experiments on multiple stocks and market conditions show that FinRS achieves superior profitability and stability compared to state-of-the-art methods.\", \"citations\": 0}\n{\"title\": \"Bridging Language Models and Financial Analysis\", \"year\": 2025, \"authors\": \"Alejandro Lopez-Lira, Jihoon Kwon, Sangwoong Yoon, Jy-yong Sohn, Chanyeol Choi\", \"url\": \"https://api.semanticscholar.org/CorpusId:277451826\", \"relevance\": 1, \"abstract\": \"The rapid advancements in Large Language Models (LLMs) have unlocked transformative possibilities in natural language processing, particularly within the financial sector. Financial data is often embedded in intricate relationships across textual content, numerical tables, and visual charts, posing challenges that traditional methods struggle to address effectively. However, the emergence of LLMs offers new pathways for processing and analyzing this multifaceted data with increased efficiency and insight. Despite the fast pace of innovation in LLM research, there remains a significant gap in their practical adoption within the finance industry, where cautious integration and long-term validation are prioritized. This disparity has led to a slower implementation of emerging LLM techniques, despite their immense potential in financial applications. As a result, many of the latest advancements in LLM technology remain underexplored or not fully utilized in this domain. This survey seeks to bridge this gap by providing a comprehensive overview of recent developments in LLM research and examining their applicability to the financial sector. Building on previous survey literature, we highlight several novel LLM methodologies, exploring their distinctive capabilities and their potential relevance to financial data analysis. By synthesizing insights from a broad range of studies, this paper aims to serve as a valuable resource for researchers and practitioners, offering direction on promising research avenues and outlining future opportunities for advancing LLM applications in finance.\", \"citations\": 5}\n{\"title\": \"MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading\", \"year\": 2025, \"authors\": \"Siyi Wu, Zhaoyang Guan, Leyi Zhao, Xinyuan Song, Xinyu Ying, Hanlin Zhang, Michele Pak, Yangfan He, Yi Xin, Jianhui Wang, Tianyu Shi\", \"url\": \"https://api.semanticscholar.org/CorpusId:280322773\", \"relevance\": 1, \"abstract\": \"Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \\\\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.\", \"citations\": 1}\n{\"title\": \"Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges\", \"year\": 2025, \"authors\": \"Liyuan Chen, Shuoling Liu, Jiangpeng Yan, Xiaoyu Wang, Henglin Liu, Chuang Li, Kechen Jiao, Jixuan Ying, Yang Liu, Qiang Yang, Xiu Li\", \"url\": \"https://api.semanticscholar.org/CorpusId:280017319\", \"relevance\": 1, \"abstract\": \"\", \"citations\": 2}\n{\"title\": \"P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis\", \"year\": 2025, \"authors\": \"Chen-Che Lu, Yun-Cheng Chou, Teng-Ruei Chen\", \"url\": \"https://api.semanticscholar.org/CorpusId:282389311\", \"relevance\": 1, \"abstract\": \"Recent advances in large language models (LLMs) have enabled multi-agent reasoning systems capable of collaborative decision-making. However, in financial analysis, most frameworks remain narrowly focused on either isolated single-agent predictors or loosely connected analyst ensembles, and they lack a coherent reasoning workflow that unifies diverse data modalities. We introduce P1GPT, a layered multi-agent LLM framework for multi-modal financial information analysis and interpretable trading decision support. Unlike prior systems that emulate trading teams through role simulation, P1GPT implements a structured reasoning pipeline that systematically fuses technical, fundamental, and news-based insights through coordinated agent communication and integration-time synthesis. Backtesting on multi-modal datasets across major U.S. equities demonstrates that P1GPT achieves superior cumulative and risk-adjusted returns, maintains low drawdowns, and provides transparent causal rationales. These findings suggest that structured reasoning workflows, rather than agent role imitation, offer a scalable path toward explainable and trustworthy financial AI systems.\", \"citations\": 0}\n{\"title\": \"A Survey of Financial AI: Architectures, Advances and Open Challenges\", \"year\": 2024, \"authors\": \"Junhua Liu\", \"url\": \"https://api.semanticscholar.org/CorpusId:274149777\", \"relevance\": 1, \"abstract\": \"Financial AI empowers sophisticated approaches to financial market forecasting, portfolio optimization, and automated trading. This survey provides a systematic analysis of these developments across three primary dimensions: predictive models that capture complex market dynamics, decision-making frameworks that optimize trading and investment strategies, and knowledge augmentation systems that leverage unstructured financial information. We examine significant innovations including foundation models for financial time series, graph-based architectures for market relationship modeling, and hierarchical frameworks for portfolio optimization. Analysis reveals crucial trade-offs between model sophistication and practical constraints, particularly in high-frequency trading applications. We identify critical gaps and open challenges between theoretical advances and industrial implementation, outlining open challenges and opportunities for improving both model performance and practical applicability.\", \"citations\": 10}\n{\"title\": \"A Survey on the Optimization of Large Language Model-based Agents\", \"year\": 2025, \"authors\": \"Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang He\", \"url\": \"https://api.semanticscholar.org/CorpusId:277065966\", \"relevance\": 1, \"abstract\": \"With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness in complex agent-related environments. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective remains lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the evaluation for agents, review key applications of LLM-based agents, and discuss the major challenges and promising future directions. A curated collection of the surveyed works is provided at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.\", \"citations\": 21}\n{\"title\": \"Large Language Models in equity markets: applications, techniques, and insights\", \"year\": 2025, \"authors\": \"Aakanksha Jadhav, Vishal Mirza\", \"url\": \"https://api.semanticscholar.org/CorpusId:280955402\", \"relevance\": 1, \"abstract\": \"Recent breakthroughs in Large Language Models (LLMs) have the potential to disrupt equity investing by enabling sophisticated data analysis, market prediction, and automated trading. This paper presents a comprehensive review of 84 research studies conducted between 2022 and early 2025, synthesizing the state of LLM applications in stock investing. We provide a dual-layered categorization: first, by financial applications such as stock price forecasting, sentiment analysis, portfolio management, and algorithmic trading; second, by technical methodologies, including prompting, fine-tuning, multi-agent frameworks, reinforcement learning, and custom architectures. Additionally, we consolidate findings on the datasets used, ranging from financial statements to multimodal data (news, market trends, earnings transcripts, social media), and systematically compare general-purpose vs. finance-specialized LLMs used in research. Our analysis identifies key research trends, commonalities, and divergences across studies, evaluating both their empirical contributions and methodological innovations. We highlight the strengths of existing research, such as improved sentiment extraction and the use of reinforcement learning to factor market feedback, alongside critical gaps in scalability, interpretability, and real-world validation. Finally, we propose directions for future research, emphasizing hybrid modeling approaches, architectures that factor reasoning and large context windows, and robust evaluation frameworks to advance AI-driven financial strategies. By mapping the intersection of LLMs and equity markets, this review provides a foundation and roadmap for future research and practical implementation in the financial sector.\", \"citations\": 10}\n{\"title\": \"LLM-Powered AI Agent Systems and Their Applications in Industry\", \"year\": 2025, \"authors\": \"Guannan Liang, Qianqian Tong\", \"url\": \"https://api.semanticscholar.org/CorpusId:278789054\", \"relevance\": 1, \"abstract\": \"The emergence of Large Language Models (LLMs) has reshaped agent systems. Unlike traditional rule-based agents with limited task scope, LLM-powered agents offer greater flexibility, cross-domain reasoning, and natural language interaction. Moreover, with the integration of multi-modal LLMs, current agent systems are highly capable of processing diverse data modalities, including text, images, audio, and structured tabular data, enabling richer and more adaptive real-world behavior. This paper comprehensively examines the evolution of agent systems from the pre-LLM era to current LLM-powered architectures. We categorize agent systems into software-based, physical, and adaptive hybrid systems, highlighting applications across customer service, software development, manufacturing automation, personalized education, financial trading, and healthcare. We further discuss the primary challenges posed by LLM-powered agents, including high inference latency, output uncertainty, lack of evaluation metrics, and security vulnerabilities, and propose potential solutions to mitigate these concerns.\", \"citations\": 18}\n{\"title\": \"When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments\", \"year\": 2024, \"authors\": \"Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhengting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li, Mengnan Du, Yongfeng Zhang\", \"url\": \"https://api.semanticscholar.org/CorpusId:271533952\", \"relevance\": 1, \"abstract\": \"Can AI Agents simulate real-world trading environments to investigate the impact of external factors on stock trading activities (e.g., macroeconomics, policy changes, company fundamentals, and global events)? These factors, which frequently influence trading behaviors, are critical elements in the quest for maximizing investors' profits. Our work attempts to solve this problem through large language model based agents. We have developed a multi-agent AI system called StockAgent, driven by LLMs, designed to simulate investors' trading behaviors in response to the real stock market. The StockAgent allows users to evaluate the impact of different external factors on investor trading and to analyze trading behavior and profitability effects. Additionally, StockAgent avoids the test set leakage issue present in existing trading simulation systems based on AI Agents. Specifically, it prevents the model from leveraging prior knowledge it may have acquired related to the test data. We evaluate different LLMs under the framework of StockAgent in a stock trading environment that closely resembles real-world conditions. The experimental results demonstrate the impact of key external factors on stock market trading, including trading behavior and stock price fluctuation rules. This research explores the study of agents' free trading gaps in the context of no prior knowledge related to market data. The patterns identified through StockAgent simulations provide valuable insights for LLM-based investment advice and stock recommendation. The code is available at https://github.com/MingyuJ666/Stockagent.\", \"citations\": 23}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_25","type":"reasoning","text":"**Planning arXiv PDF download**"}}
{"type":"item.started","item":{"id":"item_26","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nimport json,glob\npath=sorted(glob.glob('paper_search_results/*.jsonl'))[-1]\nrows=[json.loads(l) for l in open(path)]\nsel=[r for r in rows if r.get('relevance',0)>=2]\nprint('total',len(rows),'relevance>=2',len(sel))\nfor i,r in enumerate(sel[:30],1):\n    print(f\\\"{i:02d}. {r['year']} | rel={r['relevance']} | {r['title']}\\\")\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_26","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nimport json,glob\npath=sorted(glob.glob('paper_search_results/*.jsonl'))[-1]\nrows=[json.loads(l) for l in open(path)]\nsel=[r for r in rows if r.get('relevance',0)>=2]\nprint('total',len(rows),'relevance>=2',len(sel))\nfor i,r in enumerate(sel[:30],1):\n    print(f\\\"{i:02d}. {r['year']} | rel={r['relevance']} | {r['title']}\\\")\nPY\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 71 relevance>=2 7\n01. 2025 | rel=3 | StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?\n02. 2025 | rel=3 | FinPos: A Position-Aware Trading Agent System for Real Financial Markets\n03. 2024 | rel=3 | Hierarchical Organization Simulacra in the Investment Sector\n04. 2025 | rel=3 | Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?\n05. 2025 | rel=3 | HedgeAgents: A Balanced-aware Multi-agent Financial Trading System\n06. 2026 | rel=3 | Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance\n07. 2026 | rel=3 | Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_27","type":"reasoning","text":"**Planning bulk paper downloads**"}}
{"type":"item.completed","item":{"id":"item_28","type":"agent_message","text":"I found 7 high-relevance papers (`relevance >= 2`) from paper-finder. I’ll now resolve and download their PDFs first, then add a few foundational baselines (e.g., FinMem/TradingAgents) to make the review more actionable."}}
{"type":"item.started","item":{"id":"item_29","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nimport os, json, glob, re\nfrom pathlib import Path\nimport arxiv, requests\n\nPath('papers').mkdir(exist_ok=True)\npath=sorted(glob.glob('paper_search_results/*.jsonl'))[-1]\nrows=[json.loads(l) for l in open(path)]\nselected=[r for r in rows if r.get('relevance',0)>=2]\nextra_titles=[\n    'FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design',\n    'TradingAgents: Multi-Agents LLM Financial Trading Framework',\n    'A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist',\n]\nfor t in extra_titles:\n    if not any(r['title']==t for r in selected):\n        selected.append({'title':t,'year':None,'authors':'','relevance':1,'abstract':'','url':''})\n\nmeta=[]\n\ndef clean(s):\n    s=re.sub(r'[\"'^A-Za-z0-9]+'\"','_',s).strip('_')\n    return s[:70]\n\nfor r in selected:\n    title=r['title']\n    query='ti:\\\"'+title+'\\\"'\n    best=None\n    try:\n        search=arxiv.Search(query=query,max_results=5,sort_by=arxiv.SortCriterion.Relevance)\n        cands=list(search.results())\n        if not cands:\n            # fallback broad\n            search=arxiv.Search(query=title,max_results=5,sort_by=arxiv.SortCriterion.Relevance)\n            cands=list(search.results())\n        if cands:\n            best=cands[0]\n    except Exception:\n        best=None\n\n    entry={**r,'status':'not_found','file':'','arxiv_id':'','pdf_url':''}\n    if best is not None:\n        arxiv_id=str(best.get_short_id())\n        fname=f\\\"{arxiv_id}_{clean(title)}.pdf\\\"\n        out=str(Path('papers')/fname)\n        pdf_url=best.pdf_url\n        try:\n            with requests.get(pdf_url,timeout=60) as resp:\n                resp.raise_for_status()\n                with open(out,'wb') as f:\n                    f.write(resp.content)\n            if os.path.getsize(out)>50000:\n                entry.update({'status':'downloaded','file':fname,'arxiv_id':arxiv_id,'pdf_url':pdf_url,'matched_title':best.title,'published':str(best.published.date())})\n            else:\n                entry['status']='tiny_file'\n        except Exception as e:\n            entry['status']='download_error'\n            entry['error']=str(e)\n    meta.append(entry)\n\nwith open('papers/papers_metadata.json','w') as f:\n    json.dump(meta,f,indent=2)\nprint('Downloaded',sum(1 for m in meta if m['status']=='downloaded'),'of',len(meta))\nfor m in meta:\n    print(m['status'], '|', m['title'][:80], '|', m.get('file',''))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_29","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nimport os, json, glob, re\nfrom pathlib import Path\nimport arxiv, requests\n\nPath('papers').mkdir(exist_ok=True)\npath=sorted(glob.glob('paper_search_results/*.jsonl'))[-1]\nrows=[json.loads(l) for l in open(path)]\nselected=[r for r in rows if r.get('relevance',0)>=2]\nextra_titles=[\n    'FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design',\n    'TradingAgents: Multi-Agents LLM Financial Trading Framework',\n    'A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist',\n]\nfor t in extra_titles:\n    if not any(r['title']==t for r in selected):\n        selected.append({'title':t,'year':None,'authors':'','relevance':1,'abstract':'','url':''})\n\nmeta=[]\n\ndef clean(s):\n    s=re.sub(r'[\"'^A-Za-z0-9]+'\"','_',s).strip('_')\n    return s[:70]\n\nfor r in selected:\n    title=r['title']\n    query='ti:\\\"'+title+'\\\"'\n    best=None\n    try:\n        search=arxiv.Search(query=query,max_results=5,sort_by=arxiv.SortCriterion.Relevance)\n        cands=list(search.results())\n        if not cands:\n            # fallback broad\n            search=arxiv.Search(query=title,max_results=5,sort_by=arxiv.SortCriterion.Relevance)\n            cands=list(search.results())\n        if cands:\n            best=cands[0]\n    except Exception:\n        best=None\n\n    entry={**r,'status':'not_found','file':'','arxiv_id':'','pdf_url':''}\n    if best is not None:\n        arxiv_id=str(best.get_short_id())\n        fname=f\\\"{arxiv_id}_{clean(title)}.pdf\\\"\n        out=str(Path('papers')/fname)\n        pdf_url=best.pdf_url\n        try:\n            with requests.get(pdf_url,timeout=60) as resp:\n                resp.raise_for_status()\n                with open(out,'wb') as f:\n                    f.write(resp.content)\n            if os.path.getsize(out)>50000:\n                entry.update({'status':'downloaded','file':fname,'arxiv_id':arxiv_id,'pdf_url':pdf_url,'matched_title':best.title,'published':str(best.published.date())})\n            else:\n                entry['status']='tiny_file'\n        except Exception as e:\n            entry['status']='download_error'\n            entry['error']=str(e)\n    meta.append(entry)\n\nwith open('papers/papers_metadata.json','w') as f:\n    json.dump(meta,f,indent=2)\nprint('Downloaded',sum(1 for m in meta if m['status']=='downloaded'),'of',len(meta))\nfor m in meta:\n    print(m['status'], '|', m['title'][:80], '|', m.get('file',''))\nPY\"","aggregated_output":"<stdin>:30: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\nDownloaded 10 of 10\ndownloaded | StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets? | 2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf\ndownloaded | FinPos: A Position-Aware Trading Agent System for Real Financial Markets | 2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf\ndownloaded | Hierarchical Organization Simulacra in the Investment Sector | 2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf\ndownloaded | Can LLM-based Financial Investing Strategies Outperform the Market in Long Run? | 2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf\ndownloaded | HedgeAgents: A Balanced-aware Multi-agent Financial Trading System | 2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf\ndownloaded | Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time L | 2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf\ndownloaded | Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style S | 2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf\ndownloaded | FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Charact | 2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf\ndownloaded | TradingAgents: Multi-Agents LLM Financial Trading Framework | 2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf\ndownloaded | A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified | 2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_30","type":"reasoning","text":"**Planning paper extraction**"}}
{"type":"item.completed","item":{"id":"item_31","type":"agent_message","text":"All 10 target PDFs were downloaded successfully. I’ll now run the PDF chunker and perform deeper extraction on the highest-relevance papers (`StockBench`, `FinPos`, `FINSABER`, `Look-Ahead-Bench`) so the literature notes include methods, baselines, datasets, and metrics beyond abstracts."}}
{"type":"item.started","item":{"id":"item_32","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -lh papers/*.pdf | sed -n '1,200p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_32","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -lh papers/*.pdf | sed -n '1,200p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n-rw-rw---- 1 haokunliu haokunliu 7.5M Feb 22 01:38 papers/2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf\n-rw-rw---- 1 haokunliu haokunliu  15M Feb 22 01:38 papers/2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf\n-rw-rw---- 1 haokunliu haokunliu 581K Feb 22 01:38 papers/2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf\n-rw-rw---- 1 haokunliu haokunliu 1.9M Feb 22 01:38 papers/2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf\n-rw-rw---- 1 haokunliu haokunliu 2.1M Feb 22 01:38 papers/2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf\n-rw-rw---- 1 haokunliu haokunliu 1.2M Feb 22 01:38 papers/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf\n-rw-rw---- 1 haokunliu haokunliu 1.9M Feb 22 01:38 papers/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf\n-rw-rw---- 1 haokunliu haokunliu 3.7M Feb 22 01:38 papers/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf\n-rw-rw---- 1 haokunliu haokunliu 147K Feb 22 01:38 papers/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf\n-rw-rw---- 1 haokunliu haokunliu 1.3M Feb 22 01:38 papers/2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_33","type":"reasoning","text":"**Running chunker on papers**"}}
{"type":"item.started","item":{"id":"item_34","type":"command_execution","command":"/bin/bash -lc 'pwd && source .venv/bin/activate && mkdir -p papers/pages && for f in papers/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf papers/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf papers/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf papers/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf; do python .claude/skills/paper-finder/scripts/pdf_chunker.py \"$f\" --pages-per-chunk 3 --output-dir papers/pages; done && ls -1 papers/pages | sed -n '\"'1,120p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_34","type":"command_execution","command":"/bin/bash -lc 'pwd && source .venv/bin/activate && mkdir -p papers/pages && for f in papers/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf papers/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf papers/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf papers/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf; do python .claude/skills/paper-finder/scripts/pdf_chunker.py \"$f\" --pages-per-chunk 3 --output-dir papers/pages; done && ls -1 papers/pages | sed -n '\"'1,120p'\"","aggregated_output":"Splitting PDF: papers/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf\nTotal pages: 14\nPages per chunk: 3\nCreated 5 chunk files in: papers/pages\nManifest: papers/pages/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_manifest.txt\nSplitting PDF: papers/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf\nTotal pages: 22\nPages per chunk: 3\nCreated 8 chunk files in: papers/pages\nManifest: papers/pages/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_manifest.txt\nSplitting PDF: papers/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf\nTotal pages: 15\nPages per chunk: 3\nCreated 5 chunk files in: papers/pages\nManifest: papers/pages/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__manifest.txt\nSplitting PDF: papers/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf\nTotal pages: 8\nPages per chunk: 3\nCreated 3 chunk files in: papers/pages\nManifest: papers/pages/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__manifest.txt\n2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_001.pdf\n2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_002.pdf\n2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_003.pdf\n2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_004.pdf\n2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_005.pdf\n2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__manifest.txt\n2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_001.pdf\n2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_002.pdf\n2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_003.pdf\n2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_004.pdf\n2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_005.pdf\n2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_manifest.txt\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_001.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_002.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_003.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_004.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_005.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_006.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_007.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_008.pdf\n2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_manifest.txt\n2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__chunk_001.pdf\n2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__chunk_002.pdf\n2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__chunk_003.pdf\n2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__manifest.txt\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_35","type":"reasoning","text":"**Planning comprehensive PDF extraction**"}}
{"type":"item.started","item":{"id":"item_36","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pypdf import PdfReader\nfrom pathlib import Path\nimport re, json\n\nchunk_dir=Path('papers/pages')\nids=[\n'2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market',\n'2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market',\n'2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_',\n'2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_'\n]\nnotes={}\nfor pid in ids:\n    chunks=sorted(chunk_dir.glob(pid+'_chunk_*.pdf'))\n    chunk_summ=[]\n    full_text=[]\n    for c in chunks:\n        r=PdfReader(str(c))\n        txt='\\\\n'.join((p.extract_text() or '') for p in r.pages)\n        txt=re.sub(r'\\\\s+',' ',txt)\n        full_text.append(txt)\n        chunk_summ.append({'chunk':c.name,'chars':len(txt),'preview':txt[:600]})\n    alltxt=' '.join(full_text)\n    urls=sorted(set(re.findall(r'https?://[\"'^'\"\\\\s\\\\)\\\\]]+',alltxt)))\n    dsets=sorted(set(re.findall(r'\\\\b(?:S&P\\\\s?500|DJIA|Dow\\\\s?Jones|CSI\\\\s?300|NASDAQ|CRSP|Compustat|LOBSTER|FRED|Polymarket|A-share|Bitcoin|BTC|SPY|Russell\\\\s?1000|Yahoo\\\\s?Finance)\\\\b',alltxt,re.I)))\n    metrics=sorted(set(re.findall(r'\\\\b(?:Sharpe ratio|Sortino ratio|maximum drawdown|max drawdown|MDD|cumulative return|annualized return|Calmar ratio|volatility|win rate|alpha|beta)\\\\b',alltxt,re.I)))\n    baselines=sorted(set(re.findall(r'\\\\b(?:buy[- ]and[- ]hold|Buy and Hold|moving average|MACD|RSI|DQN|PPO|A2C|FinMem|TradingAgents|FinAgent|GPT-4o|Claude|Qwen|DeepSeek|Llama)\\\\b',alltxt,re.I)))\n    notes[pid]={'num_chunks':len(chunks),'chunk_summaries':chunk_summ,'urls':urls[:20],'dataset_terms':dsets,'metric_terms':metrics,'baseline_terms':baselines}\n\nPath('artifacts').mkdir(exist_ok=True)\nPath('artifacts/deep_paper_notes.json').write_text(json.dumps(notes,indent=2))\nprint('Wrote artifacts/deep_paper_notes.json')\nfor k,v in notes.items():\n    print('\\\\n',k,'chunks=',v['num_chunks'],'datasets=',v['dataset_terms'][:8],'metrics=',v['metric_terms'][:8])\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_36","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pypdf import PdfReader\nfrom pathlib import Path\nimport re, json\n\nchunk_dir=Path('papers/pages')\nids=[\n'2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market',\n'2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market',\n'2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_',\n'2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_'\n]\nnotes={}\nfor pid in ids:\n    chunks=sorted(chunk_dir.glob(pid+'_chunk_*.pdf'))\n    chunk_summ=[]\n    full_text=[]\n    for c in chunks:\n        r=PdfReader(str(c))\n        txt='\\\\n'.join((p.extract_text() or '') for p in r.pages)\n        txt=re.sub(r'\\\\s+',' ',txt)\n        full_text.append(txt)\n        chunk_summ.append({'chunk':c.name,'chars':len(txt),'preview':txt[:600]})\n    alltxt=' '.join(full_text)\n    urls=sorted(set(re.findall(r'https?://[\"'^'\"\\\\s\\\\)\\\\]]+',alltxt)))\n    dsets=sorted(set(re.findall(r'\\\\b(?:S&P\\\\s?500|DJIA|Dow\\\\s?Jones|CSI\\\\s?300|NASDAQ|CRSP|Compustat|LOBSTER|FRED|Polymarket|A-share|Bitcoin|BTC|SPY|Russell\\\\s?1000|Yahoo\\\\s?Finance)\\\\b',alltxt,re.I)))\n    metrics=sorted(set(re.findall(r'\\\\b(?:Sharpe ratio|Sortino ratio|maximum drawdown|max drawdown|MDD|cumulative return|annualized return|Calmar ratio|volatility|win rate|alpha|beta)\\\\b',alltxt,re.I)))\n    baselines=sorted(set(re.findall(r'\\\\b(?:buy[- ]and[- ]hold|Buy and Hold|moving average|MACD|RSI|DQN|PPO|A2C|FinMem|TradingAgents|FinAgent|GPT-4o|Claude|Qwen|DeepSeek|Llama)\\\\b',alltxt,re.I)))\n    notes[pid]={'num_chunks':len(chunks),'chunk_summaries':chunk_summ,'urls':urls[:20],'dataset_terms':dsets,'metric_terms':metrics,'baseline_terms':baselines}\n\nPath('artifacts').mkdir(exist_ok=True)\nPath('artifacts/deep_paper_notes.json').write_text(json.dumps(notes,indent=2))\nprint('Wrote artifacts/deep_paper_notes.json')\nfor k,v in notes.items():\n    print('\\\\n',k,'chunks=',v['num_chunks'],'datasets=',v['dataset_terms'][:8],'metrics=',v['metric_terms'][:8])\nPY\"","aggregated_output":"Wrote artifacts/deep_paper_notes.json\n\n 2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market chunks= 5 datasets= ['DJIA', 'Dow Jones'] metrics= ['Max Drawdown', 'Maximum Drawdown', 'Sharpe ratio', 'Sortino Ratio', 'Sortino ratio', 'cumulative return', 'maximum drawdown', 'volatility']\n\n 2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market chunks= 8 datasets= ['Dow Jones', 'Nasdaq', 'S&P 500', 'Yahoo Finance'] metrics= ['Calmar Ratio', 'Calmar ratio', 'MDD', 'Sharpe Ratio', 'Sharpe ratio', 'annualized return', 'calmar ratio', 'cumulative return']\n\n 2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_ chunks= 5 datasets= ['S&P 500'] metrics= ['Alpha', 'Beta', 'Cumulative Return', 'MDD', 'Maximum Drawdown', 'Sharpe Ratio', 'Sharpe ratio', 'Sortino Ratio']\n\n 2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_ chunks= 3 datasets= ['S&P 500'] metrics= ['Alpha', 'alpha', 'volatility']\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_37","type":"reasoning","text":"**Planning dataset info extraction**"}}
{"type":"item.started","item":{"id":"item_38","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,260p' artifacts/deep_paper_notes.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_38","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,260p' artifacts/deep_paper_notes.json\"","aggregated_output":"{\n  \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market\": {\n    \"num_chunks\": 5,\n    \"chunk_summaries\": [\n      {\n        \"chunk\": \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_001.pdf\",\n        \"chars\": 10714,\n        \"preview\": \"STOCKBENCH: CANLLMAGENTSTRADESTOCKS PROFITABLYINREAL-WORLDMARKETS? Yanxu Chen\\u2660\\u2661\\u2217 \\u2020 Zijun Yao\\u2660\\u2217 Yantao Liu\\u2660\\u2217 Jin Ye\\u2661 Jianing Yu\\u2661 Lei Hou\\u2660 Juanzi Li\\u2660 \\u2660Tsinghua University \\u2661Beijing University of Posts and Telecommunications yaozj20@mails.tsinghua.edu.cn, cyx666@bupt.edu.cn Project:https://stockbench.github.io/ /cdeGitHub:https://github.com/ChenYXxxx/stockbench ABSTRACT Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains s\"\n      },\n      {\n        \"chunk\": \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_002.pdf\",\n        \"chars\": 11765,\n        \"preview\": \"across different industries in Figure 2. Our selection covers technology, finance, and manufacturing, ensuring stock diversity. Historical Market Data.We collect and preserve historical market data containing key quantitative information. For each stock, we use official opening prices together with a concise set of fundamen- tal metrics such as market capitalization, price-to-earnings (P/E) ratio, dividend yield, and trading range. These signals provide a reliable snapshot of company health and valuation, supporting in- formed decision making. We also retain the timestamps of the collected dat\"\n      },\n      {\n        \"chunk\": \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_003.pdf\",\n        \"chars\": 10800,\n        \"preview\": \"Table 3: Performance of representative models (Kimi-K2 and GPT-OSS-120B) across different investment target sizes. Results are reported as mean return (% Mean), standard deviation of re- turns (% Std), and coefficient of variation (CV). Stocks%Mean%Std CV Kimi-K2 5\\u22124.6 0.7 0.2 10 3.2 0.6 0.2 20 1.9 1.7 0.9 30\\u22120.5 1.2 2.2 GPT-OSS-120B 5\\u22125.7 0.3 0.1 10 2.5 0.4 0.2 20\\u22120.4 3.9 10.2 30\\u22120.9 3.9 4.4 Arithmetic Schema 0 5 10 15 5.6 8 14.5 6.4 Error Rate (%) Qwen3-235B-THK Qwen3-235B-INS Arithmetic Schema 0 2 4 6 8 4 2 5.2 0.4 Error Rate (%) DPSK-V3.1-THK DPSK-V3-INS Figure 3: Error distribution (%) by\"\n      },\n      {\n        \"chunk\": \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_004.pdf\",\n        \"chars\": 10753,\n        \"preview\": \"We believe that STOCKBENCHwill serve as a valuable resource for the research community, driv- ing further advancements in the development of intelligent, autonomous financial agents capable of navigating complex market dynamics. Future work will focus on enhancing the benchmark with ad- ditional market scenarios and exploring novel agent architectures to improve trading performance. REFERENCES Anthropic. Claude 3.7 Sonnet, 2025a. URLhttps://www.anthropic.com/news/ claude-3-7-sonnet. Anthropic. Introducing claude 4.https://www.anthropic.com/news/claude-4, May 2025b. Accessed: 2025-09-14. Shuai \"\n      },\n      {\n        \"chunk\": \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market_chunk_005.pdf\",\n        \"chars\": 5546,\n        \"preview\": \"Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.arXiv preprint arXiv:2503.14476, abs/2503.14476, 2025. URLhttps://arxiv. org/abs/2503.14476. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models.arXiv preprint arXiv:2508.06471, abs/2508.06471, 2025. URLhttps://arxiv. org/abs/2508.06471. Yue Zhang, Stefan Zohren, and S\"\n      }\n    ],\n    \"urls\": [\n      \"http://papers.nips.cc/paper_files/paper/2024/hash/\",\n      \"https://aclanthology.org/2021.acl-long.254/.\",\n      \"https://aclanthology.org/2025.acl-long.126/.\",\n      \"https://ai.meta.com/blog/llama-4-multimodal-intelligence/.\",\n      \"https://arxiv.\",\n      \"https://arxiv.org/abs/\",\n      \"https://arxiv.org/abs/.2505.19457.\",\n      \"https://arxiv.org/abs/.2508.00828.\",\n      \"https://arxiv.org/abs/2303.17564.\",\n      \"https://arxiv.org/abs/2308.00065.\",\n      \"https://arxiv.org/abs/2402.02315.\",\n      \"https://arxiv.org/abs/2412.19437.\",\n      \"https://arxiv.org/abs/2502.13923.\",\n      \"https://arxiv.org/abs/2505.14810.\",\n      \"https://arxiv.org/abs/2506.07982.\",\n      \"https://arxiv.org/abs/2506.13651.\",\n      \"https://blog.google/technology/\",\n      \"https://doi.org/10.18653/\",\n      \"https://doi.org/10.18653/v1/2021.emnlp-main.300.\",\n      \"https://finnhub.io/\"\n    ],\n    \"dataset_terms\": [\n      \"DJIA\",\n      \"Dow Jones\"\n    ],\n    \"metric_terms\": [\n      \"Max Drawdown\",\n      \"Maximum Drawdown\",\n      \"Sharpe ratio\",\n      \"Sortino Ratio\",\n      \"Sortino ratio\",\n      \"cumulative return\",\n      \"maximum drawdown\",\n      \"volatility\"\n    ],\n    \"baseline_terms\": [\n      \"Claude\",\n      \"DeepSeek\",\n      \"GPT-4o\",\n      \"Llama\",\n      \"buy-and-hold\",\n      \"claude\",\n      \"gpt-4o\",\n      \"llama\"\n    ]\n  },\n  \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market\": {\n    \"num_chunks\": 8,\n    \"chunk_summaries\": [\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_001.pdf\",\n        \"chars\": 12621,\n        \"preview\": \"FinPos: A Position-Aware Trading Agent System for Real Financial Markets Bijia Liu Dongbei University of Finance and Economics qq1403492787@gmail.com Ronghao Dang\\u2217 Alibaba DAMO Academy dangronghao.drh@alibaba-inc.com Abstract The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, most existing trading agents operate under intraday, inde- pendent unit-based trading tasks, where deci- sions are made as isolated directional actions, and thus lack awareness of continuous posi- tion manage\"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_002.pdf\",\n        \"chars\": 13009,\n        \"preview\": \"causal structures from complex semantic informa- tion rather than performing high-frequency pre- cise numerical optimization. This capability gives them greater potential in mid-to-long-term strategy formulation. Under theSingle-Step Trading Task setup, the agent primarily focuses on the price di- rection of the next step, which somewhat forces the LLM struggling to build coherent strategies or cognition. By introducing position continuity and strategy coherence, thePosition-Aware Trad- ing Taskprovides LLM agents with a more fitting testing platform, fully unlocking their potential in long-te\"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_003.pdf\",\n        \"chars\": 12791,\n        \"preview\": \"Table 1: Performance comparison of different models on five stocks Models TSLA AAPL AMZN NFLX COIN CR%\\u2191SR\\u2191MDD%\\u2193CR%\\u2191SR\\u2191MDD%\\u2193CR%\\u2191SR\\u2191MDD%\\u2193CR%\\u2191SR\\u2191MDD%\\u2193CR%\\u2191SR\\u2191MDD%\\u2193Market BaselineRandom\\u221225.81\\u22120.62 62.10\\u221216.46\\u22120.23 39.40\\u22127.80\\u22120.34 61.20\\u22123.04\\u22120.02 32.60 1.27 0.14 57.60Rule-Based MethodsMACD (Wang and Kim, 2018)\\u221246.25\\u22120.55 74.20\\u221229.61\\u22120.42 51.40\\u221238.14\\u22120.47 70.30\\u221227.45\\u22120.23 47.92\\u221249.87\\u22120.54 70.83RSI (Belafsky et al., 2002)\\u221245.06\\u22120.52 72.80\\u221233.28\\u22120.48 49.70\\u221235.10\\u22120.42 68.20\\u221226.98\\u22120.21 46.08\\u221253.65\\u22120.63 66.37Reinforcement Learning MethodsA3C (Kang et al., 2018)\\u221286.99\\u22120.94 96.60\\u221266.13\\u22120.75 73.40\\u221256.63\\u22120.62\"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_004.pdf\",\n        \"chars\": 11841,\n        \"preview\": \"Xiao-Yang Liu, Hongyang Yang, Qian Chen, Run- jia Zhang, Liuqing Yang, Bowen Xiao, and Christina Dan Wang. 2020. Finrl: A deep re- inforcement learning library for automated stock trading in quantitative finance.arXiv preprint arXiv:2011.09607. R Tyrrell Rockafellar, Stanislav Uryasev, and 1 oth- ers. 2000. Optimization of conditional value-at-risk. Journal of risk, 2:21\\u201342. Ashish Kumar Shakya, Gopinatha Pillai, and Sohom Chakrabarty. 2023. Reinforcement learning algo- rithms: A brief survey.Expert Systems with Applica- tions, 231:120495. Jian Wang and Junseok Kim. 2018. Predicting stock pric\"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_005.pdf\",\n        \"chars\": 12957,\n        \"preview\": \"dynamics , price fluctuations of key materials ( e . g . , lithium , batteries ) - Macroeconomic factors : macroeconomy , Federal Reserve policies , interest rates , inflation , employment , consumer spending , GDP growth , manufacturing indices , PMI , retail sales , and other macroeconomic indicators - Financial market sentiment : significant fluctuations or sustained trends in the S & P 500 , Nasdaq , Dow Jones indices ; market overheating , overbought conditions , or panic selling that may affect overall risk appetite ; valuation adjustments in tech / growth stock sectors ; market rotation\"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_006.pdf\",\n        \"chars\": 12434,\n        \"preview\": \"exposure. System Prompt: # memory IDs s h o r t _ m e m o r y _ i d _ d e s c = \\\" ID of short - term information .\\\" m id _ m em o r y_ i d _d e s c = \\\" ID of mid - term information .\\\" l o n g _ m e m o r y _ i d _ d e s c = \\\" ID of long - term information .\\\" r e f l e c t i o n _ m e m o r y _ i d _ d e s c = \\\" ID of reflective - period information .\\\" t r a i n _ m e m o r y _ i d _ e x t r a c t _ p r o m p t = \\\" Select and store the most investment - relevant information from major sources ( e . g . , ARK , Two Sigma , Bridgewater Associates ) into the { memory_layer } memory .\\\" t e s t _ m \"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_007.pdf\",\n        \"chars\": 10595,\n        \"preview\": \"Table 3: Overview of LLM-based Trading Agents and Their Architectures Method Backbone Agents / ModulesPosition-AwareInformation ProcessedDecision Structure FinPOS GPT-4o 10 agents: 4 filters + 4 analyzers + Direction Agent + Quantity/Risk Agent Yes (continuous po- sition state + CVaR sizing) Price, news, macro events, 10-K/10-Q reports Two-stage: (1) direction rea- soning; (2) CVaR-based posi- tion sizing; reflection with multi- timescale reward FinCon GPT-4o 7 agents: Data, News, Report, Analyst Re- port, Earnings Call, Stock Selection, Man- ager No (no explicit posi- tion sizing; only dis- c\"\n      },\n      {\n        \"chunk\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market_chunk_008.pdf\",\n        \"chars\": 1274,\n        \"preview\": \"Table 8: Extended results on TSLA, AAPL, and COIN (Mar\\u2013Sep 2025). Models TSLA AAPL COIN CR%\\u2191SR\\u2191MDD%\\u2193CR%\\u2191SR\\u2191MDD%\\u2193CR%\\u2191SR\\u2191MDD%\\u2193 Baseline Random\\u221232.13\\u22120.91 62.10\\u221234.21\\u22120.28 59.40\\u22120.22\\u22120.01 17.60 DRL PPO (Li et al., 2025)\\u221273.76\\u22120.56 69.88\\u221258.33\\u22120.55 61.40\\u22122.04\\u22120.04 17.80 LLM-Based Agents FinGPT (Yang et al., 2023)\\u221295.19\\u22121.07 94.36\\u221285.22\\u22121.52 74.60\\u22123.63\\u22120.04 17.00 FinAgent (Zhang et al., 2024)\\u221274.31\\u22120.89 85.65\\u221278.00\\u22121.09 71.50\\u22122.10\\u22120.03 17.50 FINMEM (Yu et al., 2024a)\\u221244.03\\u22120.52 72.10\\u221245.88\\u22120.54 47.80 0.13 0.01 19.20 FINCON (Yu et al., 2024b)7.76 0.38 59.13\\u221216.02\\u22120.13 29.03 7.351.0315.10 FinPos54.99\"\n      }\n    ],\n    \"urls\": [],\n    \"dataset_terms\": [\n      \"Dow Jones\",\n      \"Nasdaq\",\n      \"S&P 500\",\n      \"Yahoo Finance\"\n    ],\n    \"metric_terms\": [\n      \"Calmar Ratio\",\n      \"Calmar ratio\",\n      \"MDD\",\n      \"Sharpe Ratio\",\n      \"Sharpe ratio\",\n      \"annualized return\",\n      \"calmar ratio\",\n      \"cumulative return\",\n      \"maximum drawdown\",\n      \"sharpe ratio\",\n      \"volatility\"\n    ],\n    \"baseline_terms\": [\n      \"A2C\",\n      \"DQN\",\n      \"Deepseek\",\n      \"FINMEM\",\n      \"FinAgent\",\n      \"FinMEM\",\n      \"FinMem\",\n      \"Finmem\",\n      \"GPT-4o\",\n      \"Gpt-4o\",\n      \"MACD\",\n      \"PPO\",\n      \"RSI\",\n      \"TradingAgents\",\n      \"Tradingagents\",\n      \"llama\",\n      \"macd\",\n      \"ppo\",\n      \"rsi\"\n    ]\n  },\n  \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_\": {\n    \"num_chunks\": 5,\n    \"chunk_summaries\": [\n      {\n        \"chunk\": \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_001.pdf\",\n        \"chars\": 16130,\n        \"preview\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run? Weixian Waylon Li AIAI, School of Informatics The University of Edinburgh Edinburgh, United Kingdom waylon.li@ed.ac.uk Hyeonjun Kim Global Finance Research Center Sungkyunkwan University Seoul, Republic of Korea hjkimfin@gmail.com Mihai Cucuringu Dept. of Mathematics; Dept. of Statistics & OMI University of California, Los Angeles; University of Oxford United States; United Kingdom mihai@math.ucla.edu Tiejun Ma AIAI, School of Informatics The University of Edinburgh Edinburgh, United Kingdom tiejun.ma@ed.ac.uk Abst\"\n      },\n      {\n        \"chunk\": \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_002.pdf\",\n        \"chars\": 16213,\n        \"preview\": \"KDD \\u201926, August 09\\u201313, 2026, Jeju Island, Republic of Korea Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, and Tiejun Ma Type Strategy TSLA NFLX AMZN MSFT SPR\\u2191CR\\u2191MDD\\u2191AV\\u2193SPR\\u2191CR\\u2191MDD\\u2191AV\\u2193SPR\\u2191CR\\u2191MDD\\u2191AV\\u2193SPR\\u2191CR\\u2191MDD\\u2191AV\\u2193 FinMem Selection (2022-10-06 to 2023-04-10) Rule Based Buy and Hold -0.342 -20.483 -52.729 55.910 1.32643.079-20.184 41.523 -0.460 -13.250 -31.546 35.624 0.974 21.171 -14.192 28.327 SMA Cross -0.293 -5.540 -18.517 38.602 -1.020 -8.285 -15.942 20.477 -0.420 -4.433 -18.910 27.084 1.515 18.289 -8.746 20.821 WMA Cross 0.215 3.741 -18.492 42.062 -0.803 -6.004 -14.290 19.826 -0.563 -6.121\"\n      },\n      {\n        \"chunk\": \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_003.pdf\",\n        \"chars\": 19474,\n        \"preview\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run? KDD \\u201926, August 09\\u201313, 2026, Jeju Island, Republic of Korea a clear need for improved risk management within LLM-driven approaches before they can be reliably adopted in practice. Moreover, by comparingBuy and Holdwith different selection strategies, we clearly identify the relative effectiveness of each se- lection strategy:Volatility Effectselection (Sharpe 0.703) outper- formsFinCon Selection Agent(0.389) andMomentum Factor (0.384), which in turn surpassRandom Five(0.315). RL-based meth- ods exhibit the clearest\"\n      },\n      {\n        \"chunk\": \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_004.pdf\",\n        \"chars\": 20898,\n        \"preview\": \"KDD \\u201926, August 09\\u201313, 2026, Jeju Island, Republic of Korea Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, and Tiejun Ma [24] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large Language Model Based Multi-agents: A Survey of Progress and Challenges. InProceedings of the Thirty- Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024. 8048\\u20138057. [25] Campbell Harvey and Yan Liu. 2013. Backtesting.SSRN Electronic Journal42 (2013). [26] Yifan Hu, Yuante Li, Peiy\"\n      },\n      {\n        \"chunk\": \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in__chunk_005.pdf\",\n        \"chars\": 13232,\n        \"preview\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run? KDD \\u201926, August 09\\u201313, 2026, Jeju Island, Republic of Korea TSLA AMZN NIO MSFT Type Strategy SPR CR MDD SPR CR MDD SPR CR MDD SPR CR MDD FinCon Selection (2022-10-05 to 2023-06-10) Buy And Hold 0.247 2.056 -54.508 0.150 2.193 -32.177 -0.858 -51.569 -53.563 1.071 32.629 -14.452 SMA Cross -0.151 -3.973 -23.1730.599 13.731-18.910 0.810 22.047-17.976 1.641 32.057-8.746 WMA Cross 1.104 32.058 -18.492 0.513 11.765 -21.030 -0.771 -9.412 -18.732 1.526 30.344 -8.883 ATR Band -0.554 -22.136 -39.599 0.494 11.007-15.8420.681 2\"\n      }\n    ],\n    \"urls\": [\n      \"http://www.ecdf.ed.ac.uk/\",\n      \"https://alpaca.markets/\",\n      \"https://doi.org/10.\",\n      \"https://doi.org/10.1145/3770854.3785702\",\n      \"https://github.com/waylonli/FINSABER.\",\n      \"https://paperswithbacktest.com/\",\n      \"https://platform.openai.com/docs/models/text-embedding-ada-002\",\n      \"https://sec-api.io/\",\n      \"https://www.backtrader.com/\",\n      \"https://www.lseg.com/en\",\n      \"https://www.moomoo.com/ca/support/topic10_122\",\n      \"https://www.tradeup.com/pricing/detail\"\n    ],\n    \"dataset_terms\": [\n      \"S&P 500\"\n    ],\n    \"metric_terms\": [\n      \"Alpha\",\n      \"Beta\",\n      \"Cumulative Return\",\n      \"MDD\",\n      \"Maximum Drawdown\",\n      \"Sharpe Ratio\",\n      \"Sharpe ratio\",\n      \"Sortino Ratio\",\n      \"Volatility\",\n      \"alpha\",\n      \"beta\",\n      \"cumulative return\",\n      \"maximum drawdown\",\n      \"volatility\"\n    ],\n    \"baseline_terms\": [\n      \"A2C\",\n      \"Buy And Hold\",\n      \"Buy and Hold\",\n      \"Buy-and-Hold\",\n      \"FinAgent\",\n      \"FinMem\",\n      \"GPT-4o\",\n      \"LLaMA\",\n      \"Moving Average\",\n      \"PPO\",\n      \"Qwen\",\n      \"TradingAgents\",\n      \"gpt-4o\"\n    ]\n  },\n  \"2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_\": {\n    \"num_chunks\": 3,\n    \"chunk_summaries\": [\n      {\n        \"chunk\": \"2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__chunk_001.pdf\",\n        \"chars\": 12678,\n        \"preview\": \"arXiv:2601.13770v1 [cs.AI] 20 Jan 2026 Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance Mostapha Benhenda * Abstract We introduce Look-Ahead-Bench, a standard- ized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical \\ufb01nancial work\\ufb02ows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine pre- dictive capability from memorization-based per- formance, we analyze per\"\n      },\n      {\n        \"chunk\": \"2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__chunk_002.pdf\",\n        \"chars\": 12213,\n        \"preview\": \"Look-Ahead-Bench: Standardized Benchmark of Lookahead Bi as in PiT LLMs broad market exposure while controlling concentration risk, often outperforming naive buy-and-hold in diversi\\ufb01ed uni- verses due to the rebalancing premium. 3.4.3. M OMENTUM (3-M ONTH , T OP HALF ) A classic trend-following strategy that, at each monthly rebalance, ranks tickers by their past 3-month total re- turn and allocates equally to the top half (or all tickers equally if insuf\\ufb01cient history). This exploits persistenc e in intermediate-term price trends and is one of the most robust quantitative factors historically\"\n      },\n      {\n        \"chunk\": \"2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point__chunk_003.pdf\",\n        \"chars\": 5352,\n        \"preview\": \"Look-Ahead-Bench: Standardized Benchmark of Lookahead Bi as in PiT LLMs Li, X., Zeng, Y ., Xing, X., Xu, J., and Xu, X. HedgeAgents: A Balanced-aware Multi- agent Financial Trading System, 2025a. URL https://arxiv.org/abs/2502.13165. Li, X., Zeng, Y ., Xing, X., Xu, J., and Xu, X. Pro\\ufb01t Mirage: Revisiting Information Leak- age in LLM-based Financial Agents, 2025b. URL https://arxiv.org/abs/2510.07920. Li, Y ., Y u, Y ., Li, H., Chen, Z., and Khashanah, K. TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance, 2023. URL https://arx\"\n      }\n    ],\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_39","type":"reasoning","text":"**Planning financial datasets download**"}}
{"type":"item.completed","item":{"id":"item_40","type":"agent_message","text":"I’m moving to dataset collection now. I’ll download one market time-series dataset (multi-asset daily OHLCV) plus one finance-text benchmark dataset, validate schema/sample counts, and write reproducible download instructions in `datasets/README.md`."}}
{"type":"item.started","item":{"id":"item_41","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nimport json\nimport pandas as pd\nimport yfinance as yf\nfrom datasets import load_dataset\n\nroot=Path('datasets')\nroot.mkdir(exist_ok=True)\n\n# Dataset 1: Daily OHLCV for Dow 30 subset + SPY over long horizon\nsymbols=['AAPL','MSFT','JPM','GS','V','JNJ','WMT','DIS','CAT','IBM','KO','MCD','NKE','NVDA','SPY']\nstart='2010-01-01'\nend='2026-01-31'\nframes=[]\nfor s in symbols:\n    df=yf.download(s,start=start,end=end,auto_adjust=False,progress=False,threads=False)\n    if df is None or df.empty:\n        continue\n    df=df.reset_index()\n    if isinstance(df.columns,pd.MultiIndex):\n        df.columns=[c[0] if isinstance(c,tuple) else c for c in df.columns]\n    df['ticker']=s\n    frames.append(df[['Date','Open','High','Low','Close','Adj Close','Volume','ticker']])\n\nprices=pd.concat(frames,ignore_index=True)\nout_dir=root/'market_daily_ohlcv_2010_2026'\nout_dir.mkdir(exist_ok=True)\nprices.to_parquet(out_dir/'prices.parquet',index=False)\nprices.head(100).to_csv(out_dir/'sample_100_rows.csv',index=False)\n\nstats1={\n    'rows':int(len(prices)),\n    'tickers':sorted(prices['ticker'].unique().tolist()),\n    'date_min':str(prices['Date'].min().date()),\n    'date_max':str(prices['Date'].max().date()),\n    'columns':prices.columns.tolist()\n}\n\n# Dataset 2: Financial PhraseBank (sentiment)\nfp=load_dataset('financial_phrasebank','sentences_allagree')\nfp_dir=root/'financial_phrasebank_allagree'\nfp.save_to_disk(str(fp_dir/'hf_dataset'))\ntrain=fp['train']\nsample=[train[i] for i in range(min(10,len(train)))]\n(fp_dir/'samples').mkdir(exist_ok=True)\nwith open(fp_dir/'samples'/'samples.json','w') as f:\n    json.dump(sample,f,indent=2)\n\nstats2={\n    'rows':len(train),\n    'label_names':train.features['label'].names,\n    'columns':list(train.features.keys())\n}\n\nwith open(root/'dataset_stats.json','w') as f:\n    json.dump({'market_daily_ohlcv_2010_2026':stats1,'financial_phrasebank_allagree':stats2},f,indent=2)\n\nprint('Saved datasets')\nprint(json.dumps({'market_daily_ohlcv_2010_2026':stats1,'financial_phrasebank_allagree':stats2},indent=2))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_41","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nimport json\nimport pandas as pd\nimport yfinance as yf\nfrom datasets import load_dataset\n\nroot=Path('datasets')\nroot.mkdir(exist_ok=True)\n\n# Dataset 1: Daily OHLCV for Dow 30 subset + SPY over long horizon\nsymbols=['AAPL','MSFT','JPM','GS','V','JNJ','WMT','DIS','CAT','IBM','KO','MCD','NKE','NVDA','SPY']\nstart='2010-01-01'\nend='2026-01-31'\nframes=[]\nfor s in symbols:\n    df=yf.download(s,start=start,end=end,auto_adjust=False,progress=False,threads=False)\n    if df is None or df.empty:\n        continue\n    df=df.reset_index()\n    if isinstance(df.columns,pd.MultiIndex):\n        df.columns=[c[0] if isinstance(c,tuple) else c for c in df.columns]\n    df['ticker']=s\n    frames.append(df[['Date','Open','High','Low','Close','Adj Close','Volume','ticker']])\n\nprices=pd.concat(frames,ignore_index=True)\nout_dir=root/'market_daily_ohlcv_2010_2026'\nout_dir.mkdir(exist_ok=True)\nprices.to_parquet(out_dir/'prices.parquet',index=False)\nprices.head(100).to_csv(out_dir/'sample_100_rows.csv',index=False)\n\nstats1={\n    'rows':int(len(prices)),\n    'tickers':sorted(prices['ticker'].unique().tolist()),\n    'date_min':str(prices['Date'].min().date()),\n    'date_max':str(prices['Date'].max().date()),\n    'columns':prices.columns.tolist()\n}\n\n# Dataset 2: Financial PhraseBank (sentiment)\nfp=load_dataset('financial_phrasebank','sentences_allagree')\nfp_dir=root/'financial_phrasebank_allagree'\nfp.save_to_disk(str(fp_dir/'hf_dataset'))\ntrain=fp['train']\nsample=[train[i] for i in range(min(10,len(train)))]\n(fp_dir/'samples').mkdir(exist_ok=True)\nwith open(fp_dir/'samples'/'samples.json','w') as f:\n    json.dump(sample,f,indent=2)\n\nstats2={\n    'rows':len(train),\n    'label_names':train.features['label'].names,\n    'columns':list(train.features.keys())\n}\n\nwith open(root/'dataset_stats.json','w') as f:\n    json.dump({'market_daily_ohlcv_2010_2026':stats1,'financial_phrasebank_allagree':stats2},f,indent=2)\n\nprint('Saved datasets')\nprint(json.dumps({'market_daily_ohlcv_2010_2026':stats1,'financial_phrasebank_allagree':stats2},indent=2))\nPY\"","aggregated_output":"Traceback (most recent call last):\n  File \"<stdin>\", line 40, in <module>\n  File \"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 1488, in load_dataset\n    builder_instance = load_dataset_builder(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 1133, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n                     ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 1032, in dataset_module_factory\n    raise e1 from None\n  File \"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex/.venv/lib/python3.12/site-packages/datasets/load.py\", line 992, in dataset_module_factory\n    raise RuntimeError(f\"Dataset scripts are no longer supported, but found {filename}\")\nRuntimeError: Dataset scripts are no longer supported, but found financial_phrasebank.py\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_42","type":"reasoning","text":"**Noting long-running poll**"}}
{"type":"item.completed","item":{"id":"item_43","type":"reasoning","text":"**Evaluating alternative datasets**"}}
{"type":"item.completed","item":{"id":"item_44","type":"agent_message","text":"The market dataset download succeeded, but HuggingFace loading failed for `financial_phrasebank` due a script-only dataset format in `datasets==4.5.0`. I’ll switch to a script-free finance text dataset on HF and continue validation."}}
{"type":"item.started","item":{"id":"item_45","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom datasets import get_dataset_config_names\nfor name in ['takala/financial_phrasebank','zeroshot/twitter-financial-news-sentiment','ChanceFocus/fiqa-sentiment-classification']:\n    try:\n        cfg=get_dataset_config_names(name)\n        print(name,'OK configs',cfg[:5])\n    except Exception as e:\n        print(name,'ERR',e)\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_45","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom datasets import get_dataset_config_names\nfor name in ['takala/financial_phrasebank','zeroshot/twitter-financial-news-sentiment','ChanceFocus/fiqa-sentiment-classification']:\n    try:\n        cfg=get_dataset_config_names(name)\n        print(name,'OK configs',cfg[:5])\n    except Exception as e:\n        print(name,'ERR',e)\nPY\"","aggregated_output":"takala/financial_phrasebank ERR Dataset scripts are no longer supported, but found financial_phrasebank.py\nzeroshot/twitter-financial-news-sentiment OK configs ['default']\nChanceFocus/fiqa-sentiment-classification ERR Dataset 'ChanceFocus/fiqa-sentiment-classification' doesn't exist on the Hub or cannot be accessed.\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc \"pwd && mkdir -p papers datasets code && pwd && ls -la .claude/skills/paper-finder/scripts && source .venv/bin/activate && python .claude/skills/paper-finder/scripts/find_papers.py \\\"LLM agents long-term trading decisions finance\\\" --mode diligent --format json > paper_search.json && wc -c paper_search.json && sed -n '1,220p' paper_search.json\"","aggregated_output":"\nResults saved to: paper_search_results/LLM_agents_long-term_trading_decisions_finance_20260222_014009.jsonl\n542543 paper_search.json\n{\n  \"success\": true,\n  \"total\": 343,\n  \"papers\": [\n    {\n      \"title\": \"StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?\",\n      \"year\": 2025,\n      \"authors\": \"Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:281724665\",\n      \"relevance\": 3,\n      \"abstract\": \"Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.\",\n      \"citations\": 7\n    },\n    {\n      \"title\": \"FinPos: A Position-Aware Trading Agent System for Real Financial Markets\",\n      \"year\": 2025,\n      \"authors\": \"Bijia Liu, Ronghao Dang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282719341\",\n      \"relevance\": 3,\n      \"abstract\": \"The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, most existing trading agents operate under intraday, independent unit-based trading tasks, where decisions are made as isolated directional actions, and thus lack awareness of continuous position management. Therefore, we propose a position-aware trading task designed to simulate a more realistic market. To address this task, we propose FinPos, a position-aware trading agent system designed to explicitly model and manage continuous positions. FinPos enhances position awareness through three key mechanisms: (1) professional-level interpretation of heterogeneous market information; (2) a dual-agent decision structure that separates directional reasoning from risk-aware position adjustment; and (3) multi-timescale reward signals, allowing the agent to internalize position awareness through experiential feedback rather than static instructions alone. Extensive experiments demonstrate that FinPos surpasses state-of-the-art trading agents in the position-aware trading task, which closely mirrors real market conditions. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-term market decision-making.\",\n      \"citations\": 2\n    },\n    {\n      \"title\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?\",\n      \"year\": 2025,\n      \"authors\": \"Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278501425\",\n      \"relevance\": 3,\n      \"abstract\": \"Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.\",\n      \"citations\": 8\n    },\n    {\n      \"title\": \"Language Model Guided Reinforcement Learning in Quantitative Trading\",\n      \"year\": 2025,\n      \"authors\": \"Adam Darmanin, Vince Vella\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280421329\",\n      \"relevance\": 3,\n      \"abstract\": \"Algorithmic trading requires short-term tactical decisions consistent with long-term financial objectives. Reinforcement Learning (RL) has been applied to such problems, but adoption is limited by myopic behaviour and opaque policies. Large Language Models (LLMs) offer complementary strategic reasoning and multi-modal signal interpretation when guided by well-structured prompts. This paper proposes a hybrid framework in which LLMs generate high-level trading strategies to guide RL agents. We evaluate (i) the economic rationale of LLM-generated strategies through expert review, and (ii) the performance of LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and Maximum Drawdown (MDD). Empirical results indicate that LLM guidance improves both return and risk metrics relative to standard RL.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"Hierarchical Organization Simulacra in the Investment Sector\",\n      \"year\": 2024,\n      \"authors\": \"Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:273022534\",\n      \"relevance\": 3,\n      \"abstract\": \"This paper explores designing artificial organizations with professional behavior in investments using a multi-agent simulation. The method mimics hierarchical decision-making in investment firms, using news articles to inform decisions. A large-scale study analyzing over 115,000 news articles of 300 companies across 15 years compared this approach against professional traders' decisions. Results show that hierarchical simulations align closely with professional choices, both in frequency and profitability. However, the study also reveals biases in decision-making, where changes in prompt wording and perceived agent seniority significantly influence outcomes. This highlights both the potential and limitations of large language models in replicating professional financial decision-making.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading\",\n      \"year\": 2025,\n      \"authors\": \"Zheye Deng, Jiashu Wang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282139067\",\n      \"relevance\": 3,\n      \"abstract\": \"While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"QuantAgents: Towards Multi-agent Financial System via Simulated Trading\",\n      \"year\": 2025,\n      \"authors\": \"Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:281843943\",\n      \"relevance\": 3,\n      \"abstract\": \"In this paper, our objective is to develop a multi-agent financial system that incorporates simulated trading, a technique extensively utilized by financial professionals. While current LLM-based agent models demonstrate competitive performance, they still exhibit significant deviations from real-world fund companies. A critical distinction lies in the agents'reliance on ``post-reflection'', particularly in response to adverse outcomes, but lack a distinctly human capability: long-term prediction of future trends. Therefore, we introduce QuantAgents, a multi-agent system integrating simulated trading, to comprehensively evaluate various investment strategies and market scenarios without assuming actual risks. Specifically, QuantAgents comprises four agents: a simulated trading analyst, a risk control analyst, a market news analyst, and a manager, who collaborate through several meetings. Moreover, our system incentivizes agents to receive feedback on two fronts: performance in real-world markets and predictive accuracy in simulated trading. Extensive experiments demonstrate that our framework excels across all metrics, yielding an overall return of nearly 300% over the three years (https://quantagents.github.io/).\",\n      \"citations\": 4\n    },\n    {\n      \"title\": \"GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents\",\n      \"year\": 2025,\n      \"authors\": \"Yejin Kim, Youngbin Lee, Juhyeon Kim, Yongjae Lee\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:281724135\",\n      \"relevance\": 3,\n      \"abstract\": \"This study demonstrates that GuruAgents, prompt-guided AI agents, can systematically operationalize the strategies of legendary investment gurus. We develop five distinct GuruAgents, each designed to emulate an iconic investor, by encoding their distinct philosophies into LLM prompts that integrate financial tools and a deterministic reasoning pipeline. In a backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique behaviors driven by their prompted personas. The Buffett GuruAgent achieves the highest performance, delivering a 42.2\\\\% CAGR that significantly outperforms benchmarks, while other agents show varied results. These findings confirm that prompt engineering can successfully translate the qualitative philosophies of investment gurus into reproducible, quantitative strategies, highlighting a novel direction for automated systematic investing. The source code and data are available at https://github.com/yejining99/GuruAgents.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"HedgeAgents: A Balanced-aware Multi-agent Financial Trading System\",\n      \"year\": 2025,\n      \"authors\": \"Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:276449781\",\n      \"relevance\": 3,\n      \"abstract\": \"As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ''hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).\",\n      \"citations\": 23\n    },\n    {\n      \"title\": \"Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance\",\n      \"year\": 2026,\n      \"authors\": \"Mostapha Benhenda\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:284911867\",\n      \"relevance\": 3,\n      \"abstract\": \"We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"Hierarchical AI Multi-Agent Fundamental Investing: Evidence from China's A-Share Market\",\n      \"year\": 2025,\n      \"authors\": \"Chujun He, Zhonghao Huang, Xiangguo Li, Ye Luo, Kewei Ma, Yuxuan Xiong, Xiaowei Zhang, Mingyang Zhao\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:282384801\",\n      \"relevance\": 3,\n      \"abstract\": \"We present a multi-agent, AI-driven framework for fundamental investing that integrates macro indicators, industry-level and firm-specific information to construct optimized equity portfolios. The architecture comprises: (i) a Macro agent that dynamically screens and weights sectors based on evolving economic indicators and industry performance; (ii) four firm-level agents -- Fundamental, Technical, Report, and News -- that conduct in-depth analyses of individual firms to ensure both breadth and depth of coverage; (iii) a Portfolio agent that uses reinforcement learning to combine the agent outputs into a unified policy to generate the trading strategy; and (iv) a Risk Control agent that adjusts portfolio positions in response to market volatility. We evaluate the system on the constituents by the CSI 300 Index of China's A-share market and find that it consistently outperforms standard benchmarks and a state-of-the-art multi-agent trading system on risk-adjusted returns and drawdown control. Our core contribution is a hierarchical multi-agent design that links top-down macro screening with bottom-up fundamental analysis, offering a robust and extensible approach to factor-based portfolio construction.\",\n      \"citations\": 0\n    },\n    {\n      \"title\": \"HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization\",\n      \"year\": 2025,\n      \"authors\": \"Benjamin Coriat, E. Benhamou\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:280017786\",\n      \"relevance\": 3,\n      \"abstract\": \"This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"RAPTOR: Reasoned Agentic Portfolio Trading with Orchestrated Rebalancing\",\n      \"year\": null,\n      \"authors\": \"Blake Almon, Matthew Caliboso, Alex Kim, Rohan Dutta, Rohan Raman, Mithil Srungarapu, Vasu Sharma, Kevin Zhu, Sunishchal Dev\",\n      \"url\": \"https://www.semanticscholar.org/paper/b2bf7b437f879e222f293046d09e234a8a22db1c\",\n      \"relevance\": 3,\n      \"abstract\": \"\",\n      \"citations\": 1\n    },\n    {\n      \"title\": \"FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design\",\n      \"year\": 2023,\n      \"authors\": \"Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Jordan W. Suchow, Denghui Zhang, K. Khashanah\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:265445755\",\n      \"relevance\": 1,\n      \"abstract\": \"We introduce FinMem, a novel Large Language Models (LLM)-based agent framework for financial trading, designed to address the need for automated systems that can transform real-time data into executable decisions. FinMem comprises three core modules: Profile for customizing agent characteristics, Memory for hierarchical financial data assimilation, and Decision-making for converting insights into investment choices. The Memory module, which mimics human traders\\u2019 cognitive structure, offers interpretability and real-time tuning while handling the critical timing of various information types. It employs a layered approach to process and prioritize data based on its timeliness and relevance, ensuring that the most recent and impactful information is given appropriate weight in decision-making. FinMem\\u2019s adjustable cognitive span allows retention of critical information beyond human limits, enabling it to balance historical patterns with current market dynamics. This framework facilitates self-evolution of professional knowledge, agile reactions to investment cues, and continuous refinement of trading decisions in financial environments. When compared against advanced algorithmic agents using a large-scale real-world financial dataset, FinMem demonstrates superior performance across classic metrics like Cumulative Return and Sharpe ratio. Further tuning of the agent\\u2019s perceptual span and character setting enhances its trading performance, positioning FinMem as a cutting-edge solution for automated trading.\",\n      \"citations\": 148\n    },\n    {\n      \"title\": \"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist\",\n      \"year\": 2024,\n      \"authors\": \"Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:268041696\",\n      \"relevance\": 1,\n      \"abstract\": \"Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.\",\n      \"citations\": 126\n    },\n    {\n      \"title\": \"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making\",\n      \"year\": 2024,\n      \"authors\": \"Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Rong Liu, Zhenyu Cui, Denghui Zhang, K. Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:271064881\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management.\",\n      \"citations\": 97\n    },\n    {\n      \"title\": \"A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges\",\n      \"year\": 2024,\n      \"authors\": \"Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Poor, Qingsong Wen, Stefan Zohren\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:270562262\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.\",\n      \"citations\": 126\n    },\n    {\n      \"title\": \"TradingAgents: Multi-Agents LLM Financial Trading Framework\",\n      \"year\": 2024,\n      \"authors\": \"Yijia Xiao, Edward Sun, Di Luo, Wei Wang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:275133732\",\n      \"relevance\": 1,\n      \"abstract\": \"Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, the multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. TradingAgents is available at https://github.com/TauricResearch/TradingAgents.\",\n      \"citations\": 92\n    },\n    {\n      \"title\": \"TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance\",\n      \"year\": 2023,\n      \"authors\": \"Yang Li, Yangyang Yu, Haohang Li, Z. Chen, K. Khashanah\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:261582775\",\n      \"relevance\": 1,\n      \"abstract\": \"Large Language Models (LLMs), prominently highlighted by the recent evolution in the Generative Pre-trained Transformers (GPT) series, have displayed significant prowess across various domains, such as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of GPTs lies in their ability to decode human instructions, achieved through comprehensively processing historical inputs as an entirety within their memory system. Yet, the memory processing of GPTs does not precisely emulate the hierarchical nature of human memory. This can result in LLMs struggling to prioritize immediate and critical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework endowed with layered memories. We assert that this framework is well-suited for stock and fund trading, where the extraction of highly relevant insights from hierarchical financial data is imperative to inform trading decisions. Within this framework, one agent organizes memory into three distinct layers, each governed by a custom decay mechanism, aligning more closely with human cognitive processes. Agents can also engage in inter-agent debate. In financial trading contexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system to integrate multi-source historical actions and market insights. This equips them to navigate financial changes, formulate strategies, and debate with peer agents about investment decisions. Another standout feature of our approach is to equip agents with individualized trading traits, enhancing memory diversity and decision robustness. These sophisticated designs boost the system's responsiveness to historical trades and real-time market signals, ensuring superior automated trading accuracy.\",\n      \"citations\": 86\n    },\n    {\n      \"title\": \"From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review\",\n      \"year\": 2025,\n      \"authors\": \"M. Ferrag, Norbert Tihanyi, M. Debbah\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:278165282\",\n      \"relevance\": 1,\n      \"abstract\": \"Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.\",\n      \"citations\": 97\n    },\n    {\n      \"title\": \"Large Language Model Agent in Financial Trading: A Survey\",\n      \"year\": 2024,\n      \"authors\": \"Han Ding, Yinheng Li, Junhao Wang, Hang Chen\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:271860230\",\n      \"relevance\": 1,\n      \"abstract\": \"Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.\",\n      \"citations\": 53\n    },\n    {\n      \"title\": \"Large Language Model Agent: A Survey on Methodology, Applications and Challenges\",\n      \"year\": 2025,\n      \"authors\": \"Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiaoming Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Mengxue Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xianhong Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, Ming Zhang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:277350072\",\n      \"relevance\": 1,\n      \"abstract\": \"The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.\",\n      \"citations\": 112\n    },\n    {\n      \"title\": \"INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent\",\n      \"year\": 2024,\n      \"authors\": \"Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, K. Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, Jordan W. Suchow\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:274992211\",\n      \"relevance\": 1,\n      \"abstract\": \"Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \\\\textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.\",\n      \"citations\": 26\n    },\n    {\n      \"title\": \"FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models\",\n      \"year\": 2024,\n      \"authors\": \"Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, Chris Wang\",\n      \"url\": \"https://api.semanticscholar.org/CorpusId:269983596\",\n      \"relevance\": 1,\n      \"abstract\": \"As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \\\\url{https://github.com/AI4Finance-Foundation/FinRobot}.\",\n      \"citations\": 34\n    },\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_46","type":"reasoning","text":"**Running dataset stats generation**"}}
{"type":"item.started","item":{"id":"item_47","type":"command_execution","command":"/bin/bash -lc \"pwd && find datasets -maxdepth 3 -type f | sed -n '1,200p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_47","type":"command_execution","command":"/bin/bash -lc \"pwd && find datasets -maxdepth 3 -type f | sed -n '1,200p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ndatasets/market_daily_ohlcv_2010_2026/prices.parquet\ndatasets/market_daily_ohlcv_2010_2026/sample_100_rows.csv\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_48","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nimport json\nfrom datasets import load_dataset\n\nroot=Path('datasets')\nname='zeroshot/twitter-financial-news-sentiment'\nds=load_dataset(name)\nout=root/'twitter_financial_news_sentiment'\nout.mkdir(exist_ok=True)\nds.save_to_disk(str(out/'hf_dataset'))\ntrain=ds['train']\n(out/'samples').mkdir(exist_ok=True)\nwith open(out/'samples'/'samples.json','w') as f:\n    json.dump([train[i] for i in range(10)],f,indent=2)\nprint('rows_train',len(train),'rows_val',len(ds['validation']))\nprint('features',list(train.features.keys()))\nprint('labels',train.features['label'].names)\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_48","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nimport json\nfrom datasets import load_dataset\n\nroot=Path('datasets')\nname='zeroshot/twitter-financial-news-sentiment'\nds=load_dataset(name)\nout=root/'twitter_financial_news_sentiment'\nout.mkdir(exist_ok=True)\nds.save_to_disk(str(out/'hf_dataset'))\ntrain=ds['train']\n(out/'samples').mkdir(exist_ok=True)\nwith open(out/'samples'/'samples.json','w') as f:\n    json.dump([train[i] for i in range(10)],f,indent=2)\nprint('rows_train',len(train),'rows_val',len(ds['validation']))\nprint('features',list(train.features.keys()))\nprint('labels',train.features['label'].names)\nPY\"","aggregated_output":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n\rGenerating train split:   0%|          | 0/9543 [00:00<?, ? examples/s]\rGenerating train split: 100%|██████████| 9543/9543 [00:00<00:00, 374181.95 examples/s]\n\rGenerating validation split:   0%|          | 0/2388 [00:00<?, ? examples/s]\rGenerating validation split: 100%|██████████| 2388/2388 [00:00<00:00, 333977.92 examples/s]\n\rSaving the dataset (0/1 shards):   0%|          | 0/9543 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 9543/9543 [00:00<00:00, 1839948.66 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 9543/9543 [00:00<00:00, 1793531.53 examples/s]\n\rSaving the dataset (0/1 shards):   0%|          | 0/2388 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 2388/2388 [00:00<00:00, 749251.79 examples/s]\rSaving the dataset (1/1 shards): 100%|██████████| 2388/2388 [00:00<00:00, 721457.75 examples/s]\nrows_train 9543 rows_val 2388\nfeatures ['text', 'label']\nTraceback (most recent call last):\n  File \"<stdin>\", line 17, in <module>\nAttributeError: 'Value' object has no attribute 'names'\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_49","type":"reasoning","text":"**Adjusting label handling and rerunning**"}}
{"type":"item.started","item":{"id":"item_50","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nimport json\nimport pandas as pd\n\nroot=Path('datasets')\nprices=pd.read_parquet(root/'market_daily_ohlcv_2010_2026'/'prices.parquet')\ntext_ds=Path('datasets/twitter_financial_news_sentiment/hf_dataset/train/data-00000-of-00001.arrow')\n\n# Load sample json to infer labels\nsample=json.loads((root/'twitter_financial_news_sentiment'/'samples'/'samples.json').read_text())\nlabels=sorted({x['label'] for x in sample})\n\nstats={\n  'market_daily_ohlcv_2010_2026':{\n    'rows':int(len(prices)),\n    'tickers':sorted(prices['ticker'].unique().tolist()),\n    'date_min':str(pd.to_datetime(prices['Date']).min().date()),\n    'date_max':str(pd.to_datetime(prices['Date']).max().date()),\n    'columns':prices.columns.tolist(),\n    'file':'datasets/market_daily_ohlcv_2010_2026/prices.parquet'\n  },\n  'twitter_financial_news_sentiment':{\n    'rows_train':9543,\n    'rows_validation':2388,\n    'columns':['text','label'],\n    'example_labels':labels,\n    'location':'datasets/twitter_financial_news_sentiment/hf_dataset'\n  }\n}\n(root/'dataset_stats.json').write_text(json.dumps(stats,indent=2))\nprint(json.dumps(stats,indent=2))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_50","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom pathlib import Path\nimport json\nimport pandas as pd\n\nroot=Path('datasets')\nprices=pd.read_parquet(root/'market_daily_ohlcv_2010_2026'/'prices.parquet')\ntext_ds=Path('datasets/twitter_financial_news_sentiment/hf_dataset/train/data-00000-of-00001.arrow')\n\n# Load sample json to infer labels\nsample=json.loads((root/'twitter_financial_news_sentiment'/'samples'/'samples.json').read_text())\nlabels=sorted({x['label'] for x in sample})\n\nstats={\n  'market_daily_ohlcv_2010_2026':{\n    'rows':int(len(prices)),\n    'tickers':sorted(prices['ticker'].unique().tolist()),\n    'date_min':str(pd.to_datetime(prices['Date']).min().date()),\n    'date_max':str(pd.to_datetime(prices['Date']).max().date()),\n    'columns':prices.columns.tolist(),\n    'file':'datasets/market_daily_ohlcv_2010_2026/prices.parquet'\n  },\n  'twitter_financial_news_sentiment':{\n    'rows_train':9543,\n    'rows_validation':2388,\n    'columns':['text','label'],\n    'example_labels':labels,\n    'location':'datasets/twitter_financial_news_sentiment/hf_dataset'\n  }\n}\n(root/'dataset_stats.json').write_text(json.dumps(stats,indent=2))\nprint(json.dumps(stats,indent=2))\nPY\"","aggregated_output":"{\n  \"market_daily_ohlcv_2010_2026\": {\n    \"rows\": 60660,\n    \"tickers\": [\n      \"AAPL\",\n      \"CAT\",\n      \"DIS\",\n      \"GS\",\n      \"IBM\",\n      \"JNJ\",\n      \"JPM\",\n      \"KO\",\n      \"MCD\",\n      \"MSFT\",\n      \"NKE\",\n      \"NVDA\",\n      \"SPY\",\n      \"V\",\n      \"WMT\"\n    ],\n    \"date_min\": \"2010-01-04\",\n    \"date_max\": \"2026-01-30\",\n    \"columns\": [\n      \"Date\",\n      \"Open\",\n      \"High\",\n      \"Low\",\n      \"Close\",\n      \"Adj Close\",\n      \"Volume\",\n      \"ticker\"\n    ],\n    \"file\": \"datasets/market_daily_ohlcv_2010_2026/prices.parquet\"\n  },\n  \"twitter_financial_news_sentiment\": {\n    \"rows_train\": 9543,\n    \"rows_validation\": 2388,\n    \"columns\": [\n      \"text\",\n      \"label\"\n    ],\n    \"example_labels\": [\n      0\n    ],\n    \"location\": \"datasets/twitter_financial_news_sentiment/hf_dataset\"\n  }\n}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_51","type":"reasoning","text":"**Assessing label quality and source**"}}
{"type":"item.completed","item":{"id":"item_52","type":"agent_message","text":"I’ve downloaded and validated two datasets locally. Next I’ll clone relevant code repositories (FinGPT/FinRL/TradingAgents), then generate `papers/README.md`, `datasets/README.md`, `code/README.md`, `literature_review.md`, `resources.md`, and the completion marker."}}
{"type":"item.started","item":{"id":"item_53","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_from_disk\nimport json\n\nds=load_from_disk('datasets/twitter_financial_news_sentiment/hf_dataset')\nlabels=sorted(set(ds['train']['label']))\nprint('unique labels',labels)\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_53","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nfrom datasets import load_from_disk\nimport json\n\nds=load_from_disk('datasets/twitter_financial_news_sentiment/hf_dataset')\nlabels=sorted(set(ds['train']['label']))\nprint('unique labels',labels)\nPY\"","aggregated_output":"unique labels [0, 1, 2]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_54","type":"reasoning","text":"**Updating dataset stats labels**"}}
{"type":"item.started","item":{"id":"item_55","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nimport json\np='datasets/dataset_stats.json'\nd=json.load(open(p))\nd['twitter_financial_news_sentiment']['labels']=[0,1,2]\njson.dump(d,open(p,'w'),indent=2)\nprint('updated',p)\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_55","type":"command_execution","command":"/bin/bash -lc \"pwd && source .venv/bin/activate && python - << 'PY'\nimport json\np='datasets/dataset_stats.json'\nd=json.load(open(p))\nd['twitter_financial_news_sentiment']['labels']=[0,1,2]\njson.dump(d,open(p,'w'),indent=2)\nprint('updated',p)\nPY\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\nupdated datasets/dataset_stats.json\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_56","type":"reasoning","text":"**Cloning candidate repositories**"}}
{"type":"item.started","item":{"id":"item_57","type":"command_execution","command":"/bin/bash -lc 'pwd && git clone https://github.com/TauricResearch/TradingAgents.git code/tradingagents && pwd && git clone https://github.com/AI4Finance-Foundation/FinGPT.git code/fingpt && pwd && git clone https://github.com/AI4Finance-Foundation/FinRL.git code/finrl && pwd && ls -la code'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_57","type":"command_execution","command":"/bin/bash -lc 'pwd && git clone https://github.com/TauricResearch/TradingAgents.git code/tradingagents && pwd && git clone https://github.com/AI4Finance-Foundation/FinGPT.git code/fingpt && pwd && git clone https://github.com/AI4Finance-Foundation/FinRL.git code/finrl && pwd && ls -la code'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\nCloning into 'code/fingpt'...\n/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\nCloning into 'code/finrl'...\n/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 19\ndrwxrwx---  5 haokunliu haokunliu  5 Feb 22 01:40 .\ndrwxrwx--- 15 haokunliu haokunliu 19 Feb 22 01:38 ..\ndrwxrwx---  7 haokunliu haokunliu 20 Feb 22 01:40 fingpt\ndrwxrwx--- 11 haokunliu haokunliu 21 Feb 22 01:40 finrl\ndrwxrwx---  6 haokunliu haokunliu 15 Feb 22 01:40 tradingagents\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_58","type":"reasoning","text":"**Scanning key files**"}}
{"type":"item.started","item":{"id":"item_59","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,220p' code/tradingagents/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_60","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,220p' code/fingpt/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_61","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,220p' code/finrl/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_59","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,220p' code/tradingagents/README.md\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n<p align=\"center\">\n  <img src=\"assets/TauricResearch.png\" style=\"width: 60%; height: auto;\">\n</p>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://arxiv.org/abs/2412.20138\" target=\"_blank\"><img alt=\"arXiv\" src=\"https://img.shields.io/badge/arXiv-2412.20138-B31B1B?logo=arxiv\"/></a>\n  <a href=\"https://discord.com/invite/hk9PGKShPK\" target=\"_blank\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-TradingResearch-7289da?logo=discord&logoColor=white&color=7289da\"/></a>\n  <a href=\"./assets/wechat.png\" target=\"_blank\"><img alt=\"WeChat\" src=\"https://img.shields.io/badge/WeChat-TauricResearch-brightgreen?logo=wechat&logoColor=white\"/></a>\n  <a href=\"https://x.com/TauricResearch\" target=\"_blank\"><img alt=\"X Follow\" src=\"https://img.shields.io/badge/X-TauricResearch-white?logo=x&logoColor=white\"/></a>\n  <br>\n  <a href=\"https://github.com/TauricResearch/\" target=\"_blank\"><img alt=\"Community\" src=\"https://img.shields.io/badge/Join_GitHub_Community-TauricResearch-14C290?logo=discourse\"/></a>\n</div>\n\n<div align=\"center\">\n  <!-- Keep these links. Translations will automatically update with the README. -->\n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=de\">Deutsch</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=es\">Español</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=fr\">français</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ja\">日本語</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ko\">한국어</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=pt\">Português</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ru\">Русский</a> | \n  <a href=\"https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=zh\">中文</a>\n</div>\n\n---\n\n# TradingAgents: Multi-Agents LLM Financial Trading Framework\n\n## News\n- [2026-02] **TradingAgents v0.2.0** released with multi-provider LLM support (GPT-5.x, Gemini 3.x, Claude 4.x, Grok 4.x) and improved system architecture.\n- [2026-01] **Trading-R1** [Technical Report](https://arxiv.org/abs/2509.11420) released, with [Terminal](https://github.com/TauricResearch/Trading-R1) expected to land soon.\n\n<div align=\"center\">\n<a href=\"https://www.star-history.com/#TauricResearch/TradingAgents&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&type=Date\" />\n   <img alt=\"TradingAgents Star History\" src=\"https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&type=Date\" style=\"width: 80%; height: auto;\" />\n </picture>\n</a>\n</div>\n\n> 🎉 **TradingAgents** officially released! We have received numerous inquiries about the work, and we would like to express our thanks for the enthusiasm in our community.\n>\n> So we decided to fully open-source the framework. Looking forward to building impactful projects with you!\n\n<div align=\"center\">\n\n🚀 [TradingAgents](#tradingagents-framework) | ⚡ [Installation & CLI](#installation-and-cli) | 🎬 [Demo](https://www.youtube.com/watch?v=90gr5lwjIho) | 📦 [Package Usage](#tradingagents-package) | 🤝 [Contributing](#contributing) | 📄 [Citation](#citation)\n\n</div>\n\n## TradingAgents Framework\n\nTradingAgents is a multi-agent trading framework that mirrors the dynamics of real-world trading firms. By deploying specialized LLM-powered agents: from fundamental analysts, sentiment experts, and technical analysts, to trader, risk management team, the platform collaboratively evaluates market conditions and informs trading decisions. Moreover, these agents engage in dynamic discussions to pinpoint the optimal strategy.\n\n<p align=\"center\">\n  <img src=\"assets/schema.png\" style=\"width: 100%; height: auto;\">\n</p>\n\n> TradingAgents framework is designed for research purposes. Trading performance may vary based on many factors, including the chosen backbone language models, model temperature, trading periods, the quality of data, and other non-deterministic factors. [It is not intended as financial, investment, or trading advice.](https://tauric.ai/disclaimer/)\n\nOur framework decomposes complex trading tasks into specialized roles. This ensures the system achieves a robust, scalable approach to market analysis and decision-making.\n\n### Analyst Team\n- Fundamentals Analyst: Evaluates company financials and performance metrics, identifying intrinsic values and potential red flags.\n- Sentiment Analyst: Analyzes social media and public sentiment using sentiment scoring algorithms to gauge short-term market mood.\n- News Analyst: Monitors global news and macroeconomic indicators, interpreting the impact of events on market conditions.\n- Technical Analyst: Utilizes technical indicators (like MACD and RSI) to detect trading patterns and forecast price movements.\n\n<p align=\"center\">\n  <img src=\"assets/analyst.png\" width=\"100%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\n### Researcher Team\n- Comprises both bullish and bearish researchers who critically assess the insights provided by the Analyst Team. Through structured debates, they balance potential gains against inherent risks.\n\n<p align=\"center\">\n  <img src=\"assets/researcher.png\" width=\"70%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\n### Trader Agent\n- Composes reports from the analysts and researchers to make informed trading decisions. It determines the timing and magnitude of trades based on comprehensive market insights.\n\n<p align=\"center\">\n  <img src=\"assets/trader.png\" width=\"70%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\n### Risk Management and Portfolio Manager\n- Continuously evaluates portfolio risk by assessing market volatility, liquidity, and other risk factors. The risk management team evaluates and adjusts trading strategies, providing assessment reports to the Portfolio Manager for final decision.\n- The Portfolio Manager approves/rejects the transaction proposal. If approved, the order will be sent to the simulated exchange and executed.\n\n<p align=\"center\">\n  <img src=\"assets/risk.png\" width=\"70%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\n## Installation and CLI\n\n### Installation\n\nClone TradingAgents:\n```bash\ngit clone https://github.com/TauricResearch/TradingAgents.git\ncd TradingAgents\n```\n\nCreate a virtual environment in any of your favorite environment managers:\n```bash\nconda create -n tradingagents python=3.13\nconda activate tradingagents\n```\n\nInstall dependencies:\n```bash\npip install -r requirements.txt\n```\n\n### Required APIs\n\nTradingAgents supports multiple LLM providers. Set the API key for your chosen provider:\n\n```bash\nexport OPENAI_API_KEY=[REDACTED]          # OpenAI (GPT)\nexport GOOGLE_API_KEY=[REDACTED]          # Google (Gemini)\nexport ANTHROPIC_API_KEY=[REDACTED]       # Anthropic (Claude)\nexport XAI_API_KEY=...             # xAI (Grok)\nexport OPENROUTER_API_KEY=...      # OpenRouter\nexport ALPHA_VANTAGE_API_KEY=...   # Alpha Vantage\n```\n\nFor local models, configure Ollama with `llm_provider: \"ollama\"` in your config.\n\nAlternatively, copy `.env.example` to `.env` and fill in your keys:\n```bash\ncp .env.example .env\n```\n\n### CLI Usage\n\nYou can also try out the CLI directly by running:\n```bash\npython -m cli.main\n```\nYou will see a screen where you can select your desired tickers, date, LLMs, research depth, etc.\n\n<p align=\"center\">\n  <img src=\"assets/cli/cli_init.png\" width=\"100%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\nAn interface will appear showing results as they load, letting you track the agent's progress as it runs.\n\n<p align=\"center\">\n  <img src=\"assets/cli/cli_news.png\" width=\"100%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\n<p align=\"center\">\n  <img src=\"assets/cli/cli_transaction.png\" width=\"100%\" style=\"display: inline-block; margin: 0 2%;\">\n</p>\n\n## TradingAgents Package\n\n### Implementation Details\n\nWe built TradingAgents with LangGraph to ensure flexibility and modularity. The framework supports multiple LLM providers: OpenAI, Google, Anthropic, xAI, OpenRouter, and Ollama.\n\n### Python Usage\n\nTo use TradingAgents inside your code, you can import the `tradingagents` module and initialize a `TradingAgentsGraph()` object. The `.propagate()` function will return a decision. You can run `main.py`, here's also a quick example:\n\n```python\nfrom tradingagents.graph.trading_graph import TradingAgentsGraph\nfrom tradingagents.default_config import DEFAULT_CONFIG\n\nta = TradingAgentsGraph(debug=True, config=DEFAULT_CONFIG.copy())\n\n# forward propagate\n_, decision = ta.propagate(\"NVDA\", \"2026-01-15\")\nprint(decision)\n```\n\nYou can also adjust the default configuration to set your own choice of LLMs, debate rounds, etc.\n\n```python\nfrom tradingagents.graph.trading_graph import TradingAgentsGraph\nfrom tradingagents.default_config import DEFAULT_CONFIG\n\nconfig = DEFAULT_CONFIG.copy()\nconfig[\"llm_provider\"] = \"openai\"        # openai, google, anthropic, xai, openrouter, ollama\nconfig[\"deep_think_llm\"] = \"gpt-5.2\"     # Model for complex reasoning\nconfig[\"quick_think_llm\"] = \"gpt-5-mini\" # Model for quick tasks\nconfig[\"max_debate_rounds\"] = 2\n\nta = TradingAgentsGraph(debug=True, config=config)\n_, decision = ta.propagate(\"NVDA\", \"2026-01-15\")\nprint(decision)\n```\n\nSee `tradingagents/default_config.py` for all configuration options.\n\n## Contributing\n\nWe welcome contributions from the community! Whether it's fixing a bug, improving documentation, or suggesting a new feature, your input helps make this project better. If you are interested in this line of research, please consider joining our open-source financial AI research community [Tauric Research](https://tauric.ai/).\n\n## Citation\n\nPlease reference our work if you find *TradingAgents* provides you with some help :)\n\n```\n@misc{xiao2025tradingagentsmultiagentsllmfinancial,\n      title={TradingAgents: Multi-Agents LLM Financial Trading Framework}, \n      author={Yijia Xiao and Edward Sun and Di Luo and Wei Wang},\n      year={2025},\n      eprint={2412.20138},\n      archivePrefix={arXiv},\n      primaryClass={q-fin.TR},\n      url={https://arxiv.org/abs/2412.20138}, \n}\n```\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_60","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,220p' code/fingpt/README.md\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n<div align=\"center\">\n<img align=\"center\" width=\"30%\" alt=\"image\" src=\"https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139\">\n</div>\n\n# FinGPT: Open-Source Financial Large Language Models\n[![Downloads](https://static.pepy.tech/badge/fingpt)](https://pepy.tech/project/fingpt)\n[![Downloads](https://static.pepy.tech/badge/fingpt/week)](https://pepy.tech/project/fingpt)\n[![Join Discord](https://img.shields.io/badge/Discord-Join-blue)](https://discord.gg/trsr8SXpW5)\n[![Python 3.8](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n[![PyPI](https://img.shields.io/pypi/v/fingpt.svg)](https://pypi.org/project/fingpt/)\n![License](https://img.shields.io/github/license/AI4Finance-Foundation/fingpt.svg?color=brightgreen)\n![](https://img.shields.io/github/issues-raw/AI4Finance-Foundation/fingpt?label=Issues)\n![](https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/fingpt?label=Closed+Issues)\n![](https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/fingpt?label=Open+PRs)\n![](https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/fingpt?label=Closed+PRs)\n\n<div align=\"center\">\n<img align=\"center\" src=figs/logo_transparent_background.png width=\"40%\"/>\n</div>\n\nLet us not expect Wall Street to open-source LLMs or open APIs, due to FinTech institutes' internal regulations and policies.\n\n[Blueprint of FinGPT](https://ai4finance.org/research/fingpt-open-source-finllm.html)\n\n<https://huggingface.co/FinGPT>\n\n![Visitors](https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)\n[![](https://dcbadge.limes.pink/api/server/trsr8SXpW5?cb=1)](https://discord.gg/trsr8SXpW5)\n## Project Contributors\n\nFinGPT is an open-source financial large language model project developed and maintained by the AI4Finance Foundation.\n\nKey contributors include:\n\n- [**Hongyang (Bruce) Yang**](https://www.linkedin.com/in/brucehy/) – research and development on financial large language models and related applications\n- [other contributors…]\n\n## What's New:\n - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!  🔥[Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on Huggingface🤗!\n - [Paper Acceptance] Oct, 2023: [\"FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets\"](https://arxiv.org/abs/2310.04793) is accepted🎉 by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023 \n - [Paper Acceptance] Oct, 2023: [\"FinGPT: Democratizing Internet-scale Data for Financial Large Language Models\"](https://arxiv.org/abs/2307.10485) is accepted🎉 by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023\n - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) 🔥 produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)\n - [Paper Acceptance] Sep, 2023: [\"Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models\"](https://arxiv.org/abs/2310.04027) is accepted🎉 by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)\n - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) 🔥\n - [Paper Acceptance] Jul, 2023: [\"Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models\"](https://arxiv.org/abs/2306.12659) is accepted🎉 by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023\n - [Paper Acceptance] Jul, 2023: [\"FinGPT: Open-Source Financial Large Language Models\"](https://arxiv.org/abs/2306.06031) is accepted🎉 by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023\n - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)\n\n## Why FinGPT?\n\n1). Finance is highly dynamic. [BloombergGPT](https://arxiv.org/abs/2303.17564) trained an LLM using a mixture of finance data and general-purpose data, which took about 53 days, at a cost of around **$3M**). It is costly to retrain an LLM model like BloombergGPT every month or every week, thus lightweight adaptation is highly favorable. FinGPT can be fine-tuned swiftly to incorporate new data (the cost falls significantly, less than **$300 per fine-tuning**).\n\n2). Democratizing Internet-scale financial data is critical, say allowing timely updates of the model (monthly or weekly updates) using an automatic data curation pipeline.  BloombergGPT has privileged data access and APIs, while FinGPT presents a more accessible alternative. It prioritizes lightweight adaptation, leveraging the best available open-source LLMs.\n\n3). The key technology is \"RLHF (Reinforcement learning from human feedback)\", which is missing in BloombergGPT. RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the \"secret\" ingredient of ChatGPT and GPT4.\n\n\n### Milestone of AI Robo-Advisor: FinGPT-Forecaster\n\nTry the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)\n\nThe dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405\n\n![demo_interface](fingpt/FinGPT_Forecaster/figs/interface.png)\n\nEnter the following inputs:\n\n1) ticker symbol (e.g. AAPL, MSFT, NVDA)\n2) the day from which you want the prediction to happen (yyyy-mm-dd)\n3) the number of past weeks where market news are retrieved\n4) whether to add the latest basic financials as additional information\n\nClick Submit！ And you'll be responded with a well-rounded analysis of the company and a prediction for next week's stock price movement!\n\nFor detailed and more customized implementation, please refer to [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)\n\n\n## FinGPT Demos: \n\n### Current State-of-the-arts for Financial Sentiment Analysis\n\n* [FinGPT V3 (Updated on 10/12/2023)](./fingpt)\n  \n  * What's new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**\n  \n  * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost.\n  \n  * FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model.\n  \n  * Benchmark Results:\n  \n  * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |\n    | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |\n    | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 × RTX 3090    | 17.25 hours |     $17.25     |\n    | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 × A100      |  5.5 hours  |    $ 22.55     |\n    | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 × A100      |  5.5 hours  |    $ 22.55     |\n    | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 × RTX 3090    | 6.47 hours  |     $ 6.47     |\n    | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 × RTX 3090    | 4.15 hours  |     $ 4.15     |\n    | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |\n    | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |\n    | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 × NVIDIA K80 GPU |      -      |       -        |\n    | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 × A100     |   21 days   | $ 4.23 million |\n    | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 × A100     |   53 days   | $ 2.67 million |\n  \n    **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs. Note that BloombergGPT also used p4d.24xlarge As of July 11, 2023, the hourly rate for this instance stands at $32.773. Consequently, the estimated cost per GPU hour comes to $32.77 divided by 8, resulting in approximately **$4.10**. With this value as the reference unit price (1 GPU hour). **BloombergGPT estimated cost= 512 x 53 x 24 = 651,264 GPU hours x $4.10 = $2,670,182.40**. For **RTX 3090**, we assume its cost per hour is approximately **$1.0**, which is actually much higher than available GPUs from platforms like vast.ai.\n  \n  * Reproduce the results by running [benchmarks](./fingpt/FinGPT_Sentiment_Analysis_v3/benchmark/benchmarks.ipynb), and the detailed tutorial is on the way.\n  * Finetune your own FinGPT v3 model with the LoRA method on only an RTX 3090 with this [notebook](./fingpt/FinGPT_Sentiment_Analysis_v3/training_8bit/train_Llama2_13B.ipynb) in 8bit or this [notebook](./fingpt/FinGPT_Sentiment_Analysis_v3/training_int4/train.ipynb) in int4 (QLoRA)\n  \n* [FinGPT V1](./fingpt)\n  + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**\n \n## Instruction Tuning Datasets and Models\nThe datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>\n\n[Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)\n  \n  | Datasets | Train Rows |  Test Rows |Description  |\n  | --------- | ----------------- | ------------ | --------------------- |\n  | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |\n  | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |\n  | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|\n  | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|\n  | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|\n  | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|\n\n  Multi-task financial LLMs Models:\n```python\n  demo_tasks = [\n      'Financial Sentiment Analysis',\n      'Financial Relation Extraction',\n      'Financial Headline Classification',\n      'Financial Named Entity Recognition',]\n  demo_inputs = [\n      \"Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano\",\n      \"Apple Inc. Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.\",\n      'gold trades in red in early trade; eyes near-term range at rs 28,300-28,600',\n      'This LOAN AND SECURITY AGREEMENT dated January 27 , 1999 , between SILICON VALLEY BANK (\" Bank \"), a California - chartered bank with its principal place of business at 3003 Tasman Drive , Santa Clara , California 95054 with a loan production office located at 40 William St ., Ste .',]\n  demo_instructions = [\n      'What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.',\n      'Given phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \"relation1: word1, word2; relation2: word3, word4\". Options: product/material produced, manufacturer, distributed by, industry, position held, original broadcaster, owned by, founded by, distribution format, headquarters location, stock exchange, currency, parent organization, chief executive officer, director/manager, owner of, operator, member of, employer, chairperson, platform, subsidiary, legal form, publisher, developer, brand, business division, location of formation, creator.',\n      'Does the news headline talk about price going up? Please choose an answer from {Yes/No}.',\n      'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',]\n```\n\n  | Models | Description  | Function |\n  | --------- | --------------------- |---------------- |\n  | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |\n  | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |\n  | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |\n  | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |\n  | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |\n  | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |\n  | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |\n  | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |\n\n  \n## Tutorials\n[[Training] Beginner’s Guide to FinGPT: Training with LoRA and ChatGLM2–6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)\n\n## Understanding FinGPT: An Educational Blog Series\n+ [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications\n](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)\n+ [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance\n](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca)\n+ [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models\n](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)\n\n\n## FinGPT Ecosystem\n### FinGPT embraces a full-stack framework for FinLLMs with five layers:\n1. **Data source layer**: This layer assures comprehensive market coverage, addressing the temporal sensitivity of financial data through real-time information capture.\n2. **Data engineering layer**: Primed for real-time NLP data processing, this layer tackles the inherent challenges of high temporal sensitivity and low signal-to-noise ratio in financial data.\n3. **LLMs layer**: Focusing on a range of fine-tuning methodologies such as LoRA, this layer mitigates the highly dynamic nature of financial data, ensuring the model’s relevance and accuracy.\n4. **Task layer**: This layer is responsible for executing fundamental tasks. These tasks serve as the benchmarks for performance evaluations and cross-comparisons in the realm of FinLLMs\n5. **Application layer**: Showcasing practical applications and demos, this layer highlights the potential capability of FinGPT in the financial sector.\n\n* FinGPT Framework: Open-Source Financial Large Language Models\n\n<div align=\"center\">\n<img align=\"center\" src=figs/FinGPT_framework_20240301.png>\n</div>\n\n* [FinGPT-RAG](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_RAG): We present a retrieval-augmented large language model framework specifically designed for financial sentiment analysis, optimizing information depth and context through external knowledge retrieval, thereby ensuring nuanced predictions.\n\n<div align=\"center\">\n<img align=\"center\" src=figs/FinGPT_RAG_framework.png>\n</div>\n\n* [FinGPT-FinNLP](https://github.com/AI4Finance-Foundation/FinNLP): FinNLP provides a playground for all people interested in LLMs and NLP in Finance. Here we provide full pipelines for LLM training and finetuning in the field of finance. The full architecture is shown in the following picture. Detail codes and introductions can be found [here](https://github.com/AI4Finance-Foundation/FinNLP). Or you may refer to the [wiki](https://ai4finance-foundation.github.io/FinNLP/)\n\n<div align=\"center\">\n<img align=\"center\" src=figs/FinGPT_FinNLP_data_source.png>\n</div>\n\n* [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark): We introduce a novel Instruction Tuning paradigm optimized for open-source Large Language Models (LLMs) in finance, enhancing their adaptability to diverse financial datasets while also facilitating cost-effective, systematic benchmarking from task-specific, multi-task, and zero-shot instruction tuning tasks. \n\n\n<div align=\"center\">\n<img align=\"center\" src=figs/FinGPT_Benchmark_20231110.png>\n</div>\n\n\n\n## Open-Source Base Model used in the LLMs layer of FinGPT\n* Feel free to contribute more open-source base models tailored for various language-specific financial markets.\n\n| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications |\n|  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  |\n| [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor |\n| [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis |\n| [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis |\n| [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary |\n| [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis|\n| [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |\n\n* Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):\n  | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |\n  | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_61","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,220p' code/finrl/README.md\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n<div align=\"center\">\n<img align=\"center\" width=\"30%\" alt=\"image\" src=\"https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139\">\n</div>\n\n# FinRL®: Financial Reinforcement Learning [![twitter][1.1]][1] [![facebook][1.2]][2] [![google+][1.3]][3] [![linkedin][1.4]][4]\n\n[1.1]: http://www.tensorlet.org/wp-content/uploads/2021/01/button_twitter_22x22.png\n[1.2]: http://www.tensorlet.org/wp-content/uploads/2021/01/facebook-button_22x22.png\n[1.3]: http://www.tensorlet.org/wp-content/uploads/2021/01/button_google_22.xx_.png\n[1.4]: http://www.tensorlet.org/wp-content/uploads/2021/01/button_linkedin_22x22.png\n\n[1]: https://twitter.com/intent/tweet?text=FinRL-Financial-Deep-Reinforcement-Learning%20&url=https://github.com/AI4Finance-Foundation/FinRL&hashtags=DRL&hashtags=AI\n[2]: https://www.facebook.com/sharer.php?u=http%3A%2F%2Fgithub.com%2FAI4Finance-Foundation%2FFinRL\n[3]: https://plus.google.com/share?url=https://github.com/AI4Finance-Foundation/FinRL\n[4]: https://www.linkedin.com/sharing/share-offsite/?url=http%3A%2F%2Fgithub.com%2FAI4Finance-Foundation%2FFinRL\n\n<div align=\"center\">\n<img align=\"center\" src=figs/logo_transparent_background.png width=\"55%\"/>\n</div>\n\n[![Downloads](https://static.pepy.tech/badge/finrl)](https://pepy.tech/project/finrl)\n[![Downloads](https://static.pepy.tech/badge/finrl/week)](https://pepy.tech/project/finrl)\n[![Join Discord](https://img.shields.io/badge/Discord-Join-blue)](https://discord.gg/trsr8SXpW5)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n[![PyPI](https://img.shields.io/pypi/v/finrl.svg)](https://pypi.org/project/finrl/)\n[![Documentation Status](https://readthedocs.org/projects/finrl/badge/?version=latest)](https://finrl.readthedocs.io/en/latest/?badge=latest)\n![License](https://img.shields.io/github/license/AI4Finance-Foundation/finrl.svg?color=brightgreen)\n![](https://img.shields.io/github/issues-raw/AI4Finance-Foundation/finrl?label=Issues)\n![](https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/finrl?label=Closed+Issues)\n![](https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/finrl?label=Open+PRs)\n![](https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/finrl?label=Closed+PRs)\n\n[FinGPT](https://github.com/AI4Finance-Foundation/ChatGPT-for-FinTech): Open-source for open-finance! Revolutionize FinTech.\n\n\n[![](https://dcbadge.vercel.app/api/server/trsr8SXpW5)](https://discord.gg/trsr8SXpW5)\n\n![Visitors](https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&repo=FinRL&countColor=%23B17A)\n[![](https://dcbadge.limes.pink/api/server/trsr8SXpW5?cb=1)](https://discord.gg/trsr8SXpW5)\n\n\n\n**Financial reinforcement learning (FinRL®)** ([Document website](https://finrl.readthedocs.io/en/latest/index.html)) is **the first open-source framework** for financial reinforcement learning. FinRL has evolved into an **ecosystem**\n* [FinRL-DeepSeek](https://github.com/AI4Finance-Foundation/FinRL_DeepSeek): LLM-Infused Risk-Sensitive Reinforcement Learning for Trading Agents\n\n| Dev Roadmap  | Stage | Users | Project | Description |\n|----|----|----|----|----|\n| 0.0 (Preparation) | entrance | practitioners | [FinRL-Meta](https://github.com/AI4Finance-Foundation/FinRL-Meta)| gym-style market environments |\n| 1.0 (Proof-of-Concept)| full-stack | developers | [this repo](https://github.com/AI4Finance-Foundation/FinRL) | automatic pipeline |\n| 2.0 (Professional) | profession | experts | [ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL) | algorithms |\n| 3.0 (Production) | service | hedge funds | [Podracer](https://github.com/AI4Finance-Foundation/FinRL_Podracer) | cloud-native deployment |\n\n\n## Outline\n\n  - [Overview](#overview)\n  - [File Structure](#file-structure)\n  - [Supported Data Sources](#supported-data-sources)\n  - [Installation](#installation)\n  - [Status Update](#status-update)\n  - [Tutorials](#tutorials)\n  - [Publications](#publications)\n  - [News](#news)\n  - [Citing FinRL](#citing-finrl)\n  - [Join and Contribute](#join-and-contribute)\n    - [Contributors](#contributors)\n    - [Sponsorship](#sponsorship)\n  - [LICENSE](#license)\n\n## Project Contributors\n\nFinRL® is an open-source financial reinforcement learning framework developed and maintained by the AI4Finance Foundation.\n\nKey contributors include:\n\n- [**Hongyang (Bruce) Yang**](https://www.linkedin.com/in/brucehy/) – research and development on financial reinforcement learning frameworks, market environments, and quantitative trading applications\n- [other contributors…]\n  \n## Overview\n\nFinRL has three layers: market environments, agents, and applications.  For a trading task (on the top), an agent (in the middle) interacts with a market environment (at the bottom), making sequential decisions.\n\n<div align=\"center\">\n<img align=\"center\" src=figs/finrl_framework.png>\n</div>\n\nA quick start: Stock_NeurIPS2018.ipynb. Videos [FinRL](http://www.youtube.com/watch?v=ZSGJjtM-5jA) at [AI4Finance Youtube Channel](https://www.youtube.com/channel/UCrVri6k3KPBa3NhapVV4K5g).\n\n\n## File Structure\n\nThe main folder **finrl** has three subfolders **applications, agents, meta**. We employ a **train-test-trade** pipeline with three files: train.py, test.py, and trade.py.\n\n```\nFinRL\n├── finrl (main folder)\n│   ├── applications\n│   \t├── Stock_NeurIPS2018\n│   \t├── imitation_learning\n│   \t├── cryptocurrency_trading\n│   \t├── high_frequency_trading\n│   \t├── portfolio_allocation\n│   \t└── stock_trading\n│   ├── agents\n│   \t├── elegantrl\n│   \t├── rllib\n│   \t└── stablebaseline3\n│   ├── meta\n│   \t├── data_processors\n│   \t├── env_cryptocurrency_trading\n│   \t├── env_portfolio_allocation\n│   \t├── env_stock_trading\n│   \t├── preprocessor\n│   \t├── data_processor.py\n│       ├── meta_config_tickers.py\n│   \t└── meta_config.py\n│   ├── config.py\n│   ├── config_tickers.py\n│   ├── main.py\n│   ├── plot.py\n│   ├── train.py\n│   ├── test.py\n│   └── trade.py\n│\n├── examples\n├── unit_tests (unit tests to verify codes on env & data)\n│   ├── environments\n│   \t└── test_env_cashpenalty.py\n│   └── downloaders\n│   \t├── test_yahoodownload.py\n│   \t└── test_alpaca_downloader.py\n├── setup.py\n├── requirements.txt\n└── README.md\n```\n\n## Supported Data Sources\n\n|Data Source |Type |Range and Frequency |Request Limits|Raw Data|Preprocessed Data|\n|  ----  |  ----  |  ----  |  ----  |  ----  |  ----  |\n|[Akshare](https://alpaca.markets/docs/introduction/)| CN Securities| 2015-now, 1day| Account-specific| OHLCV| Prices&Indicators|\n|[Alpaca](https://docs.alpaca.markets/docs/getting-started)| US Stocks, ETFs| 2015-now, 1min| Account-specific| OHLCV| Prices&Indicators|\n|[Baostock](http://baostock.com/baostock/index.php/Python_API%E6%96%87%E6%A1%A3)| CN Securities| 1990-12-19-now, 5min| Account-specific| OHLCV| Prices&Indicators|\n|[Binance](https://binance-docs.github.io/apidocs/spot/en/#public-api-definitions)| Cryptocurrency| API-specific, 1s, 1min| API-specific| Tick-level daily aggegrated trades, OHLCV| Prices&Indicators|\n|[CCXT](https://docs.ccxt.com/en/latest/manual.html)| Cryptocurrency| API-specific, 1min| API-specific| OHLCV| Prices&Indicators|\n|[EODhistoricaldata](https://eodhistoricaldata.com/financial-apis/)| US Securities| Frequency-specific, 1min| API-specific | OHLCV | Prices&Indicators|\n|[IEXCloud](https://iexcloud.io/docs/api/)| NMS US securities|1970-now, 1 day|100 per second per IP|OHLCV| Prices&Indicators|\n|[JoinQuant](https://www.joinquant.com/)| CN Securities| 2005-now, 1min| 3 requests each time| OHLCV| Prices&Indicators|\n|[QuantConnect](https://www.quantconnect.com/docs/v2)| US Securities| 1998-now, 1s| NA| OHLCV| Prices&Indicators|\n|[RiceQuant](https://www.ricequant.com/doc/rqdata/python/)| CN Securities| 2005-now, 1ms| Account-specific| OHLCV| Prices&Indicators|\n[Sinopac](https://sinotrade.github.io/zh_TW/tutor/prepare/terms/) | Taiwan securities | 2023-04-13~now, 1min | Account-specific | OHLCV | Prices&Indicators|\n|[Tushare](https://tushare.pro/document/1?doc_id=131)| CN Securities, A share| -now, 1 min| Account-specific| OHLCV| Prices&Indicators|\n|[WRDS](https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/nyse-trade-and-quote-taq/)| US Securities| 2003-now, 1ms| 5 requests each time| Intraday Trades|Prices&Indicators|\n|[YahooFinance](https://pypi.org/project/yfinance/)| US Securities| Frequency-specific, 1min| 2,000/hour| OHLCV | Prices&Indicators|\n\n\n<!-- |Data Source |Type |Max Frequency |Raw Data|Preprocessed Data|\n|  ----  |  ----  |  ----  |  ----  |  ----  |\n|    AkShare |  CN Securities | 1 day  |  OHLCV |  Prices, indicators |\n|    Alpaca |  US Stocks, ETFs |  1 min |  OHLCV |  Prices, indicators |\n|    Alpha Vantage | Stock, ETF, forex, crypto, technical indicators | 1 min |  OHLCV  & Prices, indicators |\n|    Baostock |  CN Securities |  5 min |  OHLCV |  Prices, indicators |\n|    Binance |  Cryptocurrency |  1 s |  OHLCV |  Prices, indicators |\n|    CCXT |  Cryptocurrency |  1 min  |  OHLCV |  Prices, indicators |\n|    currencyapi |  Exchange rate | 1 day |  Exchange rate | Exchange rate, indicators |\n|    currencylayer |  Exchange rate | 1 day  |  Exchange rate | Exchange rate, indicators |\n|    EOD Historical Data | US stocks, and ETFs |  1 day  |  OHLCV  | Prices, indicators |\n|    Exchangerates |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |\n|    findatapy |  CN Securities | 1 day  |  OHLCV |  Prices, indicators |\n|    Financial Modeling prep | US stocks, currencies, crypto |  1 min |  OHLCV  | Prices, indicators |\n|    finnhub | US Stocks, currencies, crypto |   1 day |  OHLCV  | Prices, indicators |\n|    Fixer |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |\n|    IEXCloud |  NMS US securities | 1 day  | OHLCV |  Prices, indicators |\n|    JoinQuant |  CN Securities |  1 min  |  OHLCV |  Prices, indicators |\n|    Marketstack | 50+ countries |  1 day  |  OHLCV | Prices, indicators |\n|    Open Exchange Rates |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |\n|    pandas\\_datareader |  US Securities |  1 day |  OHLCV | Prices, indicators |\n|    pandas-finance |  US Securities |  1 day  |  OHLCV  & Prices, indicators |\n|    Polygon |  US Securities |  1 day  |  OHLCV  | Prices, indicators |\n|    Quandl | 250+ sources |  1 day  |  OHLCV  | Prices, indicators |\n|    QuantConnect |  US Securities |  1 s |  OHLCV |  Prices, indicators |\n|    RiceQuant |  CN Securities |  1 ms  |  OHLCV |  Prices, indicators |\n|    Sinopac   | Taiwan securities | 1min | OHLCV |  Prices, indicators |\n|    Tiingo | Stocks, crypto |  1 day  |  OHLCV  | Prices, indicators |\n|    Tushare |  CN Securities | 1 min  |  OHLCV |  Prices, indicators |\n|    WRDS |  US Securities |  1 ms  |  Intraday Trades | Prices, indicators |\n|    XE |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |\n|    Xignite |  Exchange rate |  1 day  |  Exchange rate | Exchange rate, indicators |\n|    YahooFinance |  US Securities | 1 min  |  OHLCV  |  Prices, indicators |\n|    ystockquote |  US Securities |  1 day  |  OHLCV | Prices, indicators | -->\n\n\n\nOHLCV: open, high, low, and close prices; volume. adjusted_close: adjusted close price\n\nTechnical indicators: 'macd', 'boll_ub', 'boll_lb', 'rsi_30', 'dx_30', 'close_30_sma', 'close_60_sma'. Users also can add new features.\n\n\n## Installation\n+ [Install description for all operating systems (MAC OS, Ubuntu, Windows 10)](./docs/source/start/installation.rst)\n+ [FinRL for Quantitative Finance: Install and Setup Tutorial for Beginners](https://ai4finance.medium.com/finrl-for-quantitative-finance-install-and-setup-tutorial-for-beginners-1db80ad39159)\n\n## Status Update\n<details><summary><b>Version History</b> <i>[click to expand]</i></summary>\n<div>\n\n* 2022-06-25\n\t0.3.5: Formal release of FinRL, neo_finrl is chenged to FinRL-Meta with related files in directory: *meta*.\n* 2021-08-25\n\t0.3.1: pytorch version with a three-layer architecture, apps (financial tasks), drl_agents (drl algorithms), neo_finrl (gym env)\n* 2020-12-14\n  \tUpgraded to **Pytorch** with stable-baselines3; Remove tensorflow 1.0 at this moment, under development to support tensorflow 2.0\n* 2020-11-27\n  \t0.1: Beta version with tensorflow 1.5\n</div>\n</details>\n\n\n## Tutorials\n\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_62","type":"reasoning","text":"**Planning README generation**"}}
{"type":"item.completed","item":{"id":"item_63","type":"agent_message","text":"I’ve inspected the cloned repos and now I’m generating the documentation files: per-folder READMEs, synthesized `literature_review.md`, and `resources.md`, then I’ll run a final checklist and create `.resource_finder_complete`."}}
{"type":"item.started","item":{"id":"item_64","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code/tradingagents | sed -n '1,120p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_65","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code/fingpt/fingpt | sed -n '1,120p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_66","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code/finrl/finrl | sed -n '1,120p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_64","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code/tradingagents | sed -n '1,120p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 420\ndrwxrwx--- 6 haokunliu haokunliu     15 Feb 22 01:40 .\ndrwxrwx--- 5 haokunliu haokunliu      5 Feb 22 01:40 ..\n-rw-rw---- 1 haokunliu haokunliu    122 Feb 22 01:40 .env.example\ndrwxrwx--- 7 haokunliu haokunliu     12 Feb 22 01:40 .git\n-rw-rw---- 1 haokunliu haokunliu   4586 Feb 22 01:40 .gitignore\n-rw-rw---- 1 haokunliu haokunliu  11357 Feb 22 01:40 LICENSE\n-rw-rw---- 1 haokunliu haokunliu  10340 Feb 22 01:40 README.md\ndrwxrwx--- 3 haokunliu haokunliu     10 Feb 22 01:40 assets\ndrwxrwx--- 3 haokunliu haokunliu     10 Feb 22 01:40 cli\n-rw-rw---- 1 haokunliu haokunliu   1154 Feb 22 01:40 main.py\n-rw-rw---- 1 haokunliu haokunliu    976 Feb 22 01:40 pyproject.toml\n-rw-rw---- 1 haokunliu haokunliu    249 Feb 22 01:40 requirements.txt\n-rw-rw---- 1 haokunliu haokunliu    637 Feb 22 01:40 test.py\ndrwxrwx--- 6 haokunliu haokunliu      7 Feb 22 01:40 tradingagents\n-rw-rw---- 1 haokunliu haokunliu 666506 Feb 22 01:40 uv.lock\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_65","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code/fingpt/fingpt | sed -n '1,120p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 90\ndrwxrwx--- 10 haokunliu haokunliu   12 Feb 22 01:40 .\ndrwxrwx---  7 haokunliu haokunliu   20 Feb 22 01:40 ..\ndrwxrwx---  4 haokunliu haokunliu   13 Feb 22 01:40 FinGPT_Benchmark\ndrwxrwx---  3 haokunliu haokunliu    5 Feb 22 01:40 FinGPT_FinancialReportAnalysis\ndrwxrwx---  5 haokunliu haokunliu   19 Feb 22 01:40 FinGPT_Forecaster\ndrwxrwx---  6 haokunliu haokunliu    7 Feb 22 01:40 FinGPT_MultiAgentsRAG\ndrwxrwx---  5 haokunliu haokunliu    6 Feb 22 01:40 FinGPT_Others\ndrwxrwx---  5 haokunliu haokunliu    9 Feb 22 01:40 FinGPT_RAG\ndrwxrwx---  4 haokunliu haokunliu    5 Feb 22 01:40 FinGPT_Sentiment_Analysis_v1\ndrwxrwx---  7 haokunliu haokunliu    9 Feb 22 01:40 FinGPT_Sentiment_Analysis_v3\n-rw-rw----  1 haokunliu haokunliu    0 Feb 22 01:40 __init__.py\n-rw-rw----  1 haokunliu haokunliu 1951 Feb 22 01:40 readme.md\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_66","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code/finrl/finrl | sed -n '1,120p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 92\ndrwxrwx---  5 haokunliu haokunliu    16 Feb 22 01:40 .\ndrwxrwx--- 11 haokunliu haokunliu    21 Feb 22 01:40 ..\n-rw-rw----  1 haokunliu haokunliu  1208 Feb 22 01:40 README.md\n-rw-rw----  1 haokunliu haokunliu   124 Feb 22 01:40 __init__.py\n-rw-rw----  1 haokunliu haokunliu   121 Feb 22 01:40 __main__.py\ndrwxrwx---  6 haokunliu haokunliu     7 Feb 22 01:40 agents\ndrwxrwx---  8 haokunliu haokunliu     9 Feb 22 01:40 applications\n-rw-rw----  1 haokunliu haokunliu  2529 Feb 22 01:40 config.py\n-rw-rw----  1 haokunliu haokunliu    85 Feb 22 01:40 config_private.py\n-rw-rw----  1 haokunliu haokunliu 22571 Feb 22 01:40 config_tickers.py\n-rw-rw----  1 haokunliu haokunliu  4669 Feb 22 01:40 main.py\ndrwxrwx---  9 haokunliu haokunliu    12 Feb 22 01:40 meta\n-rw-rw----  1 haokunliu haokunliu 11103 Feb 22 01:40 plot.py\n-rw-rw----  1 haokunliu haokunliu  4246 Feb 22 01:40 test.py\n-rw-rw----  1 haokunliu haokunliu  2126 Feb 22 01:40 trade.py\n-rw-rw----  1 haokunliu haokunliu  5215 Feb 22 01:40 train.py\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_67","type":"reasoning","text":"**Planning docs generation**"}}
{"type":"item.started","item":{"id":"item_68","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,260p' papers/papers_metadata.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_68","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,260p' papers/papers_metadata.json\"","aggregated_output":"[\n  {\n    \"title\": \"StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?\",\n    \"year\": 2025,\n    \"authors\": \"Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li\",\n    \"url\": \"https://api.semanticscholar.org/CorpusId:281724665\",\n    \"relevance\": 3,\n    \"abstract\": \"Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.\",\n    \"citations\": 7,\n    \"status\": \"downloaded\",\n    \"file\": \"2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf\",\n    \"arxiv_id\": \"2510.02209v1\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2510.02209v1\",\n    \"matched_title\": \"StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?\",\n    \"published\": \"2025-10-02\"\n  },\n  {\n    \"title\": \"FinPos: A Position-Aware Trading Agent System for Real Financial Markets\",\n    \"year\": 2025,\n    \"authors\": \"Bijia Liu, Ronghao Dang\",\n    \"url\": \"https://api.semanticscholar.org/CorpusId:282719341\",\n    \"relevance\": 3,\n    \"abstract\": \"The exceptional potential of large language models (LLMs) in handling text information has garnered significant attention in the field of financial trading. However, most existing trading agents operate under intraday, independent unit-based trading tasks, where decisions are made as isolated directional actions, and thus lack awareness of continuous position management. Therefore, we propose a position-aware trading task designed to simulate a more realistic market. To address this task, we propose FinPos, a position-aware trading agent system designed to explicitly model and manage continuous positions. FinPos enhances position awareness through three key mechanisms: (1) professional-level interpretation of heterogeneous market information; (2) a dual-agent decision structure that separates directional reasoning from risk-aware position adjustment; and (3) multi-timescale reward signals, allowing the agent to internalize position awareness through experiential feedback rather than static instructions alone. Extensive experiments demonstrate that FinPos surpasses state-of-the-art trading agents in the position-aware trading task, which closely mirrors real market conditions. More importantly, our findings reveal that LLM-centered agent systems exhibit a vast, largely unexplored potential in long-term market decision-making.\",\n    \"citations\": 2,\n    \"status\": \"downloaded\",\n    \"file\": \"2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf\",\n    \"arxiv_id\": \"2510.27251v2\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2510.27251v2\",\n    \"matched_title\": \"FinPos: A Position-Aware Trading Agent System for Real Financial Markets\",\n    \"published\": \"2025-10-31\"\n  },\n  {\n    \"title\": \"Hierarchical Organization Simulacra in the Investment Sector\",\n    \"year\": 2024,\n    \"authors\": \"Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao\",\n    \"url\": \"https://api.semanticscholar.org/CorpusId:273022534\",\n    \"relevance\": 3,\n    \"abstract\": \"This paper explores designing artificial organizations with professional behavior in investments using a multi-agent simulation. The method mimics hierarchical decision-making in investment firms, using news articles to inform decisions. A large-scale study analyzing over 115,000 news articles of 300 companies across 15 years compared this approach against professional traders' decisions. Results show that hierarchical simulations align closely with professional choices, both in frequency and profitability. However, the study also reveals biases in decision-making, where changes in prompt wording and perceived agent seniority significantly influence outcomes. This highlights both the potential and limitations of large language models in replicating professional financial decision-making.\",\n    \"citations\": 1,\n    \"status\": \"downloaded\",\n    \"file\": \"2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf\",\n    \"arxiv_id\": \"2410.00354v1\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2410.00354v1\",\n    \"matched_title\": \"Hierarchical Organization Simulacra in the Investment Sector\",\n    \"published\": \"2024-10-01\"\n  },\n  {\n    \"title\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?\",\n    \"year\": 2025,\n    \"authors\": \"Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma\",\n    \"url\": \"https://api.semanticscholar.org/CorpusId:278501425\",\n    \"relevance\": 3,\n    \"abstract\": \"Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.\",\n    \"citations\": 8,\n    \"status\": \"downloaded\",\n    \"file\": \"2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf\",\n    \"arxiv_id\": \"2505.07078v5\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2505.07078v5\",\n    \"matched_title\": \"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?\",\n    \"published\": \"2025-05-11\"\n  },\n  {\n    \"title\": \"HedgeAgents: A Balanced-aware Multi-agent Financial Trading System\",\n    \"year\": 2025,\n    \"authors\": \"Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu\",\n    \"url\": \"https://api.semanticscholar.org/CorpusId:276449781\",\n    \"relevance\": 3,\n    \"abstract\": \"As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ''hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).\",\n    \"citations\": 23,\n    \"status\": \"downloaded\",\n    \"file\": \"2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf\",\n    \"arxiv_id\": \"2502.13165v1\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2502.13165v1\",\n    \"matched_title\": \"HedgeAgents: A Balanced-aware Multi-agent Financial Trading System\",\n    \"published\": \"2025-02-17\"\n  },\n  {\n    \"title\": \"Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance\",\n    \"year\": 2026,\n    \"authors\": \"Mostapha Benhenda\",\n    \"url\": \"https://api.semanticscholar.org/CorpusId:284911867\",\n    \"relevance\": 3,\n    \"abstract\": \"We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench\",\n    \"citations\": 0,\n    \"status\": \"downloaded\",\n    \"file\": \"2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf\",\n    \"arxiv_id\": \"2601.13770v1\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2601.13770v1\",\n    \"matched_title\": \"Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance\",\n    \"published\": \"2026-01-20\"\n  },\n  {\n    \"title\": \"Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation\",\n    \"year\": 2026,\n    \"authors\": \"Zeping Li, Guancheng Wan, Keyang Chen, Yu Chen, Yiwen Zhao, Philip Torr, Guangnan Ye, Zhenfei Yin, Hongfeng Chai\",\n    \"url\": \"https://www.semanticscholar.org/paper/7550169d076668ede36673525ad2b96baa54fe0a\",\n    \"relevance\": 3,\n    \"abstract\": \"Recent works have increasingly applied Large Language Models (LLMs) as agents in financial stock market simulations to test if micro-level behaviors aggregate into macro-level phenomena. However, a crucial question arises: Do LLM agents'behaviors align with real market participants? This alignment is key to the validity of simulation results. To explore this, we select a financial stock market scenario to test behavioral consistency. Investors are typically classified as fundamental or technical traders, but most simulations fix strategies at initialization, failing to reflect real-world trading dynamics. In this work, we assess whether agents'strategy switching aligns with financial theory, providing a framework for this evaluation. We operationalize four behavioral-finance drivers-loss aversion, herding, wealth differentiation, and price misalignment-as personality traits set via prompting and stored long-term. In year-long simulations, agents process daily price-volume data, trade under a designated style, and reassess their strategy every 10 trading days. We introduce four alignment metrics and use Mann-Whitney U tests to compare agents'style-switching behavior with financial theory. Our results show that recent LLMs'switching behavior is only partially consistent with behavioral-finance theories, highlighting the need for further refinement in aligning agent behavior with financial theory.\",\n    \"citations\": 0,\n    \"status\": \"downloaded\",\n    \"file\": \"2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf\",\n    \"arxiv_id\": \"2602.07023v1\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2602.07023v1\",\n    \"matched_title\": \"Behavioral Consistency Validation for LLM Agents: An Analysis of Trading-Style Switching through Stock-Market Simulation\",\n    \"published\": \"2026-02-02\"\n  },\n  {\n    \"title\": \"FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design\",\n    \"year\": null,\n    \"authors\": \"\",\n    \"relevance\": 1,\n    \"abstract\": \"\",\n    \"url\": \"\",\n    \"status\": \"downloaded\",\n    \"file\": \"2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf\",\n    \"arxiv_id\": \"2311.13743v2\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2311.13743v2\",\n    \"matched_title\": \"FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design\",\n    \"published\": \"2023-11-23\"\n  },\n  {\n    \"title\": \"TradingAgents: Multi-Agents LLM Financial Trading Framework\",\n    \"year\": null,\n    \"authors\": \"\",\n    \"relevance\": 1,\n    \"abstract\": \"\",\n    \"url\": \"\",\n    \"status\": \"downloaded\",\n    \"file\": \"2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf\",\n    \"arxiv_id\": \"2412.20138v7\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2412.20138v7\",\n    \"matched_title\": \"TradingAgents: Multi-Agents LLM Financial Trading Framework\",\n    \"published\": \"2024-12-28\"\n  },\n  {\n    \"title\": \"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist\",\n    \"year\": null,\n    \"authors\": \"\",\n    \"relevance\": 1,\n    \"abstract\": \"\",\n    \"url\": \"\",\n    \"status\": \"downloaded\",\n    \"file\": \"2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf\",\n    \"arxiv_id\": \"2402.18485v3\",\n    \"pdf_url\": \"https://arxiv.org/pdf/2402.18485v3\",\n    \"matched_title\": \"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist\",\n    \"published\": \"2024-02-28\"\n  }\n]","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_69","type":"reasoning","text":"**Planning comprehensive documentation**"}}
{"type":"item.started","item":{"id":"item_70","type":"command_execution","command":"/bin/bash -lc \"pwd && cat > papers/README.md << 'EOF'\n# Downloaded Papers\n\nThis folder contains papers for the project **Refining LLM Trading**.\n\n## Priority Papers from Paper-Finder (relevance >= 2)\n\n1. [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf)\n- Authors: Yanxu Chen et al.\n- Year: 2025\n- arXiv: 2510.02209\n- Why relevant: Direct benchmark for multi-month sequential LLM trading and risk-adjusted metrics.\n\n2. [FinPos: A Position-Aware Trading Agent System for Real Financial Markets](2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf)\n- Authors: Bijia Liu, Ronghao Dang\n- Year: 2025\n- arXiv: 2510.27251\n- Why relevant: Explicitly proposes position-aware, longer-horizon decision making instead of one-step trade classification.\n\n3. [Hierarchical Organization Simulacra in the Investment Sector](2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf)\n- Authors: Chung-Chi Chen et al.\n- Year: 2024\n- arXiv: 2410.00354\n- Why relevant: Studies multi-agent investment roles over long time horizons and compares with professional decisions.\n\n4. [Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?](2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf)\n- Authors: Weixian Waylon Li et al.\n- Year: 2025\n- arXiv: 2505.07078\n- Why relevant: Core evidence paper for long-run robustness; introduces broad backtests and highlights bias in short-window gains.\n\n5. [HedgeAgents: A Balanced-aware Multi-agent Financial Trading System](2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf)\n- Authors: Xiangyu Li et al.\n- Year: 2025\n- arXiv: 2502.13165\n- Why relevant: Shows multi-agent risk-balancing and hedging approaches for volatile regimes.\n\n6. [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf)\n- Author: Mostapha Benhenda\n- Year: 2026\n- arXiv: 2601.13770\n- Why relevant: Evaluates temporal leakage and look-ahead bias that often inflate short-term paper results.\n\n7. [Behavioral Consistency Validation for LLM Agents](2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf)\n- Authors: Zeping Li et al.\n- Year: 2026\n- arXiv: 2602.07023\n- Why relevant: Tests whether agent strategy switching behavior remains realistic over longer simulations.\n\n## Foundational/Strong Baseline Papers\n\n8. [FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design](2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf)\n- Year: 2023\n- arXiv: 2311.13743\n- Why relevant: Strong baseline with layered memory and explicit agent persona design.\n\n9. [TradingAgents: Multi-Agents LLM Financial Trading Framework](2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf)\n- Year: 2024\n- arXiv: 2412.20138\n- Why relevant: Widely reused open-source framework for role-based LLM trading agents.\n\n10. [A Multimodal Foundation Agent for Financial Trading](2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf)\n- Year: 2024\n- arXiv: 2402.18485\n- Why relevant: Strong multimodal baseline (text + price/chart + tools) for comparison with long-horizon methods.\n\n## Notes\n- Structured metadata: \"'`papers/papers_metadata.json`\n- Deep-reading chunk outputs for key papers: `papers/pages/`\nEOF\n\ncat > datasets/.gitignore << '\"'EOF'\n# Exclude all data files (can be large)\n*\n\n# But include documentation and small samples\n\"'!.gitignore\n!README.md\n!**/README.md\n!**/samples/\n!**/samples.json\n!**/sample_100_rows.csv\n!**/download.sh\n!**/download_instructions.txt\n!dataset_stats.json\nEOF\n\ncat > datasets/README.md << '\"'EOF'\n# Downloaded Datasets\n\nThis directory contains datasets for experiments on long-horizon LLM trading.\nLarge data files are intentionally excluded from git by \"'`datasets/.gitignore`.\n\n## Dataset 1: market_daily_ohlcv_2010_2026\n\n### Overview\n- Source: Yahoo Finance via `yfinance`\n- Size: 60,660 rows across 15 tickers\n- Format: Parquet (`prices.parquet`) + CSV sample\n- Task: Daily portfolio/trading decision simulation\n- Splits: Time-based split recommended (e.g., train 2010-2020, val 2021-2023, test 2024-2026)\n- License: Yahoo Finance terms\n\n### Download Instructions\n\nUsing Python (recommended):\n```python\nimport pandas as pd\nimport yfinance as yf\n\nsymbols = [\"AAPL\",\"MSFT\",\"JPM\",\"GS\",\"V\",\"JNJ\",\"WMT\",\"DIS\",\"CAT\",\"IBM\",\"KO\",\"MCD\",\"NKE\",\"NVDA\",\"SPY\"]\nframes = []\nfor s in symbols:\n    df = yf.download(s, start=\"2010-01-01\", end=\"2026-01-31\", auto_adjust=False, progress=False)\n    df = df.reset_index()\n    df[\"ticker\"] = s\n    frames.append(df[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\",\"ticker\"]])\nprices = pd.concat(frames, ignore_index=True)\nprices.to_parquet(\"datasets/market_daily_ohlcv_2010_2026/prices.parquet\", index=False)\n```\n\n### Loading the Dataset\n```python\nimport pandas as pd\nprices = pd.read_parquet(\"datasets/market_daily_ohlcv_2010_2026/prices.parquet\")\n```\n\n### Sample Data\n- `datasets/market_daily_ohlcv_2010_2026/sample_100_rows.csv`\n\n### Notes\n- Directly aligned with long-horizon backtesting and portfolio decisions.\n- Includes market regimes (post-2010 bull/bear/recovery periods).\n\n## Dataset 2: twitter_financial_news_sentiment\n\n### Overview\n- Source: HuggingFace `zeroshot/twitter-financial-news-sentiment`\n- Size: train 9,543 / validation 2,388\n- Format: HuggingFace dataset saved to disk + JSON sample\n- Task: Financial text sentiment signal extraction for trading agent context\n- Splits: Provided by source dataset (train/validation)\n- License: See dataset card on HuggingFace\n\n### Download Instructions\n\nUsing HuggingFace:\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\nds.save_to_disk(\"datasets/twitter_financial_news_sentiment/hf_dataset\")\n```\n\n### Loading the Dataset\n```python\nfrom datasets import load_from_disk\n\nds = load_from_disk(\"datasets/twitter_financial_news_sentiment/hf_dataset\")\n```\n\n### Sample Data\n- `datasets/twitter_financial_news_sentiment/samples/samples.json`\n\n### Notes\n- Useful for building sentiment features to combine with price signals.\n- Best used as auxiliary signal, not standalone trading objective.\n\n## Validation Summary\n- Dataset stats: `datasets/dataset_stats.json`\n- Quick checks completed: file existence, row counts, schema, and sample export.\nEOF\n\ncat > code/README.md << '\"'EOF'\n# Cloned Repositories\n\n## Repo 1: TradingAgents\n- URL: https://github.com/TauricResearch/TradingAgents\n- Purpose: Multi-agent LLM trading framework with analyst/researcher/risk/trader teams\n- Location: \"'`code/tradingagents/`\n- Key files:\n  - `code/tradingagents/main.py`\n  - `code/tradingagents/tradingagents/`\n  - `code/tradingagents/cli/main.py`\n- Notes:\n  - Requires LLM API keys and market data keys.\n  - Strong baseline for role-based decision pipelines.\n\n## Repo 2: FinGPT\n- URL: https://github.com/AI4Finance-Foundation/FinGPT\n- Purpose: Financial LLM ecosystem (sentiment, forecasting, RAG, benchmark)\n- Location: `code/fingpt/`\n- Key files:\n  - `code/fingpt/fingpt/FinGPT_Forecaster/`\n  - `code/fingpt/fingpt/FinGPT_Benchmark/`\n  - `code/fingpt/fingpt/FinGPT_Sentiment_Analysis_v3/`\n- Notes:\n  - Useful for sentiment + forecasting modules and benchmark datasets.\n  - Includes references to Dow30 forecasting datasets.\n\n## Repo 3: FinRL\n- URL: https://github.com/AI4Finance-Foundation/FinRL\n- Purpose: Financial RL baseline library with train/test/trade pipeline\n- Location: `code/finrl/`\n- Key files:\n  - `code/finrl/finrl/train.py`\n  - `code/finrl/finrl/test.py`\n  - `code/finrl/finrl/trade.py`\n  - `code/finrl/finrl/meta/env_stock_trading/`\n- Notes:\n  - Critical non-LLM baseline (PPO/A2C/DDPG etc.) for comparisons.\n  - Includes reusable market environments and preprocessing modules.\n\n## Potential Application to This Hypothesis\n- Long-horizon LLM strategies can be compared against FinRL baselines on identical data splits.\n- TradingAgents/FinGPT can be adapted to weekly or monthly decision cadence.\n- FinRL can provide risk-aware benchmark policies and execution environments.\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_70","type":"command_execution","command":"/bin/bash -lc \"pwd && cat > papers/README.md << 'EOF'\n# Downloaded Papers\n\nThis folder contains papers for the project **Refining LLM Trading**.\n\n## Priority Papers from Paper-Finder (relevance >= 2)\n\n1. [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf)\n- Authors: Yanxu Chen et al.\n- Year: 2025\n- arXiv: 2510.02209\n- Why relevant: Direct benchmark for multi-month sequential LLM trading and risk-adjusted metrics.\n\n2. [FinPos: A Position-Aware Trading Agent System for Real Financial Markets](2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf)\n- Authors: Bijia Liu, Ronghao Dang\n- Year: 2025\n- arXiv: 2510.27251\n- Why relevant: Explicitly proposes position-aware, longer-horizon decision making instead of one-step trade classification.\n\n3. [Hierarchical Organization Simulacra in the Investment Sector](2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf)\n- Authors: Chung-Chi Chen et al.\n- Year: 2024\n- arXiv: 2410.00354\n- Why relevant: Studies multi-agent investment roles over long time horizons and compares with professional decisions.\n\n4. [Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?](2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf)\n- Authors: Weixian Waylon Li et al.\n- Year: 2025\n- arXiv: 2505.07078\n- Why relevant: Core evidence paper for long-run robustness; introduces broad backtests and highlights bias in short-window gains.\n\n5. [HedgeAgents: A Balanced-aware Multi-agent Financial Trading System](2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf)\n- Authors: Xiangyu Li et al.\n- Year: 2025\n- arXiv: 2502.13165\n- Why relevant: Shows multi-agent risk-balancing and hedging approaches for volatile regimes.\n\n6. [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf)\n- Author: Mostapha Benhenda\n- Year: 2026\n- arXiv: 2601.13770\n- Why relevant: Evaluates temporal leakage and look-ahead bias that often inflate short-term paper results.\n\n7. [Behavioral Consistency Validation for LLM Agents](2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf)\n- Authors: Zeping Li et al.\n- Year: 2026\n- arXiv: 2602.07023\n- Why relevant: Tests whether agent strategy switching behavior remains realistic over longer simulations.\n\n## Foundational/Strong Baseline Papers\n\n8. [FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design](2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf)\n- Year: 2023\n- arXiv: 2311.13743\n- Why relevant: Strong baseline with layered memory and explicit agent persona design.\n\n9. [TradingAgents: Multi-Agents LLM Financial Trading Framework](2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf)\n- Year: 2024\n- arXiv: 2412.20138\n- Why relevant: Widely reused open-source framework for role-based LLM trading agents.\n\n10. [A Multimodal Foundation Agent for Financial Trading](2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf)\n- Year: 2024\n- arXiv: 2402.18485\n- Why relevant: Strong multimodal baseline (text + price/chart + tools) for comparison with long-horizon methods.\n\n## Notes\n- Structured metadata: \"'`papers/papers_metadata.json`\n- Deep-reading chunk outputs for key papers: `papers/pages/`\nEOF\n\ncat > datasets/.gitignore << '\"'EOF'\n# Exclude all data files (can be large)\n*\n\n# But include documentation and small samples\n\"'!.gitignore\n!README.md\n!**/README.md\n!**/samples/\n!**/samples.json\n!**/sample_100_rows.csv\n!**/download.sh\n!**/download_instructions.txt\n!dataset_stats.json\nEOF\n\ncat > datasets/README.md << '\"'EOF'\n# Downloaded Datasets\n\nThis directory contains datasets for experiments on long-horizon LLM trading.\nLarge data files are intentionally excluded from git by \"'`datasets/.gitignore`.\n\n## Dataset 1: market_daily_ohlcv_2010_2026\n\n### Overview\n- Source: Yahoo Finance via `yfinance`\n- Size: 60,660 rows across 15 tickers\n- Format: Parquet (`prices.parquet`) + CSV sample\n- Task: Daily portfolio/trading decision simulation\n- Splits: Time-based split recommended (e.g., train 2010-2020, val 2021-2023, test 2024-2026)\n- License: Yahoo Finance terms\n\n### Download Instructions\n\nUsing Python (recommended):\n```python\nimport pandas as pd\nimport yfinance as yf\n\nsymbols = [\"AAPL\",\"MSFT\",\"JPM\",\"GS\",\"V\",\"JNJ\",\"WMT\",\"DIS\",\"CAT\",\"IBM\",\"KO\",\"MCD\",\"NKE\",\"NVDA\",\"SPY\"]\nframes = []\nfor s in symbols:\n    df = yf.download(s, start=\"2010-01-01\", end=\"2026-01-31\", auto_adjust=False, progress=False)\n    df = df.reset_index()\n    df[\"ticker\"] = s\n    frames.append(df[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\",\"ticker\"]])\nprices = pd.concat(frames, ignore_index=True)\nprices.to_parquet(\"datasets/market_daily_ohlcv_2010_2026/prices.parquet\", index=False)\n```\n\n### Loading the Dataset\n```python\nimport pandas as pd\nprices = pd.read_parquet(\"datasets/market_daily_ohlcv_2010_2026/prices.parquet\")\n```\n\n### Sample Data\n- `datasets/market_daily_ohlcv_2010_2026/sample_100_rows.csv`\n\n### Notes\n- Directly aligned with long-horizon backtesting and portfolio decisions.\n- Includes market regimes (post-2010 bull/bear/recovery periods).\n\n## Dataset 2: twitter_financial_news_sentiment\n\n### Overview\n- Source: HuggingFace `zeroshot/twitter-financial-news-sentiment`\n- Size: train 9,543 / validation 2,388\n- Format: HuggingFace dataset saved to disk + JSON sample\n- Task: Financial text sentiment signal extraction for trading agent context\n- Splits: Provided by source dataset (train/validation)\n- License: See dataset card on HuggingFace\n\n### Download Instructions\n\nUsing HuggingFace:\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\nds.save_to_disk(\"datasets/twitter_financial_news_sentiment/hf_dataset\")\n```\n\n### Loading the Dataset\n```python\nfrom datasets import load_from_disk\n\nds = load_from_disk(\"datasets/twitter_financial_news_sentiment/hf_dataset\")\n```\n\n### Sample Data\n- `datasets/twitter_financial_news_sentiment/samples/samples.json`\n\n### Notes\n- Useful for building sentiment features to combine with price signals.\n- Best used as auxiliary signal, not standalone trading objective.\n\n## Validation Summary\n- Dataset stats: `datasets/dataset_stats.json`\n- Quick checks completed: file existence, row counts, schema, and sample export.\nEOF\n\ncat > code/README.md << '\"'EOF'\n# Cloned Repositories\n\n## Repo 1: TradingAgents\n- URL: https://github.com/TauricResearch/TradingAgents\n- Purpose: Multi-agent LLM trading framework with analyst/researcher/risk/trader teams\n- Location: \"'`code/tradingagents/`\n- Key files:\n  - `code/tradingagents/main.py`\n  - `code/tradingagents/tradingagents/`\n  - `code/tradingagents/cli/main.py`\n- Notes:\n  - Requires LLM API keys and market data keys.\n  - Strong baseline for role-based decision pipelines.\n\n## Repo 2: FinGPT\n- URL: https://github.com/AI4Finance-Foundation/FinGPT\n- Purpose: Financial LLM ecosystem (sentiment, forecasting, RAG, benchmark)\n- Location: `code/fingpt/`\n- Key files:\n  - `code/fingpt/fingpt/FinGPT_Forecaster/`\n  - `code/fingpt/fingpt/FinGPT_Benchmark/`\n  - `code/fingpt/fingpt/FinGPT_Sentiment_Analysis_v3/`\n- Notes:\n  - Useful for sentiment + forecasting modules and benchmark datasets.\n  - Includes references to Dow30 forecasting datasets.\n\n## Repo 3: FinRL\n- URL: https://github.com/AI4Finance-Foundation/FinRL\n- Purpose: Financial RL baseline library with train/test/trade pipeline\n- Location: `code/finrl/`\n- Key files:\n  - `code/finrl/finrl/train.py`\n  - `code/finrl/finrl/test.py`\n  - `code/finrl/finrl/trade.py`\n  - `code/finrl/finrl/meta/env_stock_trading/`\n- Notes:\n  - Critical non-LLM baseline (PPO/A2C/DDPG etc.) for comparisons.\n  - Includes reusable market environments and preprocessing modules.\n\n## Potential Application to This Hypothesis\n- Long-horizon LLM strategies can be compared against FinRL baselines on identical data splits.\n- TradingAgents/FinGPT can be adapted to weekly or monthly decision cadence.\n- FinRL can provide risk-aware benchmark policies and execution environments.\nEOF'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_71","type":"reasoning","text":"**Preparing literature review materials**"}}
{"type":"item.started","item":{"id":"item_72","type":"command_execution","command":"/bin/bash -lc \"pwd && cat > literature_review.md << 'EOF'\n# Literature Review: Refining LLM Trading Toward Long-Horizon Decisions\n\n## Review Scope\n\n### Research Question\nDo LLM trading agents perform better when optimized for longer-horizon, position-aware decisions rather than short-term day-to-day profit?\n\n### Inclusion Criteria\n- LLM-agent-based trading or financial decision systems\n- Benchmarks with sequential decision making (not only static QA)\n- Works reporting portfolio/risk metrics (return, Sharpe/Sortino, drawdown)\n- Practical code availability or reproducible setup\n\n### Exclusion Criteria\n- Pure sentiment classification without trading evaluation\n- Static financial QA with no sequential decision loop\n- Non-financial agent work without market environment testing\n\n### Time Frame\n- Primary focus: 2023-2026\n\n### Sources\n- Paper-finder skill output (\"'`paper_search_results/*.jsonl`)\n- arXiv PDFs downloaded to `papers/`\n- GitHub baseline repositories (`code/`)\n\n## Search Log\n\n| Date | Query | Source | Results | Notes |\n|------|-------|--------|---------|------|\n| 2026-02-22 | `LLM agents long-term trading decisions finance` | paper-finder | 71 | 7 high-relevance papers (`relevance >= 2`) |\n| 2026-02-22 | title-matched arXiv retrieval | arXiv API | 10 downloaded | Added key baseline papers (FinMem/TradingAgents/FinAgent) |\n\n## Screening Results\n\n- Title/abstract screened: 71\n- Full papers downloaded: 10\n- Deep-read with PDF chunker: 4\n  - `StockBench`\n  - `FinPos`\n  - `Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?` (FINSABER)\n  - `Look-Ahead-Bench`\n\n## Key Papers\n\n### 1) StockBench (2025)\n- **Authors**: Yanxu Chen et al.\n- **Source**: arXiv 2510.02209\n- **Key Contribution**: Contamination-aware benchmark for realistic multi-month LLM trading.\n- **Methodology**: Daily sequential agent decisions using price/fundamental/news signals.\n- **Datasets Used**: Dow Jones stocks with point-in-time style market inputs.\n- **Baselines**: Buy-and-hold and multiple frontier LLMs.\n- **Metrics**: Cumulative return, maximum drawdown, Sortino ratio, volatility.\n- **Key Result**: Most LLM agents fail to consistently beat buy-and-hold; some models improve risk-adjusted behavior.\n- **Code**: https://github.com/ChenYXxxx/stockbench\n- **Relevance**: Strong evidence that long-horizon evaluation is necessary and harder than short-run tests suggest.\n\n### 2) FinPos (2025)\n- **Authors**: Bijia Liu, Ronghao Dang\n- **Source**: arXiv 2510.27251\n- **Key Contribution**: Position-aware task formulation for continuous portfolio states.\n- **Methodology**: Dual-stage decision (direction + risk/size), multi-timescale reward reflection.\n- **Datasets Used**: Multi-stock US equities with Yahoo Finance style inputs and macro/news context.\n- **Baselines**: Rule-based (MACD/RSI), RL (A2C/DQN/PPO), LLM agents (FinMem/FinCon/TradingAgents/FinAgent).\n- **Metrics**: CR, Sharpe, MDD, Calmar.\n- **Key Result**: Position-aware architecture improves stability and risk-adjusted outcomes vs one-step agents.\n- **Code**: not clearly released in paper text.\n- **Relevance**: Directly supports the hypothesis that longer-term, position continuity helps LLM trading systems.\n\n### 3) Can LLM-based Financial Investing Strategies Outperform the Market in Long Run? (FINSABER, 2025)\n- **Authors**: Weixian Waylon Li et al.\n- **Source**: arXiv 2505.07078\n- **Key Contribution**: Long-horizon robustness study over broader universe and longer windows.\n- **Methodology**: Two-decade-style backtesting framework with regime analysis and broad symbol coverage.\n- **Datasets Used**: US equities universe (S&P 500-centric setup), external financial/news data integrations.\n- **Baselines**: Buy-and-hold, factor/rule strategies, RL, and LLM-agent baselines (FinMem/FinAgent/TradingAgents).\n- **Metrics**: Sharpe, Sortino, alpha, beta, CR, MDD, volatility.\n- **Key Result**: Apparent gains from narrow windows deteriorate under broad long-run testing.\n- **Code**: https://github.com/waylonli/FINSABER\n- **Relevance**: Core support for the proposed research direction and robust experimental design requirements.\n\n### 4) Look-Ahead-Bench (2026)\n- **Author**: Mostapha Benhenda\n- **Source**: arXiv 2601.13770\n- **Key Contribution**: Standardized benchmark for measuring temporal leakage/look-ahead bias.\n- **Methodology**: Practical workflow evaluation with decay analysis across market regimes.\n- **Datasets Used**: S&P 500-like setup with point-in-time constraints.\n- **Baselines**: Market-cap weighted, momentum, buy-and-hold style quantitative baselines + standard vs PiT LLMs.\n- **Metrics**: Alpha decay and regime robustness.\n- **Key Result**: Conventional LLMs exhibit stronger look-ahead bias; PiT models are more stable.\n- **Code**: https://github.com/benstaf/lookaheadbench\n- **Relevance**: Essential for preventing false conclusions in short-term backtests.\n\n### 5) FinMem (2023)\n- Layered memory + persona design for LLM trading agents.\n- Strong historical baseline for architecture comparisons.\n\n### 6) TradingAgents (2024)\n- Open multi-agent framework inspired by trading-firm organization.\n- Good engineering baseline for modular long-horizon experiments.\n\n### 7) FinAgent (2024)\n- Multimodal (text/chart/numeric) foundation agent with tool augmentation.\n- Useful baseline for integrating multiple signal modalities.\n\n### 8) HedgeAgents (2025)\n- Hedging-aware multi-agent design for improved robustness during volatile periods.\n- Useful for risk-aware portfolio-level comparisons.\n\n### 9) Hierarchical Organization Simulacra (2024)\n- Simulates hierarchical investment organizations over long history and many companies.\n- Highlights influence of prompt/role design on behavior and profitability.\n\n### 10) Behavioral Consistency Validation (2026)\n- Examines whether agent style switching aligns with behavioral finance theory.\n- Useful for evaluating realism and stability of long-run agent behavior.\n\n## Common Methodologies\n\n- **Role-based multi-agent decomposition**: TradingAgents, FinPos, HedgeAgents, FinAgent.\n- **Memory/reflection loops**: FinMem, FinPos, FinCon-style systems.\n- **Point-in-time or contamination-aware evaluation**: StockBench, Look-Ahead-Bench, FINSABER.\n- **Hybrid signal fusion**: price + technical indicators + macro/news/fundamentals.\n\n## Standard Baselines\n\n- **Passive**: Buy-and-hold (must include).\n- **Rule-based**: MACD, RSI, SMA/WMA crossover, momentum.\n- **RL baselines**: A2C/DQN/PPO and FinRL pipelines.\n- **LLM-agent baselines**: FinMem, TradingAgents, FinAgent/FinCon-style variants.\n\n## Evaluation Metrics\n\n- **Return metrics**: Cumulative return, annualized return.\n- **Risk-adjusted metrics**: Sharpe, Sortino, Calmar.\n- **Risk metrics**: Maximum drawdown, volatility.\n- **Bias/validity metrics**: Alpha decay/look-ahead robustness by regime.\n\n## Datasets in the Literature\n\n- **US equities (Dow/S&P universes)**: Common in StockBench, FinPos, FINSABER, Look-Ahead-Bench.\n- **Price + fundamental + news combinations**: Typical for LLM-agent systems.\n- **Sentiment corpora (e.g., finance tweets/news)**: Used for auxiliary signals and LLM adaptation.\n\n## Gaps and Opportunities\n\n1. Many claimed gains are sensitive to short windows, limited symbol sets, or leakage.\n2. Position continuity and risk-aware sizing are under-studied relative to directional prediction.\n3. Behavioral realism and regime adaptation are not yet standard evaluation requirements.\n4. Few studies provide robust out-of-period, multi-regime, and multi-universe comparisons simultaneously.\n\n## Recommendations for Our Experiment\n\n- **Recommended datasets**:\n  - `datasets/market_daily_ohlcv_2010_2026/` for long-run multi-asset backtesting.\n  - `datasets/twitter_financial_news_sentiment/` for auxiliary sentiment context.\n- **Recommended baselines**:\n  - Buy-and-hold, MACD/RSI/SMA crossover, FinRL PPO/A2C.\n  - LLM-agent baselines adapted from TradingAgents and/or FinMem-style memory policies.\n- **Recommended metrics**:\n  - CR, Sharpe, Sortino, MDD, Calmar, volatility.\n  - Regime-sliced performance and alpha-decay style leakage diagnostics.\n- **Methodological considerations**:\n  - Enforce point-in-time data boundaries.\n  - Use time-based splits and rolling/expanding-window evaluation.\n  - Compare daily vs weekly/monthly decision cadence to test the hypothesis directly.\nEOF\n\ncat > resources.md << '\"'EOF'\n# Resources Catalog\n\n## Summary\nThis document catalogs all gathered resources for the project **Refining LLM Trading**, including papers, datasets, and code repositories.\n\n## Papers\nTotal papers downloaded: 10\n\n| Title | Authors | Year | File | Key Info |\n|-------|---------|------|------|----------|\n| StockBench | Chen et al. | 2025 | \"'`papers/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf` | Multi-month benchmark; buy-and-hold remains hard to beat |\n| FinPos | Liu, Dang | 2025 | `papers/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf` | Position-aware long-horizon LLM trading |\n| Hierarchical Organization Simulacra | Chen et al. | 2024 | `papers/2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf` | Multi-agent hierarchy over long history |\n| Can LLM-based ... Outperform ... Long Run? | Li et al. | 2025 | `papers/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf` | Long-horizon robustness and survivorship-bias critique |\n| HedgeAgents | Li et al. | 2025 | `papers/2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf` | Hedging-aware architecture |\n| Look-Ahead-Bench | Benhenda | 2026 | `papers/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf` | Temporal leakage benchmark |\n| Behavioral Consistency Validation | Li et al. | 2026 | `papers/2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf` | Behavioral-finance consistency checks |\n| FinMem | Yu et al. | 2023 | `papers/2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf` | Layered memory baseline |\n| TradingAgents | Xiao et al. | 2024 | `papers/2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf` | Open-source multi-agent baseline |\n| FinAgent | Zhang et al. | 2024 | `papers/2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf` | Multimodal foundation trading agent |\n\nSee `papers/README.md` and `papers/papers_metadata.json` for details.\n\n## Datasets\nTotal datasets downloaded: 2\n\n| Name | Source | Size | Task | Location | Notes |\n|------|--------|------|------|----------|-------|\n| market_daily_ohlcv_2010_2026 | Yahoo Finance (`yfinance`) | 60,660 rows, 15 tickers | Long-horizon trading/backtest | `datasets/market_daily_ohlcv_2010_2026/` | Includes `prices.parquet` + sample CSV |\n| twitter_financial_news_sentiment | HuggingFace `zeroshot/twitter-financial-news-sentiment` | Train 9,543 / Val 2,388 | Sentiment feature extraction | `datasets/twitter_financial_news_sentiment/` | Saved to disk + sample JSON |\n\nSee `datasets/README.md` and `datasets/dataset_stats.json`.\n\n## Code Repositories\nTotal repositories cloned: 3\n\n| Name | URL | Purpose | Location | Notes |\n|------|-----|---------|----------|-------|\n| TradingAgents | https://github.com/TauricResearch/TradingAgents | LLM multi-agent trading framework | `code/tradingagents/` | Practical baseline architecture |\n| FinGPT | https://github.com/AI4Finance-Foundation/FinGPT | Financial LLM ecosystem (sentiment, forecasting, benchmark) | `code/fingpt/` | Useful modules + datasets references |\n| FinRL | https://github.com/AI4Finance-Foundation/FinRL | RL trading baselines and environments | `code/finrl/` | Strong non-LLM baseline suite |\n\nSee `code/README.md`.\n\n## Resource Gathering Notes\n\n### Search Strategy\n- Used paper-finder (`find_papers.py`) as primary search method.\n- Filtered high-priority papers by relevance score (`>=2`).\n- Added high-impact baseline papers required for experiment design.\n- Downloaded all selected papers as PDFs into `papers/`.\n\n### Selection Criteria\n- Direct relevance to long-horizon vs short-horizon LLM trading.\n- Availability of reproducible methodology and benchmarks.\n- Presence of robust metrics and risk-aware evaluation.\n- Open-source code preferred.\n\n### Challenges Encountered\n- `uv add` failed because this repo is not packaged for editable install; used `uv pip install` fallback.\n- HF dataset `financial_phrasebank` was script-based and unsupported in current `datasets` package; used script-free alternative (`twitter-financial-news-sentiment`).\n\n### Gaps and Workarounds\n- Some newest papers provide limited implementation details; supplemented with open-source framework repos (TradingAgents, FinGPT, FinRL).\n- For robust long-horizon experiments, point-in-time and leakage controls must be enforced during experiment-runner phase.\n\n## Recommendations for Experiment Design\n\n1. **Primary dataset(s)**: `market_daily_ohlcv_2010_2026` for sequential long-horizon evaluation; augment with `twitter_financial_news_sentiment` for sentiment signals.\n2. **Baseline methods**: Buy-and-hold, MACD/RSI/SMA crossover, FinRL PPO/A2C, TradingAgents default workflow.\n3. **Evaluation metrics**: CR, Sharpe, Sortino, MDD, Calmar, volatility, plus regime-sliced performance and leakage checks.\n4. **Code to adapt/reuse**: `code/tradingagents/` for LLM-agent orchestration, `code/finrl/` for RL baselines/environments, `code/fingpt/` for finance-specific LLM modules.\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_72","type":"command_execution","command":"/bin/bash -lc \"pwd && cat > literature_review.md << 'EOF'\n# Literature Review: Refining LLM Trading Toward Long-Horizon Decisions\n\n## Review Scope\n\n### Research Question\nDo LLM trading agents perform better when optimized for longer-horizon, position-aware decisions rather than short-term day-to-day profit?\n\n### Inclusion Criteria\n- LLM-agent-based trading or financial decision systems\n- Benchmarks with sequential decision making (not only static QA)\n- Works reporting portfolio/risk metrics (return, Sharpe/Sortino, drawdown)\n- Practical code availability or reproducible setup\n\n### Exclusion Criteria\n- Pure sentiment classification without trading evaluation\n- Static financial QA with no sequential decision loop\n- Non-financial agent work without market environment testing\n\n### Time Frame\n- Primary focus: 2023-2026\n\n### Sources\n- Paper-finder skill output (\"'`paper_search_results/*.jsonl`)\n- arXiv PDFs downloaded to `papers/`\n- GitHub baseline repositories (`code/`)\n\n## Search Log\n\n| Date | Query | Source | Results | Notes |\n|------|-------|--------|---------|------|\n| 2026-02-22 | `LLM agents long-term trading decisions finance` | paper-finder | 71 | 7 high-relevance papers (`relevance >= 2`) |\n| 2026-02-22 | title-matched arXiv retrieval | arXiv API | 10 downloaded | Added key baseline papers (FinMem/TradingAgents/FinAgent) |\n\n## Screening Results\n\n- Title/abstract screened: 71\n- Full papers downloaded: 10\n- Deep-read with PDF chunker: 4\n  - `StockBench`\n  - `FinPos`\n  - `Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?` (FINSABER)\n  - `Look-Ahead-Bench`\n\n## Key Papers\n\n### 1) StockBench (2025)\n- **Authors**: Yanxu Chen et al.\n- **Source**: arXiv 2510.02209\n- **Key Contribution**: Contamination-aware benchmark for realistic multi-month LLM trading.\n- **Methodology**: Daily sequential agent decisions using price/fundamental/news signals.\n- **Datasets Used**: Dow Jones stocks with point-in-time style market inputs.\n- **Baselines**: Buy-and-hold and multiple frontier LLMs.\n- **Metrics**: Cumulative return, maximum drawdown, Sortino ratio, volatility.\n- **Key Result**: Most LLM agents fail to consistently beat buy-and-hold; some models improve risk-adjusted behavior.\n- **Code**: https://github.com/ChenYXxxx/stockbench\n- **Relevance**: Strong evidence that long-horizon evaluation is necessary and harder than short-run tests suggest.\n\n### 2) FinPos (2025)\n- **Authors**: Bijia Liu, Ronghao Dang\n- **Source**: arXiv 2510.27251\n- **Key Contribution**: Position-aware task formulation for continuous portfolio states.\n- **Methodology**: Dual-stage decision (direction + risk/size), multi-timescale reward reflection.\n- **Datasets Used**: Multi-stock US equities with Yahoo Finance style inputs and macro/news context.\n- **Baselines**: Rule-based (MACD/RSI), RL (A2C/DQN/PPO), LLM agents (FinMem/FinCon/TradingAgents/FinAgent).\n- **Metrics**: CR, Sharpe, MDD, Calmar.\n- **Key Result**: Position-aware architecture improves stability and risk-adjusted outcomes vs one-step agents.\n- **Code**: not clearly released in paper text.\n- **Relevance**: Directly supports the hypothesis that longer-term, position continuity helps LLM trading systems.\n\n### 3) Can LLM-based Financial Investing Strategies Outperform the Market in Long Run? (FINSABER, 2025)\n- **Authors**: Weixian Waylon Li et al.\n- **Source**: arXiv 2505.07078\n- **Key Contribution**: Long-horizon robustness study over broader universe and longer windows.\n- **Methodology**: Two-decade-style backtesting framework with regime analysis and broad symbol coverage.\n- **Datasets Used**: US equities universe (S&P 500-centric setup), external financial/news data integrations.\n- **Baselines**: Buy-and-hold, factor/rule strategies, RL, and LLM-agent baselines (FinMem/FinAgent/TradingAgents).\n- **Metrics**: Sharpe, Sortino, alpha, beta, CR, MDD, volatility.\n- **Key Result**: Apparent gains from narrow windows deteriorate under broad long-run testing.\n- **Code**: https://github.com/waylonli/FINSABER\n- **Relevance**: Core support for the proposed research direction and robust experimental design requirements.\n\n### 4) Look-Ahead-Bench (2026)\n- **Author**: Mostapha Benhenda\n- **Source**: arXiv 2601.13770\n- **Key Contribution**: Standardized benchmark for measuring temporal leakage/look-ahead bias.\n- **Methodology**: Practical workflow evaluation with decay analysis across market regimes.\n- **Datasets Used**: S&P 500-like setup with point-in-time constraints.\n- **Baselines**: Market-cap weighted, momentum, buy-and-hold style quantitative baselines + standard vs PiT LLMs.\n- **Metrics**: Alpha decay and regime robustness.\n- **Key Result**: Conventional LLMs exhibit stronger look-ahead bias; PiT models are more stable.\n- **Code**: https://github.com/benstaf/lookaheadbench\n- **Relevance**: Essential for preventing false conclusions in short-term backtests.\n\n### 5) FinMem (2023)\n- Layered memory + persona design for LLM trading agents.\n- Strong historical baseline for architecture comparisons.\n\n### 6) TradingAgents (2024)\n- Open multi-agent framework inspired by trading-firm organization.\n- Good engineering baseline for modular long-horizon experiments.\n\n### 7) FinAgent (2024)\n- Multimodal (text/chart/numeric) foundation agent with tool augmentation.\n- Useful baseline for integrating multiple signal modalities.\n\n### 8) HedgeAgents (2025)\n- Hedging-aware multi-agent design for improved robustness during volatile periods.\n- Useful for risk-aware portfolio-level comparisons.\n\n### 9) Hierarchical Organization Simulacra (2024)\n- Simulates hierarchical investment organizations over long history and many companies.\n- Highlights influence of prompt/role design on behavior and profitability.\n\n### 10) Behavioral Consistency Validation (2026)\n- Examines whether agent style switching aligns with behavioral finance theory.\n- Useful for evaluating realism and stability of long-run agent behavior.\n\n## Common Methodologies\n\n- **Role-based multi-agent decomposition**: TradingAgents, FinPos, HedgeAgents, FinAgent.\n- **Memory/reflection loops**: FinMem, FinPos, FinCon-style systems.\n- **Point-in-time or contamination-aware evaluation**: StockBench, Look-Ahead-Bench, FINSABER.\n- **Hybrid signal fusion**: price + technical indicators + macro/news/fundamentals.\n\n## Standard Baselines\n\n- **Passive**: Buy-and-hold (must include).\n- **Rule-based**: MACD, RSI, SMA/WMA crossover, momentum.\n- **RL baselines**: A2C/DQN/PPO and FinRL pipelines.\n- **LLM-agent baselines**: FinMem, TradingAgents, FinAgent/FinCon-style variants.\n\n## Evaluation Metrics\n\n- **Return metrics**: Cumulative return, annualized return.\n- **Risk-adjusted metrics**: Sharpe, Sortino, Calmar.\n- **Risk metrics**: Maximum drawdown, volatility.\n- **Bias/validity metrics**: Alpha decay/look-ahead robustness by regime.\n\n## Datasets in the Literature\n\n- **US equities (Dow/S&P universes)**: Common in StockBench, FinPos, FINSABER, Look-Ahead-Bench.\n- **Price + fundamental + news combinations**: Typical for LLM-agent systems.\n- **Sentiment corpora (e.g., finance tweets/news)**: Used for auxiliary signals and LLM adaptation.\n\n## Gaps and Opportunities\n\n1. Many claimed gains are sensitive to short windows, limited symbol sets, or leakage.\n2. Position continuity and risk-aware sizing are under-studied relative to directional prediction.\n3. Behavioral realism and regime adaptation are not yet standard evaluation requirements.\n4. Few studies provide robust out-of-period, multi-regime, and multi-universe comparisons simultaneously.\n\n## Recommendations for Our Experiment\n\n- **Recommended datasets**:\n  - `datasets/market_daily_ohlcv_2010_2026/` for long-run multi-asset backtesting.\n  - `datasets/twitter_financial_news_sentiment/` for auxiliary sentiment context.\n- **Recommended baselines**:\n  - Buy-and-hold, MACD/RSI/SMA crossover, FinRL PPO/A2C.\n  - LLM-agent baselines adapted from TradingAgents and/or FinMem-style memory policies.\n- **Recommended metrics**:\n  - CR, Sharpe, Sortino, MDD, Calmar, volatility.\n  - Regime-sliced performance and alpha-decay style leakage diagnostics.\n- **Methodological considerations**:\n  - Enforce point-in-time data boundaries.\n  - Use time-based splits and rolling/expanding-window evaluation.\n  - Compare daily vs weekly/monthly decision cadence to test the hypothesis directly.\nEOF\n\ncat > resources.md << '\"'EOF'\n# Resources Catalog\n\n## Summary\nThis document catalogs all gathered resources for the project **Refining LLM Trading**, including papers, datasets, and code repositories.\n\n## Papers\nTotal papers downloaded: 10\n\n| Title | Authors | Year | File | Key Info |\n|-------|---------|------|------|----------|\n| StockBench | Chen et al. | 2025 | \"'`papers/2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf` | Multi-month benchmark; buy-and-hold remains hard to beat |\n| FinPos | Liu, Dang | 2025 | `papers/2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf` | Position-aware long-horizon LLM trading |\n| Hierarchical Organization Simulacra | Chen et al. | 2024 | `papers/2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf` | Multi-agent hierarchy over long history |\n| Can LLM-based ... Outperform ... Long Run? | Li et al. | 2025 | `papers/2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf` | Long-horizon robustness and survivorship-bias critique |\n| HedgeAgents | Li et al. | 2025 | `papers/2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf` | Hedging-aware architecture |\n| Look-Ahead-Bench | Benhenda | 2026 | `papers/2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf` | Temporal leakage benchmark |\n| Behavioral Consistency Validation | Li et al. | 2026 | `papers/2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf` | Behavioral-finance consistency checks |\n| FinMem | Yu et al. | 2023 | `papers/2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf` | Layered memory baseline |\n| TradingAgents | Xiao et al. | 2024 | `papers/2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf` | Open-source multi-agent baseline |\n| FinAgent | Zhang et al. | 2024 | `papers/2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf` | Multimodal foundation trading agent |\n\nSee `papers/README.md` and `papers/papers_metadata.json` for details.\n\n## Datasets\nTotal datasets downloaded: 2\n\n| Name | Source | Size | Task | Location | Notes |\n|------|--------|------|------|----------|-------|\n| market_daily_ohlcv_2010_2026 | Yahoo Finance (`yfinance`) | 60,660 rows, 15 tickers | Long-horizon trading/backtest | `datasets/market_daily_ohlcv_2010_2026/` | Includes `prices.parquet` + sample CSV |\n| twitter_financial_news_sentiment | HuggingFace `zeroshot/twitter-financial-news-sentiment` | Train 9,543 / Val 2,388 | Sentiment feature extraction | `datasets/twitter_financial_news_sentiment/` | Saved to disk + sample JSON |\n\nSee `datasets/README.md` and `datasets/dataset_stats.json`.\n\n## Code Repositories\nTotal repositories cloned: 3\n\n| Name | URL | Purpose | Location | Notes |\n|------|-----|---------|----------|-------|\n| TradingAgents | https://github.com/TauricResearch/TradingAgents | LLM multi-agent trading framework | `code/tradingagents/` | Practical baseline architecture |\n| FinGPT | https://github.com/AI4Finance-Foundation/FinGPT | Financial LLM ecosystem (sentiment, forecasting, benchmark) | `code/fingpt/` | Useful modules + datasets references |\n| FinRL | https://github.com/AI4Finance-Foundation/FinRL | RL trading baselines and environments | `code/finrl/` | Strong non-LLM baseline suite |\n\nSee `code/README.md`.\n\n## Resource Gathering Notes\n\n### Search Strategy\n- Used paper-finder (`find_papers.py`) as primary search method.\n- Filtered high-priority papers by relevance score (`>=2`).\n- Added high-impact baseline papers required for experiment design.\n- Downloaded all selected papers as PDFs into `papers/`.\n\n### Selection Criteria\n- Direct relevance to long-horizon vs short-horizon LLM trading.\n- Availability of reproducible methodology and benchmarks.\n- Presence of robust metrics and risk-aware evaluation.\n- Open-source code preferred.\n\n### Challenges Encountered\n- `uv add` failed because this repo is not packaged for editable install; used `uv pip install` fallback.\n- HF dataset `financial_phrasebank` was script-based and unsupported in current `datasets` package; used script-free alternative (`twitter-financial-news-sentiment`).\n\n### Gaps and Workarounds\n- Some newest papers provide limited implementation details; supplemented with open-source framework repos (TradingAgents, FinGPT, FinRL).\n- For robust long-horizon experiments, point-in-time and leakage controls must be enforced during experiment-runner phase.\n\n## Recommendations for Experiment Design\n\n1. **Primary dataset(s)**: `market_daily_ohlcv_2010_2026` for sequential long-horizon evaluation; augment with `twitter_financial_news_sentiment` for sentiment signals.\n2. **Baseline methods**: Buy-and-hold, MACD/RSI/SMA crossover, FinRL PPO/A2C, TradingAgents default workflow.\n3. **Evaluation metrics**: CR, Sharpe, Sortino, MDD, Calmar, volatility, plus regime-sliced performance and leakage checks.\n4. **Code to adapt/reuse**: `code/tradingagents/` for LLM-agent orchestration, `code/finrl/` for RL baselines/environments, `code/fingpt/` for finance-specific LLM modules.\nEOF'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_73","type":"reasoning","text":"**Creating completion marker with timestamp**"}}
{"type":"item.started","item":{"id":"item_74","type":"command_execution","command":"/bin/bash -lc 'pwd && papers_count=$(ls papers/*.pdf 2>/dev/null | wc -l) && datasets_count=2 && repos_count=$(find code -mindepth 1 -maxdepth 1 -type d | wc -l) && ts=$(date -Iseconds) && cat > .resource_finder_complete << EOF\nResource finding phase completed successfully.\nTimestamp: ${ts}\nPapers downloaded: ${papers_count}\nDatasets downloaded: ${datasets_count}\nRepositories cloned: ${repos_count}\nEOF\ncat .resource_finder_complete'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_74","type":"command_execution","command":"/bin/bash -lc 'pwd && papers_count=$(ls papers/*.pdf 2>/dev/null | wc -l) && datasets_count=2 && repos_count=$(find code -mindepth 1 -maxdepth 1 -type d | wc -l) && ts=$(date -Iseconds) && cat > .resource_finder_complete << EOF\nResource finding phase completed successfully.\nTimestamp: ${ts}\nPapers downloaded: ${papers_count}\nDatasets downloaded: ${datasets_count}\nRepositories cloned: ${repos_count}\nEOF\ncat .resource_finder_complete'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\nResource finding phase completed successfully.\nTimestamp: 2026-02-22T01:42:39-06:00\nPapers downloaded: 10\nDatasets downloaded: 2\nRepositories cloned: 3\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_75","type":"reasoning","text":"**Planning final file verification**"}}
{"type":"item.completed","item":{"id":"item_76","type":"agent_message","text":"Documentation and completion marker are in place. I’m running a final checklist now to verify all required files/directories and counts before I report completion."}}
{"type":"item.started","item":{"id":"item_77","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la papers | sed -n '1,200p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_78","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la datasets | sed -n '1,200p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_79","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code | sed -n '1,200p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_80","type":"command_execution","command":"/bin/bash -lc 'pwd && ls -l literature_review.md resources.md .resource_finder_complete'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_77","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la papers | sed -n '1,200p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 29852\ndrwxrwx---  3 haokunliu haokunliu       15 Feb 22 01:41 .\ndrwxrwx--- 15 haokunliu haokunliu       22 Feb 22 01:42 ..\n-rw-rw----  1 haokunliu haokunliu  7799225 Feb 22 01:38 2311.13743v2_FinMem_A_Performance_Enhanced_LLM_Trading_Agent_With_Layered_Memory_an.pdf\n-rw-rw----  1 haokunliu haokunliu 15364998 Feb 22 01:38 2402.18485v3_A_Multimodal_Foundation_Agent_for_Financial_Trading_Tool_Augmented_Div.pdf\n-rw-rw----  1 haokunliu haokunliu   593929 Feb 22 01:38 2410.00354v1_Hierarchical_Organization_Simulacra_in_the_Investment_Sector.pdf\n-rw-rw----  1 haokunliu haokunliu  1903362 Feb 22 01:38 2412.20138v7_TradingAgents_Multi_Agents_LLM_Financial_Trading_Framework.pdf\n-rw-rw----  1 haokunliu haokunliu  2198906 Feb 22 01:38 2502.13165v1_HedgeAgents_A_Balanced_aware_Multi_agent_Financial_Trading_System.pdf\n-rw-rw----  1 haokunliu haokunliu  1169177 Feb 22 01:38 2505.07078v5_Can_LLM_based_Financial_Investing_Strategies_Outperform_the_Market_in_.pdf\n-rw-rw----  1 haokunliu haokunliu  1919391 Feb 22 01:38 2510.02209v1_StockBench_Can_LLM_Agents_Trade_Stocks_Profitably_In_Real_world_Market.pdf\n-rw-rw----  1 haokunliu haokunliu  3832048 Feb 22 01:38 2510.27251v2_FinPos_A_Position_Aware_Trading_Agent_System_for_Real_Financial_Market.pdf\n-rw-rw----  1 haokunliu haokunliu   150243 Feb 22 01:38 2601.13770v1_Look_Ahead_Bench_a_Standardized_Benchmark_of_Look_ahead_Bias_in_Point_.pdf\n-rw-rw----  1 haokunliu haokunliu  1357992 Feb 22 01:38 2602.07023v1_Behavioral_Consistency_Validation_for_LLM_Agents_An_Analysis_of_Tradin.pdf\n-rw-rw----  1 haokunliu haokunliu     3555 Feb 22 01:41 README.md\ndrwxrwx---  2 haokunliu haokunliu       27 Feb 22 01:39 pages\n-rw-rw----  1 haokunliu haokunliu    15327 Feb 22 01:38 papers_metadata.json\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_78","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la datasets | sed -n '1,200p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 32\ndrwxrwx---  4 haokunliu haokunliu    7 Feb 22 01:41 .\ndrwxrwx--- 15 haokunliu haokunliu   22 Feb 22 01:42 ..\n-rw-rw----  1 haokunliu haokunliu  246 Feb 22 01:41 .gitignore\n-rw-rw----  1 haokunliu haokunliu 2692 Feb 22 01:41 README.md\n-rw-rw----  1 haokunliu haokunliu  859 Feb 22 01:40 dataset_stats.json\ndrwxrwx---  2 haokunliu haokunliu    4 Feb 22 01:39 market_daily_ohlcv_2010_2026\ndrwxrwx---  4 haokunliu haokunliu    4 Feb 22 01:40 twitter_financial_news_sentiment\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_79","type":"command_execution","command":"/bin/bash -lc \"pwd && ls -la code | sed -n '1,200p'\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\ntotal 63\ndrwxrwx---  5 haokunliu haokunliu    6 Feb 22 01:41 .\ndrwxrwx--- 15 haokunliu haokunliu   22 Feb 22 01:42 ..\n-rw-rw----  1 haokunliu haokunliu 1692 Feb 22 01:41 README.md\ndrwxrwx---  7 haokunliu haokunliu   20 Feb 22 01:40 fingpt\ndrwxrwx--- 11 haokunliu haokunliu   21 Feb 22 01:40 finrl\ndrwxrwx---  6 haokunliu haokunliu   15 Feb 22 01:40 tradingagents\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_80","type":"command_execution","command":"/bin/bash -lc 'pwd && ls -l literature_review.md resources.md .resource_finder_complete'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n-rw-rw---- 1 haokunliu haokunliu  152 Feb 22 01:42 .resource_finder_complete\n-rw-rw---- 1 haokunliu haokunliu 8267 Feb 22 01:42 literature_review.md\n-rw-rw---- 1 haokunliu haokunliu 5192 Feb 22 01:42 resources.md\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_81","type":"reasoning","text":"**Updating dependency declarations**"}}
{"type":"item.started","item":{"id":"item_82","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,120p' pyproject.toml\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_82","type":"command_execution","command":"/bin/bash -lc \"pwd && sed -n '1,120p' pyproject.toml\"","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n[project]\nname = \"research-workspace\"\nversion = \"0.1.0\"\ndescription = \"Research workspace for experiments\"\nrequires-python = \">=3.10\"\ndependencies = []\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_83","type":"reasoning","text":"**Applying patch for dependencies**"}}
{"type":"item.completed","item":{"id":"item_84","type":"file_change","changes":[{"path":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex/pyproject.toml","kind":"update"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_85","type":"reasoning","text":"**Verifying dataset script and downloads**"}}
{"type":"item.started","item":{"id":"item_86","type":"command_execution","command":"/bin/bash -lc 'pwd && git status --short'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_86","type":"command_execution","command":"/bin/bash -lc 'pwd && git status --short'","aggregated_output":"/data/hypogenicai/workspaces/refine-llm-trading-2354-codex\n M .gitignore\n?? .claude/\n?? .codex/\n?? .gemini/\n?? .idea-explorer/pipeline_state.json\n?? .resource_finder_complete\n?? artifacts/\n?? code/\n?? datasets/\n?? literature_review.md\n?? logs/\n?? paper_search.json\n?? paper_search_results/\n?? papers/\n?? pyproject.toml\n?? resources.md\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_87","type":"reasoning","text":"**Removing empty clutter file**"}}
{"type":"item.completed","item":{"id":"item_88","type":"agent_message","text":"I noticed an empty intermediate file from the first paper-finder attempt; I’m removing it to keep the workspace clean, then I’ll send the final resource summary."}}
{"type":"item.completed","item":{"id":"item_89","type":"reasoning","text":"**Noting command restrictions and output plan**"}}
{"type":"item.completed","item":{"id":"item_90","type":"agent_message","text":"Completed. I followed the full resource-finding workflow and created all required deliverables in the workspace.\n\n- Environment setup completed with `uv venv`, local `pyproject.toml`, and package install in `.venv` (used `uv pip install` fallback after `uv add` packaging error).\n- Papers downloaded to `papers/`: 10 PDFs, including all 7 high-relevance (`relevance >= 2`) results from paper-finder.\n- Deep reading performed with PDF chunking for 4 key papers in `papers/pages/` (all chunks processed for each selected key paper).\n- Datasets downloaded to `datasets/`: 2 datasets, validated and sampled.\n- Repositories cloned to `code/`: 3 relevant repos.\n- Documentation completed:\n  - `papers/README.md`\n  - `datasets/.gitignore`\n  - `datasets/README.md`\n  - `code/README.md`\n  - `literature_review.md`\n  - `resources.md`\n- Completion marker created:\n  - `.resource_finder_complete`\n\nKey counts (also in `.resource_finder_complete`):\n- Papers downloaded: 10\n- Datasets downloaded: 2\n- Repositories cloned: 3\n\nNotable generated artifacts:\n- Paper metadata: `papers/papers_metadata.json`\n- Deep-read extraction notes: `artifacts/deep_paper_notes.json`\n- Dataset stats: `datasets/dataset_stats.json`\n\nOne minor issue: removing an empty intermediate file was blocked by policy, so it may still exist if present."}}
{"type":"turn.completed","usage":{"input_tokens":1706876,"cached_input_tokens":1569408,"output_tokens":15710}}
